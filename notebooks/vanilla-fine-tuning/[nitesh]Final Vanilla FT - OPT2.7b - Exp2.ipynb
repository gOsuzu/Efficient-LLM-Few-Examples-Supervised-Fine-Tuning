{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1BceVUAvDAUht306cAQTJRRapAJRYummn","timestamp":1714490679858},{"file_id":"14Ns_9ncA9MimDZGTdbGAwO-d46xoeRVB","timestamp":1714461649212},{"file_id":"1ytKBpLWHGEZQNGuMGDJ_A9tpviXlMHQ3","timestamp":1714461631160},{"file_id":"1-WeRH2m8157msxCi4BWAFEMa6fmYdKHQ","timestamp":1714452733001},{"file_id":"1o7G7hBqY_teAY1dfBxFaPYfOS218mNZb","timestamp":1714201977972},{"file_id":"1-Eg41pChNrXtqPstHWgyHtI5d4QPSb7-","timestamp":1714174001163},{"file_id":"1uf_FvSnFW56aaBHVBm6yJQDBTn20lRBQ","timestamp":1713724435266}],"machine_shape":"hm","gpuType":"L4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# 1. IMPORT LIBRARIES"],"metadata":{"id":"Tw8CFJ12Pp6T"}},{"cell_type":"code","source":["#!pip install -q datasets accelerate\n","#!pip install -q git+https://github.com/huggingface/transformers.git@main\n","# !pip install -q git+https://github.com/huggingface/peft.git\n","# !pip install -q bitsandbytes datasets accelerate loralib\n","\n","\n","\n","#!pip install -q datasets accelerate\n","# !pip install -q git+https://github.com/huggingface/transformers.git@main\n","# !pip install -q git+https://github.com/huggingface/peft.git\n","# !pip install -q bitsandbytes datasets accelerate loralib\n","!pip install -q datasets\n","!pip install --upgrade transformers\n","!pip install tensorflow\n","\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","import gc\n","from torch.cuda.amp import autocast, GradScaler\n","\n","\n","from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, OPTForCausalLM, GPT2Tokenizer, AdamW, AutoModelForSequenceClassification\n","\n","!pip install -q accelerate\n","\n","# !pip install -q git+https://github.com/huggingface/transformers.git@main\n","# !pip install -q git+https://github.com/huggingface/peft.git\n","# !pip install -q bitsandbytes datasets accelerate loralib\n","\n","\n","import matplotlib.pyplot as plt\n","import pandas as pd\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2ayaqVMRAXaT","outputId":"032a7ba1-0730-49d3-c41d-6480156d1715","executionInfo":{"status":"ok","timestamp":1714489839474,"user_tz":240,"elapsed":24414,"user":{"displayName":"Nitesh Agarwal","userId":"18032940909522888771"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.22.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n","Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.11.0)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.62.2)\n","Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n","Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n","Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n","Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.2)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"]}]},{"cell_type":"code","source":["\n","!python --version\n","!nvcc --version\n","!pip install nvcc4jupyter\n","%load_ext nvcc4jupyter\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mY6pDshTHDtg","outputId":"2d71cb70-1e4a-4f03-f642-a26cfe6e8f5e","executionInfo":{"status":"ok","timestamp":1714489845076,"user_tz":240,"elapsed":5606,"user":{"displayName":"Nitesh Agarwal","userId":"18032940909522888771"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Python 3.10.12\n","nvcc: NVIDIA (R) Cuda compiler driver\n","Copyright (c) 2005-2023 NVIDIA Corporation\n","Built on Tue_Aug_15_22:02:13_PDT_2023\n","Cuda compilation tools, release 12.2, V12.2.140\n","Build cuda_12.2.r12.2/compiler.33191640_0\n","Requirement already satisfied: nvcc4jupyter in /usr/local/lib/python3.10/dist-packages (1.2.1)\n","Detected platform \"Colab\". Running its setup...\n","Source files will be saved in \"/tmp/tmp6dy03s96\".\n"]}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import random"],"metadata":{"id":"hOBeMp00RKSi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2. SET MAIN INPUTS FOR NOTEBOOK"],"metadata":{"id":"gBZINjFskJOT"}},{"cell_type":"code","source":["bx_size = 1                                 # batch size for inference OR TRAIN with the OPT\n","format_train_val = 'gpt3'                   # 'minimal' or 'gpt3\n","task_name = 'mnli'                          # 'mnli'\n","model_name = \"facebook/opt-2.7b\"            # model options below\n","examples_per_exp =  16                       # 16\n","num_experiments = 10                         # 10\n","num_validations = 1024   # not used yet in this NB (when later on doing validation needs to be specified at 1024)\n","\n","SEL_EXP_TRAIN_CD = 2                        # Select experiment to run\n","\n","\n","# model_name = \"facebook/opt-125m\"\n","# model_name = \"facebook/opt-350m\"\n","# model_name = \"facebook/opt-1.3b\"\n","# model_name = \"facebook/opt-2.7b\"\n","# model_name = \"facebook/opt-6.7b\""],"metadata":{"id":"tvPGy9EnkIKK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2. SET DEVICE"],"metadata":{"id":"ZBgRakKbCXl3"}},{"cell_type":"code","source":["import os\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""],"metadata":{"id":"aKf7vOw2MTPk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","\n","# Check if CUDA (GPU support) is available\n","cuda_available = torch.cuda.is_available()\n","print(f\"CUDA Available: {cuda_available}\")\n","\n","# If CUDA is available, print the GPU name(s)\n","if cuda_available:\n","    print(f\"GPU Name(s): {torch.cuda.get_device_name(0)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"18useFCOLdX2","outputId":"701ffaca-c702-4269-b2bb-6cf1ffe4bb79","executionInfo":{"status":"ok","timestamp":1714489845076,"user_tz":240,"elapsed":5,"user":{"displayName":"Nitesh Agarwal","userId":"18032940909522888771"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["CUDA Available: True\n","GPU Name(s): NVIDIA L4\n"]}]},{"cell_type":"code","source":["device = \"cpu\"\n","\n","# device_count = torch.cuda.device_count()\n","# if device_count > 0:\n","#     print(\"Select GPU device\")\n","#     device = torch.device(\"cuda\")\n","# else:\n","#     print(\"Select GPU device\")\n","#     device = torch.device(\"cpu\")\n","\n","print(device)\n","# torch.cuda.is_available()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QXFJO33ECUVB","outputId":"dcaafead-96c3-4878-b6e3-4b2cd4f6099d","executionInfo":{"status":"ok","timestamp":1714489845076,"user_tz":240,"elapsed":4,"user":{"displayName":"Nitesh Agarwal","userId":"18032940909522888771"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cpu\n"]}]},{"cell_type":"markdown","source":["## 3. IMPORT TOKENIZER AND SELECT MODEL"],"metadata":{"id":"DjaLyC3l7hVV"}},{"cell_type":"code","source":["# Choose model to work with:\n","\n","# model_name = \"facebook/opt-125m\"\n","# model_name = \"facebook/opt-350m\"\n","# model_name = \"facebook/opt-1.3b\"\n","# model_name = \"facebook/opt-2.7b\"\n","# model_name = \"facebook/opt-6.7b\"\n","\n","model_name = model_name # it is set up in top of NB"],"metadata":{"id":"2Kt5rM4s7g0T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["OPT_tokenizer = GPT2Tokenizer.from_pretrained(model_name)"],"metadata":{"id":"g49c1NTt_rcO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4. IMPORT NLI DATASET FOR TRAINING AND VALIDATION: MNLI"],"metadata":{"id":"kMS1b9WqPZWC"}},{"cell_type":"code","source":["# reference: https://github.com/uds-lsv/llmft/blob/main/notebooks/majority_baseline.ipynb\n","# this reference is useful for cleaning the neutral sentences of the dataset, just keeping the 0 and 1."],"metadata":{"id":"kGFeGMISPx7D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from collections import Counter\n","from datasets import load_dataset, ClassLabel"],"metadata":{"id":"9bQUsgPp7g9m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# this comes from original paper, to remove neutral examples from MNLI\n","def binarize_mnli(dataset, remove_neutral=True):\n","    if remove_neutral:\n","        # neutral class has label 1\n","        dataset = dataset.filter(lambda example: example[\"label\"] != 1)\n","\n","    # change labels of contradiction examples from 2 to 1\n","    def change_label(example):\n","        # convert labels 2 into labels 1. this merges the neutral and contradiction class\n","        example[\"label\"] = 1 if example[\"label\"] == 2 else example[\"label\"]\n","        return example\n","\n","    # change labels\n","    dataset = dataset.map(change_label)\n","\n","    # change features to reflect the new labels\n","    features = dataset[\"train\"].features.copy()\n","    features[\"label\"] = ClassLabel(num_classes=2, names=['entailment', 'contradiction'], id=None)\n","    dataset = dataset.cast(features)  # overwrite old features\n","\n","    return dataset\n"],"metadata":{"id":"l1N1cM5z7m16"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = load_dataset(\"glue\", task_name)"],"metadata":{"id":"T10hQZj079Le"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# binarize dataset\n","if task_name == \"mnli\":\n","    dataset = binarize_mnli(dataset, remove_neutral=True) # mnli\n"],"metadata":{"id":"QI3oAEj279Hl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# analyze and visualize dataset imported\n","\n","print(\"task_name:\", task_name)\n","for split in [\"train\", \"validation_matched\"]:\n","    c = Counter(dataset[split][\"label\"])\n","    total = len(list(c.elements()))\n","    print(\"Total number of samples:\", total)\n","    print(split)\n","    for k in c:\n","        print(f\"fraction of labels per class: {k}={c[k] / total}\")\n","print(dataset)"],"metadata":{"id":"qIFzmEFR8GQ1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714489850307,"user_tz":240,"elapsed":333,"user":{"displayName":"Nitesh Agarwal","userId":"18032940909522888771"}},"outputId":"f0a2e904-7868-4630-d615-e2012f293a79"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["task_name: mnli\n","Total number of samples: 261802\n","train\n","fraction of labels per class: 0=0.49999236063895613\n","fraction of labels per class: 1=0.5000076393610439\n","Total number of samples: 6692\n","validation_matched\n","fraction of labels per class: 1=0.4801255230125523\n","fraction of labels per class: 0=0.5198744769874477\n","DatasetDict({\n","    train: Dataset({\n","        features: ['premise', 'hypothesis', 'label', 'idx'],\n","        num_rows: 261802\n","    })\n","    validation_matched: Dataset({\n","        features: ['premise', 'hypothesis', 'label', 'idx'],\n","        num_rows: 6692\n","    })\n","    validation_mismatched: Dataset({\n","        features: ['premise', 'hypothesis', 'label', 'idx'],\n","        num_rows: 6703\n","    })\n","    test_matched: Dataset({\n","        features: ['premise', 'hypothesis', 'label', 'idx'],\n","        num_rows: 9796\n","    })\n","    test_mismatched: Dataset({\n","        features: ['premise', 'hypothesis', 'label', 'idx'],\n","        num_rows: 9847\n","    })\n","})\n"]}]},{"cell_type":"code","source":["# Perform the filters and splits from the original datasets\n","\n","\n","random_split_seed = 42\n","\n","examples_per_exp =  examples_per_exp # set above 16\n","num_experiments = num_experiments # set above 10\n","num_validations = num_validations # set above 16*64 #64*16 = 1024 #6692\n","\n","max_train_samples = examples_per_exp*num_experiments\n","train_dataset = dataset['train']\n","print(train_dataset)\n","\n","train_dataset_yes_all = dataset['train'].filter(lambda example: example[\"label\"] == 0)\n","train_dataset_no_all = dataset['train'].filter(lambda example: example[\"label\"] == 1)\n","print(train_dataset_yes_all)\n","print(train_dataset_no_all)\n","\n","val_dataset_all_indomain = dataset['validation_matched']\n","\n","# randomly select a subset of the training data\n","max_train_samples = min(len(train_dataset), max_train_samples)\n","\n","np.random.seed(random_split_seed)\n","indices_yes = np.random.choice(range(len(train_dataset_yes_all)), size=int(max_train_samples/2), replace=False)\n","print(\"indices_yes: \", indices_yes)\n","\n","np.random.seed(random_split_seed+1)\n","indices_no = np.random.choice(range(len(train_dataset_no_all)), size=int(max_train_samples/2), replace=False)\n","print(\"indices_no: \", indices_no)\n","\n","np.random.seed(random_split_seed+2)\n","indices_val_indomain = np.random.choice(range(len(val_dataset_all_indomain)), size=num_validations, replace=False)\n","print(\"indices_val: \", indices_val_indomain)"],"metadata":{"id":"7v9CdYUvSxGf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714489850307,"user_tz":240,"elapsed":6,"user":{"displayName":"Nitesh Agarwal","userId":"18032940909522888771"}},"outputId":"4deb0936-e2e7-48bc-adb9-b88d3d73beb3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset({\n","    features: ['premise', 'hypothesis', 'label', 'idx'],\n","    num_rows: 261802\n","})\n","Dataset({\n","    features: ['premise', 'hypothesis', 'label', 'idx'],\n","    num_rows: 130899\n","})\n","Dataset({\n","    features: ['premise', 'hypothesis', 'label', 'idx'],\n","    num_rows: 130903\n","})\n","indices_yes:  [108195  86013  39482  39689  10288  11589  94511  78690  36953  74067\n","  93678  83921  83896  21665  76736    651  48482  40811 127490  49367\n"," 121664  39918  60933 126502  65765  12966  33438   7201  19815  49187\n","  29116  48565 125127  60274  33985 130032 104535 120345 104033  44914\n","  89806  87143 103906  15697  29521   4906  46884  75442  57625  32365\n","  70562  78463  18684  45639  30223 118624  40945  75797  63681  77117\n","  16126 130579   2132 113346  68080   7433 120366 122242  75493  64389\n","  95467  86480  52323  42308 101738  51386 126981  27346  45655 121440]\n","indices_no:  [ 54039  34647  34994 102702  14063 110662  33077  24477  24337  19083\n","  61263 109299 107760  88071  22063  90740 113958   9163  45235  32885\n","  58399  59560 102582  10964  38283  16146  72067  55788  60576  21220\n","  41478 123489  38278  15117  71374  69791  39777 122448  10098  35761\n","  74547 109598  19072  61567  56626 102957  18014  14118  46250 117891\n","  87958 113798 107148 121622  88599   8239 119796  69862   2704 112545\n"," 121565 111890  19129 115169  29330  47129  79077  34942  28934  12323\n","  85926 103422  91532  32522   4654 108738  24476  86650 117487  61013]\n","indices_val:  [1771 4591 1869 ... 1111  809 1914]\n"]}]},{"cell_type":"code","source":["train_dataset_yes = train_dataset_yes_all.select(indices_yes)\n","train_dataset_no = train_dataset_no_all.select(indices_no)\n","\n","val_dataset_indomain = val_dataset_all_indomain.select(indices_val_indomain)\n","print(\"Train Dataset Yes: \", train_dataset_yes)\n","print(\"Train Dataset No: \", train_dataset_no)\n","print(\"Validation Dataset (in-domain): \", val_dataset_indomain)"],"metadata":{"id":"W5nx1ugOr5kd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714489850307,"user_tz":240,"elapsed":5,"user":{"displayName":"Nitesh Agarwal","userId":"18032940909522888771"}},"outputId":"e42efbcd-1352-4d64-e3c4-0be0e3473926"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train Dataset Yes:  Dataset({\n","    features: ['premise', 'hypothesis', 'label', 'idx'],\n","    num_rows: 80\n","})\n","Train Dataset No:  Dataset({\n","    features: ['premise', 'hypothesis', 'label', 'idx'],\n","    num_rows: 80\n","})\n","Validation Dataset (in-domain):  Dataset({\n","    features: ['premise', 'hypothesis', 'label', 'idx'],\n","    num_rows: 1024\n","})\n"]}]},{"cell_type":"code","source":["# Calculate the number of 0 and 1 in validation dataset\n","# and calculate the majority class accuracy\n","\n","val_dataset_indomain_yes = val_dataset_indomain.filter(lambda example: example[\"label\"] == 0)\n","val_dataset_indomain_no = val_dataset_indomain.filter(lambda example: example[\"label\"] == 1)\n","print(val_dataset_indomain_yes)\n","print(val_dataset_indomain_no)\n","print(\"Majority Class Accuracy: \", 100*max(len(val_dataset_indomain_yes), len(val_dataset_indomain_no))/(len(val_dataset_indomain_yes) + len(val_dataset_indomain_no)))"],"metadata":{"id":"15sgsMLaVGwC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714489850307,"user_tz":240,"elapsed":4,"user":{"displayName":"Nitesh Agarwal","userId":"18032940909522888771"}},"outputId":"4fdc45b6-f8af-4913-c0cc-50b6491f9df6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset({\n","    features: ['premise', 'hypothesis', 'label', 'idx'],\n","    num_rows: 536\n","})\n","Dataset({\n","    features: ['premise', 'hypothesis', 'label', 'idx'],\n","    num_rows: 488\n","})\n","Majority Class Accuracy:  52.34375\n"]}]},{"cell_type":"code","source":["\n","format_train_val = format_train_val # set it at the top of notebook in a common place\n","\n","def format_examples(example_val, format_val = format_train_val):\n","    if format_val== 'minimal':\n","      # \"minimal\" format\n","      return {'text': \"{\"  + example_val['premise'] + \"} {\" + example_val['hypothesis'] + \"}\" + \" ? Ġ\"}\n","    elif format_val== 'gpt3':\n","      # \"gpt3\" format\n","      return {'text': \"{\"  + example_val['premise'] + \"} question: {\" + example_val['hypothesis'] + \"}\" + \" Yes or No? answer: Ġ\"}\n","\n","def create_combined_dataset(train_ds_yes, train_ds_no, num_expts=num_experiments, num_train_examples=examples_per_exp):\n","    combined_dataset = []\n","    train_examples_yes = [example for example in train_ds_yes]\n","    train_examples_no = [example for example in train_ds_no]\n","\n","    for irep in range(num_expts):\n","          sampled_train_exs_yes = train_examples_yes[int(irep*num_train_examples/2) : int((irep +1)*num_train_examples/2)]\n","          sampled_train_exs_no = train_examples_no[int(irep*num_train_examples/2) : int((irep +1)*num_train_examples/2)]\n","          merged_sampled_train_exs = sampled_train_exs_yes + sampled_train_exs_no\n","          shuffled_list = merged_sampled_train_exs.copy()\n","          random.seed(irep)\n","          random.shuffle(shuffled_list)\n","\n","          for idx_shuffled_list in range(len(shuffled_list)):\n","\n","            if shuffled_list[idx_shuffled_list]['label'] == 0:\n","              target_token = 9904\n","            else:\n","              target_token = 3084\n","\n","            combined_ex = {'text': '', 'label': torch.tensor(shuffled_list[idx_shuffled_list]['label'], dtype=torch.long).to(device), 'exp': irep+1, 'target_token': torch.tensor(target_token, dtype=torch.long).to(device)}\n","\n","            combined_ex['text'] += shuffled_list[idx_shuffled_list]['text']\n","\n","            combined_dataset.append([combined_ex])\n","\n","    return combined_dataset\n","\n","\n","def dynamic_padding_collate_fn(batch):\n","\n","    batch = [item for sublist in batch for item in sublist]\n","\n","    texts = [item['text'] for item in batch]\n","    labels = [item['label'] for item in batch]\n","    exps = [item['exp'] for item in batch]\n","    target_tokens = [item['target_token'] for item in batch]\n","\n","    # choose option\n","    tokenized_inputs = OPT_tokenizer(texts, padding=\"longest\", truncation=True, return_tensors=\"pt\")\n","    # tokenized_inputs = OPT_tokenizer(texts, padding=\"max_length\", max_length = 2048, truncation=True, return_tensors=\"pt\")\n","\n","    labels_tensor = torch.unsqueeze(torch.tensor(labels, dtype=torch.long).to(device),0)\n","    exps_tensor = torch.unsqueeze(torch.tensor(exps, dtype=torch.long).to(device),0)\n","    target_token_tensor = torch.unsqueeze(torch.tensor(target_tokens, dtype=torch.long).to(device),0)\n","\n","    return {\n","        'text': texts,\n","        'input_ids': tokenized_inputs['input_ids'],\n","        'attention_mask': tokenized_inputs['attention_mask'],\n","        'label': labels_tensor,\n","        'exp': exps_tensor,\n","        'target_token': target_token_tensor\n","    }\n","\n","class CustomDataset(Dataset):\n","    def __init__(self, combined_dataset):\n","        self.dataset = combined_dataset\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx):\n","        return self.dataset[idx]\n"],"metadata":{"id":"JlqLEdhW84X1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","formatted_train_dataset_yes = train_dataset_yes.map(format_examples)\n","formatted_train_dataset_no = train_dataset_no.map(format_examples)\n","\n","# print result to check correctness\n","\n","combined_dataset = create_combined_dataset(\n","                                          train_ds_yes = formatted_train_dataset_yes,\n","                                          train_ds_no = formatted_train_dataset_no,\n","                                          num_expts=num_experiments,\n","                                          num_train_examples=examples_per_exp\n","                                           )\n","\n","custom_dataset = CustomDataset(combined_dataset)\n","custom_dataset_experiment = CustomDataset([item for item in custom_dataset if item[0]['exp'] == SEL_EXP_TRAIN_CD])\n","print(custom_dataset_experiment)\n","\n","# Last step, we create Dataloader passing the bx_size for inference/training (typically: 1, 4, 8, 16)\n","bx_size = bx_size # set it up at the beg of NB\n","dataloader_experiment = DataLoader(custom_dataset_experiment, batch_size=bx_size, collate_fn=dynamic_padding_collate_fn, shuffle=False) #shuffle=False for reproducibility"],"metadata":{"id":"Iux0yDPz9l91","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714489850308,"user_tz":240,"elapsed":4,"user":{"displayName":"Nitesh Agarwal","userId":"18032940909522888771"}},"outputId":"6516db69-5114-4dc7-98e5-b510f8c21b34"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<__main__.CustomDataset object at 0x78052b55a230>\n"]}]},{"cell_type":"code","source":["# This is to inspect that the dataloader is performing as expected\n","# Also using the decoding to check back that results are expected and examples can be compared\n","\n","# USE SIMILAR TO THIS TO PASS TO YOUR MODEL - SEE TENSOR DIMENSIONS AND ADJUST WITH SQUEEZE / UNSQUEEZE AS NEEDED THE DATALOADER OUTPUT AS INPUT TO YOUR MODEL\n","\n","for i, batch in enumerate(dataloader_experiment):\n","    if i<200:\n","      print(\"Item Number: \", i, \"experiment#: \", batch['exp'])\n","      print(\"DETOKENIZE: \", OPT_tokenizer.batch_decode(batch['input_ids']))\n","      print(\"Labels: \", batch['label'])\n","      print(\"Target Token: \", batch['target_token'])\n","      print(\"Input_ids: \", batch['input_ids'])\n","      print(\"Attention_Mask: \", batch['attention_mask'])\n","    else:\n","      break"],"metadata":{"id":"gLkf358E_4kn","executionInfo":{"status":"ok","timestamp":1714489850514,"user_tz":240,"elapsed":209,"user":{"displayName":"Nitesh Agarwal","userId":"18032940909522888771"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c12c370f-e172-4524-efd8-be7f92581e06"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"]},{"output_type":"stream","name":"stdout","text":["Item Number:  0 experiment#:  tensor([[2]])\n","DETOKENIZE:  [\"</s>{There's a small zoo area where you can see snakes, lizards, birds of prey, wolves, hyenas, foxes, and various desert cats, including cheetahs and leopards.} question: {The zoo is home to a variety of animals including mammals, birds, lizards, and snakes.} Yes or No? answer: Ġ\"]\n","Labels:  tensor([[0]])\n","Target Token:  tensor([[9904]])\n","Input_ids:  tensor([[    2, 45152,   970,    18,    10,   650, 14188,   443,   147,    47,\n","            64,   192, 24328,     6,   784, 39700,     6,  7723,     9, 18644,\n","             6, 27365,     6, 14791,   225,   281,     6, 23602,   293,     6,\n","             8,  1337, 10348, 10017,     6,   217,  5851,   594, 25118,     8,\n","          2084,  1517,  5954, 49463,   864,    35, 25522,   133, 14188,    16,\n","           184,     7,    10,  3143,     9,  3122,   217, 27772,     6,  7723,\n","             6,   784, 39700,     6,     8, 24328, 49463,  3216,    50,   440,\n","           116,  1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1]])\n","Item Number:  1 experiment#:  tensor([[2]])\n","DETOKENIZE:  [\"</s>{and i lived with Dana in school} question: {I didn't lived with Dana} Yes or No? answer: Ġ\"]\n","Labels:  tensor([[1]])\n","Target Token:  tensor([[3084]])\n","Input_ids:  tensor([[    2, 45152,   463,   939,  3033,    19, 11014,    11,   334, 24303,\n","           864,    35, 25522,   100,   399,    75,  3033,    19, 11014, 24303,\n","          3216,    50,   440,   116,  1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1]])\n","Item Number:  2 experiment#:  tensor([[2]])\n","DETOKENIZE:  ['</s>{I did so, and felt something bitty and sticky shoved down my throat.} question: {I felt something bitty and sticky forced down my throat.} Yes or No? answer: Ġ']\n","Labels:  tensor([[0]])\n","Target Token:  tensor([[9904]])\n","Input_ids:  tensor([[    2, 45152,   100,   222,    98,     6,     8,  1299,   402,   741,\n","         18308,     8, 25247, 30988,   159,   127, 14599, 49463,   864,    35,\n","         25522,   100,  1299,   402,   741, 18308,     8, 25247,  1654,   159,\n","           127, 14599, 49463,  3216,    50,   440,   116,  1948,    35,  4236,\n","         21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n","Item Number:  3 experiment#:  tensor([[2]])\n","DETOKENIZE:  ['</s>{Because a large proportion of street costs are fixed,27 the unit (per piece) street cost initially declines rapidly as volume increases and continues to decline at a decreasing rate.} question: {Street costs remain at $0.00 forever no matter what.} Yes or No? answer: Ġ']\n","Labels:  tensor([[1]])\n","Target Token:  tensor([[3084]])\n","Input_ids:  tensor([[    2, 45152, 10105,    10,   739, 10301,     9,  2014,  1042,    32,\n","          4460,     6,  2518,     5,  1933,    36,  1741,  2125,    43,  2014,\n","           701,  3225, 11081,  6042,    25,  3149,  3488,     8,  1388,     7,\n","          2991,    23,    10, 20910,   731, 49463,   864,    35, 25522, 24265,\n","          1042,  1091,    23,    68,   288,     4,   612,  6000,   117,   948,\n","            99, 49463,  3216,    50,   440,   116,  1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n","Item Number:  4 experiment#:  tensor([[2]])\n","DETOKENIZE:  ['</s>{The value of scale is about $4.} question: {The scale has a value of $4.} Yes or No? answer: Ġ']\n","Labels:  tensor([[0]])\n","Target Token:  tensor([[9904]])\n","Input_ids:  tensor([[    2, 45152,   133,   923,     9,  3189,    16,    59,    68,   306,\n","         49463,   864,    35, 25522,   133,  3189,    34,    10,   923,     9,\n","            68,   306, 49463,  3216,    50,   440,   116,  1948,    35,  4236,\n","         21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1]])\n","Item Number:  5 experiment#:  tensor([[2]])\n","DETOKENIZE:  [\"</s>{yeah it's it's been uh we we bought this house with the idea that we were going to spend you know spend a lot of time working on it and uh it was part you know part of the excitement was was getting a good deal on an older house it just it really hadn't been taken care of very well it it was actually it was rented out for a couple of years and things like that so we uh we ended up getting a fairly good deal on it but uh there just isn't enough time i i've i find myself going to work knowing that that there's a job about half done at home and i really if i would if i'd just stay home and finish it i'd feel a lot better but uh i'm i'm starting to learn that there's always something else so that you could  once you get done you can always start over and and you make up all kinds of excuses so uh} question: {we bought the house with the intention of doing it up, but we just haven't had the time, which is really frustrating.} Yes or No? answer: Ġ\"]\n","Labels:  tensor([[0]])\n","Target Token:  tensor([[9904]])\n","Input_ids:  tensor([[    2, 45152, 42803,    24,    18,    24,    18,    57, 37463,    52,\n","            52,  2162,    42,   790,    19,     5,  1114,    14,    52,    58,\n","           164,     7,  1930,    47,   216,  1930,    10,   319,     9,    86,\n","           447,    15,    24,     8, 37463,    24,    21,   233,    47,   216,\n","           233,     9,     5,  8354,    21,    21,   562,    10,   205,   432,\n","            15,    41,  2530,   790,    24,    95,    24,   269,  5844,    75,\n","            57,   551,   575,     9,   182,   157,    24,    24,    21,   888,\n","            24,    21, 16425,    66,    13,    10,   891,     9,   107,     8,\n","           383,   101,    14,    98,    52, 37463,    52,  1249,    62,   562,\n","            10,  5342,   205,   432,    15,    24,    53, 37463,    89,    95,\n","           965,    75,   615,    86,   939,   939,   348,   939,   465,  2185,\n","           164,     7,   173,  4730,    14,    14,    89,    18,    10,   633,\n","            59,   457,   626,    23,   184,     8,   939,   269,   114,   939,\n","            74,   114,   939,  1017,    95,  1095,   184,     8,  2073,    24,\n","           939,  1017,   619,    10,   319,   357,    53, 37463,   939,   437,\n","           939,   437,  1158,     7,  1532,    14,    89,    18,   460,   402,\n","          1493,    98,    14,    47,   115,  1437,   683,    47,   120,   626,\n","            47,    64,   460,   386,    81,     8,     8,    47,   146,    62,\n","            70,  6134,     9, 19791,    98, 37463, 24303,   864,    35, 25522,\n","          1694,  2162,     5,   790,    19,     5,  6589,     9,   608,    24,\n","            62,     6,    53,    52,    95,  2220,    75,    56,     5,    86,\n","             6,    61,    16,   269, 10314, 49463,  3216,    50,   440,   116,\n","          1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1]])\n","Item Number:  6 experiment#:  tensor([[2]])\n","DETOKENIZE:  ['</s>{Here the streets are almost always full of people, who congregate in local bars and restaurants.} question: {The streets are usually filled with people in bars and eateries.} Yes or No? answer: Ġ']\n","Labels:  tensor([[0]])\n","Target Token:  tensor([[9904]])\n","Input_ids:  tensor([[    2, 45152, 11773,     5,  2827,    32,   818,   460,   455,     9,\n","            82,     6,    54, 22880,   877,    11,   400,  5692,     8,  4329,\n","         49463,   864,    35, 25522,   133,  2827,    32,  2333,  3820,    19,\n","            82,    11,  5692,     8, 19969,   918, 49463,  3216,    50,   440,\n","           116,  1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n","Item Number:  7 experiment#:  tensor([[2]])\n","DETOKENIZE:  ['</s>{Somehow I got the feeling, when reading The Microsoft Way, that I was reading about the greatness of the French army and its Maginot line in 1939.} question: {The Microsoft Way did not remind me of the French army.} Yes or No? answer: Ġ']\n","Labels:  tensor([[1]])\n","Target Token:  tensor([[3084]])\n","Input_ids:  tensor([[    2, 45152,  6323,  9178,    38,   300,     5,  2157,     6,    77,\n","          2600,    20,  3709,  4846,  2156,    14,    38,    21,  2600,    59,\n","             5, 23657,     9,     5,  1515,  3835,     8,    63,  3771,   179,\n","          1242,   516,    11, 28234, 49463,   864,    35, 25522,   133,  3709,\n","          4846,   222,    45,  8736,   162,     9,     5,  1515,  3835, 49463,\n","          3216,    50,   440,   116,  1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n","Item Number:  8 experiment#:  tensor([[2]])\n","DETOKENIZE:  [\"</s>{Shouldn't have thought it.} question: {Shouldn't have entertained that idea.} Yes or No? answer: Ġ\"]\n","Labels:  tensor([[0]])\n","Target Token:  tensor([[9904]])\n","Input_ids:  tensor([[    2, 45152, 31231,   282,    75,    33,   802,    24, 49463,   864,\n","            35, 25522, 31231,   282,    75,    33, 23979,    14,  1114, 49463,\n","          3216,    50,   440,   116,  1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1]])\n","Item Number:  9 experiment#:  tensor([[2]])\n","DETOKENIZE:  [\"</s>{As the road climbs higher you'll reach a hidden valley, left behind as the glaciers of the Ice Age melted.} question: {The valley is noticeable almost immediately. } Yes or No? answer: Ġ\"]\n","Labels:  tensor([[1]])\n","Target Token:  tensor([[3084]])\n","Input_ids:  tensor([[    2, 45152,  1620,     5,   921, 30318,   723,    47,   581,  1338,\n","            10,  7397, 15044,     6,   314,   639,    25,     5, 35823,     9,\n","             5,  8761,  8927, 24136, 49463,   864,    35, 25522,   133, 15044,\n","            16, 20228,   818,  1320,     4, 35524,  3216,    50,   440,   116,\n","          1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n","Item Number:  10 experiment#:  tensor([[2]])\n","DETOKENIZE:  [\"</s>{So it's kind of a problem.} question: {It isn't a problem, this whole situation is not complex.} Yes or No? answer: Ġ\"]\n","Labels:  tensor([[1]])\n","Target Token:  tensor([[3084]])\n","Input_ids:  tensor([[    2, 45152,  2847,    24,    18,   761,     9,    10,   936, 49463,\n","           864,    35, 25522,   243,   965,    75,    10,   936,     6,    42,\n","          1086,  1068,    16,    45,  2632, 49463,  3216,    50,   440,   116,\n","          1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n","Item Number:  11 experiment#:  tensor([[2]])\n","DETOKENIZE:  ['</s>{Linda Samels Ceballos entered Loyola Law School in Los Angeles knowing she wanted to represent the poor.} question: {Linda Ceballos went to Loyal Law School.} Yes or No? answer: Ġ']\n","Labels:  tensor([[0]])\n","Target Token:  tensor([[9904]])\n","Input_ids:  tensor([[    2, 45152,   574,  8865,  1960,  2507,   230,  3209,  1250,   366,\n","          2867,   226,  2160,  3019,  2589,   835,    11,  1287,  1422,  4730,\n","            79,   770,     7,  3594,     5,  2129, 49463,   864,    35, 25522,\n","           574,  8865,   230,  3209,  1250,   366,   439,     7, 33872,  2589,\n","           835, 49463,  3216,    50,   440,   116,  1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1]])\n","Item Number:  12 experiment#:  tensor([[2]])\n","DETOKENIZE:  [\"</s>{I will share her story another time as it has little connection to Susan other than to say I used to work for them and then later I did not.} question: {Her story is totally intertwined with Susan's.} Yes or No? answer: Ġ\"]\n","Labels:  tensor([[1]])\n","Target Token:  tensor([[3084]])\n","Input_ids:  tensor([[    2, 45152,   100,    40,   458,    69,   527,   277,    86,    25,\n","            24,    34,   410,  2748,     7,  6470,    97,    87,     7,   224,\n","            38,   341,     7,   173,    13,   106,     8,   172,   423,    38,\n","           222,    45, 49463,   864,    35, 25522, 13584,   527,    16,  4940,\n","         35506,    19,  6470,    18, 49463,  3216,    50,   440,   116,  1948,\n","            35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1]])\n","Item Number:  13 experiment#:  tensor([[2]])\n","DETOKENIZE:  ['</s>{The English gutter press, which was just developing a wide audience, whipped up public hatred toward him over his sex crimes and made him a pariah.} question: {He had a clean criminal record.} Yes or No? answer: Ġ']\n","Labels:  tensor([[1]])\n","Target Token:  tensor([[3084]])\n","Input_ids:  tensor([[    2, 45152,   133,  2370,   821, 12158,  1228,     6,    61,    21,\n","            95,  2623,    10,  1810,  2437,     6, 23046,    62,   285, 13453,\n","          1706,   123,    81,    39,  2099,  3474,     8,   156,   123,    10,\n","          2242,  7138, 49463,   864,    35, 25522,   894,    56,    10,  2382,\n","          1837,   638, 49463,  3216,    50,   440,   116,  1948,    35,  4236,\n","         21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1]])\n","Item Number:  14 experiment#:  tensor([[2]])\n","DETOKENIZE:  [\"</s>{could they do road work and those kind of things and now and then you get somebody like Charles Manson who just even hasn't a guy around} question: {They couldn't do things like roadwork.} Yes or No? answer: Ġ\"]\n","Labels:  tensor([[1]])\n","Target Token:  tensor([[3084]])\n","Input_ids:  tensor([[    2, 45152, 17304,    51,   109,   921,   173,     8,   167,   761,\n","             9,   383,     8,   122,     8,   172,    47,   120,  4909,   101,\n","          3163, 23445,    54,    95,   190,  2282,    75,    10,  2173,   198,\n","         24303,   864,    35, 25522,  1213,  1705,    75,   109,   383,   101,\n","           921,  6014, 49463,  3216,    50,   440,   116,  1948,    35,  4236,\n","         21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1]])\n","Item Number:  15 experiment#:  tensor([[2]])\n","DETOKENIZE:  ['</s>{The Astronomer said, \"I don\\'t understand you.\"} question: {They are confused.} Yes or No? answer: Ġ']\n","Labels:  tensor([[0]])\n","Target Token:  tensor([[9904]])\n","Input_ids:  tensor([[    2, 45152,   133, 24494, 11032,    26,     6,    22,   100,   218,\n","            75,  1346,    47,    72, 24303,   864,    35, 25522,  1213,    32,\n","         10985, 49463,  3216,    50,   440,   116,  1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1]])\n"]}]},{"cell_type":"markdown","source":["# IN-DOMAIN VALIDATION DATASET"],"metadata":{"id":"4gxoA3Lop7hO"}},{"cell_type":"code","source":["\n","def create_combined_dataset_indomain_VALIDATION(val_dataset, num_expts=num_experiments):\n","    combined_dataset = []\n","\n","    for irep in range(num_expts):\n","      for val_ex in val_dataset:\n","\n","            if val_ex['label'] == 0:\n","              target_token = 9904\n","            else:\n","              target_token = 3084\n","\n","            combined_ex = {'text': '', 'label': torch.tensor(val_ex['label'], dtype=torch.long).to(device), 'exp': irep+1, 'target_token': torch.tensor(target_token, dtype=torch.long).to(device)}\n","\n","            combined_ex['text'] += val_ex['text']\n","\n","            # Append the new combined example to the combined dataset\n","            combined_dataset.append([combined_ex])\n","\n","    return combined_dataset\n"],"metadata":{"id":"wc31z8yZUQWm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["formatted_val_dataset_indomain = val_dataset_indomain.map(format_examples)\n","\n","combined_dataset_INDOMAIN_VALIDATION = create_combined_dataset_indomain_VALIDATION(\n","                                          val_dataset = formatted_val_dataset_indomain,\n","                                          num_expts=num_experiments\n","                                           )\n","\n","custom_dataset_indomain_validation = CustomDataset(combined_dataset_INDOMAIN_VALIDATION)\n","custom_dataset_indomain_val_experiment = CustomDataset([item for item in custom_dataset_indomain_validation if item[0]['exp'] == SEL_EXP_TRAIN_CD])\n","print(custom_dataset_indomain_val_experiment)\n","\n","# Last step, we create Dataloader passing the bx_size for inference/training (typically: 1, 4, 8, 16)\n","bx_size = bx_size # set it up at the beg of NB\n","dataloader_indomain_val_experiment = DataLoader(custom_dataset_indomain_val_experiment, batch_size=bx_size, collate_fn=dynamic_padding_collate_fn, shuffle=False) #shuffle=False for reproducibility"],"metadata":{"id":"7XG7PTIOrV1O","executionInfo":{"status":"ok","timestamp":1714489851280,"user_tz":240,"elapsed":767,"user":{"displayName":"Nitesh Agarwal","userId":"18032940909522888771"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"6cff3abf-82da-4998-cbd7-d3bfe954703e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<__main__.CustomDataset object at 0x78058210bd00>\n"]}]},{"cell_type":"code","source":["\n","for i, batch in enumerate(dataloader_indomain_val_experiment):\n","    if i<20:\n","      print(\"Item Number: \", i, \"experiment#: \", batch['exp'])\n","      print(\"DETOKENIZE: \", OPT_tokenizer.batch_decode(batch['input_ids']))\n","      print(\"Labels: \", batch['label'])\n","      print(\"Target Token: \", batch['target_token'])\n","      print(\"Input_ids: \", batch['input_ids'])\n","      print(\"Attention_Mask: \", batch['attention_mask'])\n","    else:\n","      break"],"metadata":{"id":"oQBs1lgxsPhO","executionInfo":{"status":"ok","timestamp":1714489851280,"user_tz":240,"elapsed":8,"user":{"displayName":"Nitesh Agarwal","userId":"18032940909522888771"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"399eb91a-40a1-44e9-f15e-096bcb671bbd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Item Number:  0 experiment#:  tensor([[2]])\n","DETOKENIZE:  [\"</s>{no it didn't} question: {Yes it did.} Yes or No? answer: Ġ\"]\n","Labels:  tensor([[1]])\n","Target Token:  tensor([[3084]])\n","Input_ids:  tensor([[    2, 45152,  2362,    24,   399,    75, 24303,   864,    35, 25522,\n","          9904,    24,   222, 49463,  3216,    50,   440,   116,  1948,    35,\n","          4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n","Item Number:  1 experiment#:  tensor([[2]])\n","DETOKENIZE:  [\"</s>{Who? asked Tommy.} question: {Tommy didn't know, who.} Yes or No? answer: Ġ\"]\n","Labels:  tensor([[0]])\n","Target Token:  tensor([[9904]])\n","Input_ids:  tensor([[    2, 45152, 12375,   116,   553,  8880, 49463,   864,    35, 25522,\n","         15691,  4783,   399,    75,   216,     6,    54, 49463,  3216,    50,\n","           440,   116,  1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1]])\n","Item Number:  2 experiment#:  tensor([[2]])\n","DETOKENIZE:  ['</s>{Paroseas cave, reef, and wreck diving around its shores, giving the diver a wide range of environments to explore.} question: {The diver has no variety in places to explore, they are monotonous. } Yes or No? answer: Ġ']\n","Labels:  tensor([[1]])\n","Target Token:  tensor([[3084]])\n","Input_ids:  tensor([[    2, 45152, 22011,  3876,   281, 12742,     6, 28350,     6,     8,\n","         15107, 12909,   198,    63, 20597,     6,  1311,     5, 13105,    10,\n","          1810,  1186,     9, 11534,     7,  5393, 49463,   864,    35, 25522,\n","           133, 13105,    34,   117,  3143,    11,  2127,     7,  5393,     6,\n","            51,    32,  6154, 27334,  1827,     4, 35524,  3216,    50,   440,\n","           116,  1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1]])\n","Item Number:  3 experiment#:  tensor([[2]])\n","DETOKENIZE:  [\"</s>{um i've visited the Wyoming area i'm not sure exactly where Dances with Wolves was filmed} question: {I don't know even though I visited the area.} Yes or No? answer: Ġ\"]\n","Labels:  tensor([[0]])\n","Target Token:  tensor([[9904]])\n","Input_ids:  tensor([[    2, 45152,   783,   939,   348,  3790,     5, 11027,   443,   939,\n","           437,    45,   686,  2230,   147,   211,  5332,    19, 13889,    21,\n","         10571, 24303,   864,    35, 25522,   100,   218,    75,   216,   190,\n","           600,    38,  3790,     5,   443, 49463,  3216,    50,   440,   116,\n","          1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n","Item Number:  4 experiment#:  tensor([[2]])\n","DETOKENIZE:  [\"</s>{i think Buffalo is an up an coming team they're going to they're showing some real promise for the next uh few years} question: {Buffalo is showing some real promise for the next few years, I think they are an up and coming team.} Yes or No? answer: Ġ\"]\n","Labels:  tensor([[0]])\n","Target Token:  tensor([[9904]])\n","Input_ids:  tensor([[    2, 45152,   118,   206,  5958,    16,    41,    62,    41,   567,\n","           165,    51,   214,   164,     7,    51,   214,  2018,   103,   588,\n","          4198,    13,     5,   220, 37463,   367,   107, 24303,   864,    35,\n","         25522, 42021,  7747,    16,  2018,   103,   588,  4198,    13,     5,\n","           220,   367,   107,     6,    38,   206,    51,    32,    41,    62,\n","             8,   567,   165, 49463,  3216,    50,   440,   116,  1948,    35,\n","          4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n","Item Number:  5 experiment#:  tensor([[2]])\n","DETOKENIZE:  ['</s>{They did this to us.} question: {This was done by them.} Yes or No? answer: Ġ']\n","Labels:  tensor([[0]])\n","Target Token:  tensor([[9904]])\n","Input_ids:  tensor([[    2, 45152,  1213,   222,    42,     7,   201, 49463,   864,    35,\n","         25522,   713,    21,   626,    30,   106, 49463,  3216,    50,   440,\n","           116,  1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1]])\n","Item Number:  6 experiment#:  tensor([[2]])\n","DETOKENIZE:  ['</s>{uh-huh how about any matching programs} question: {What about matching programs? } Yes or No? answer: Ġ']\n","Labels:  tensor([[0]])\n","Target Token:  tensor([[9904]])\n","Input_ids:  tensor([[    2, 45152,  2957,    12,   298,  2957,   141,    59,   143,  8150,\n","          1767, 24303,   864,    35, 25522,  2264,    59,  8150,  1767,   116,\n","         35524,  3216,    50,   440,   116,  1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1]])\n","Item Number:  7 experiment#:  tensor([[2]])\n","DETOKENIZE:  ['</s>{MC2000-2, was initially considered and recommended by the Commission under the market test rules.} question: {MC2000-2 was recommended by the Commission.} Yes or No? answer: Ġ']\n","Labels:  tensor([[0]])\n","Target Token:  tensor([[9904]])\n","Input_ids:  tensor([[    2, 45152,  6018, 17472,    12,   176,     6,    21,  3225,  1687,\n","             8,  5131,    30,     5,  1463,   223,     5,   210,  1296,  1492,\n","         49463,   864,    35, 25522,  6018, 17472,    12,   176,    21,  5131,\n","            30,     5,  1463, 49463,  3216,    50,   440,   116,  1948,    35,\n","          4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n","Item Number:  8 experiment#:  tensor([[2]])\n","DETOKENIZE:  ['</s>{Asked about abortion the other day on CNN, Republican National Committee Chairman Jim Nicholson also invoked what is apparently the party-line  inclusive party.} question: {The Republican National Committee Chairman freelanced on the topic of abortion when asked about it on CNN instead of reiterating the party-line.} Yes or No? answer: Ġ']\n","Labels:  tensor([[1]])\n","Target Token:  tensor([[3084]])\n","Input_ids:  tensor([[    2, 45152, 46688,    59,  6428,     5,    97,   183,    15,  3480,\n","             6,  1172,   496,  1674,  3356,  2488, 19408,    67, 29198,    99,\n","            16,  4100,     5,   537,    12,  1902,  1437, 10510,   537, 49463,\n","           864,    35, 25522,   133,  1172,   496,  1674,  3356, 30270, 16325,\n","            15,     5,  5674,     9,  6428,    77,   553,    59,    24,    15,\n","          3480,  1386,     9, 26209,  1295,     5,   537,    12,  1902, 49463,\n","          3216,    50,   440,   116,  1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n","Item Number:  9 experiment#:  tensor([[2]])\n","DETOKENIZE:  ['</s>{It was like looking into a mirror, except infinitely more realistic.} question: {It was more realistic than looking in a mirror. } Yes or No? answer: Ġ']\n","Labels:  tensor([[0]])\n","Target Token:  tensor([[9904]])\n","Input_ids:  tensor([[    2, 45152,   243,    21,   101,   546,    88,    10,  9807,     6,\n","          4682, 40489,    55, 10556, 49463,   864,    35, 25522,   243,    21,\n","            55, 10556,    87,   546,    11,    10,  9807,     4, 35524,  3216,\n","            50,   440,   116,  1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n","Item Number:  10 experiment#:  tensor([[2]])\n","DETOKENIZE:  ['</s>{John Kasich dropped his presidential bid.} question: {John Kasich recommitted himself to the presidential bid and plans on winning.} Yes or No? answer: Ġ']\n","Labels:  tensor([[1]])\n","Target Token:  tensor([[3084]])\n","Input_ids:  tensor([[    2, 45152, 10567, 26004,  1882,    39,  1939,  2311, 49463,   864,\n","            35, 25522, 10567, 26004, 37573, 16430,  1003,     7,     5,  1939,\n","          2311,     8,   708,    15,  1298, 49463,  3216,    50,   440,   116,\n","          1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n","Item Number:  11 experiment#:  tensor([[2]])\n","DETOKENIZE:  [\"</s>{well that's good that's great} question: {Shit, that is bad, that is horrible.} Yes or No? answer: Ġ\"]\n","Labels:  tensor([[1]])\n","Target Token:  tensor([[3084]])\n","Input_ids:  tensor([[    2, 45152,  3056,    14,    18,   205,    14,    18,   372, 24303,\n","           864,    35, 25522,  3609,   405,     6,    14,    16,  1099,     6,\n","            14,    16, 11385, 49463,  3216,    50,   440,   116,  1948,    35,\n","          4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1]])\n","Item Number:  12 experiment#:  tensor([[2]])\n","DETOKENIZE:  ['</s>{There were beads of perspiration on his brow.} question: {He was perfectly calm and dry as he waited.} Yes or No? answer: Ġ']\n","Labels:  tensor([[1]])\n","Target Token:  tensor([[3084]])\n","Input_ids:  tensor([[    2, 45152,   970,    58, 35036,     9, 20187, 41678,    15,    39,\n","         27423, 49463,   864,    35, 25522,   894,    21,  6683,  6327,     8,\n","          3841,    25,    37,  9010, 49463,  3216,    50,   440,   116,  1948,\n","            35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1]])\n","Item Number:  13 experiment#:  tensor([[2]])\n","DETOKENIZE:  ['</s>{Even the most aged and infirm travel here to die, for nothing is more blessed for a devout Hindu than to die in the great waters of the Varanasi and thus be released from the eternal cycle of rebirth.} question: {Devout Hindus believe that dying in the Varanasi frees a soul from the cycle of rebirth.} Yes or No? answer: Ġ']\n","Labels:  tensor([[0]])\n","Target Token:  tensor([[9904]])\n","Input_ids:  tensor([[    2, 45152,  8170,     5,   144,  5180,     8,  4047,  9856,  1504,\n","           259,     7,  1597,     6,    13,  1085,    16,    55, 12230,    13,\n","            10, 36906, 12316,    87,     7,  1597,    11,     5,   372,  5794,\n","             9,     5,  9676, 16264,   118,     8,  4634,    28,   703,    31,\n","             5, 25023,  4943,     9, 39652, 49463,   864,    35, 25522, 30504,\n","           995, 30618,   679,    14,  8180,    11,     5,  9676, 16264,   118,\n","          7619,   293,    10,  7047,    31,     5,  4943,     9, 39652, 49463,\n","          3216,    50,   440,   116,  1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1]])\n","Item Number:  14 experiment#:  tensor([[2]])\n","DETOKENIZE:  ['</s>{You and your friends are not welcome here, said Severn.} question: {Severn said the people were always welcome there.} Yes or No? answer: Ġ']\n","Labels:  tensor([[1]])\n","Target Token:  tensor([[3084]])\n","Input_ids:  tensor([[    2, 45152,  1185,     8,   110,   964,    32,    45,  2814,   259,\n","             6,    26,  1608, 12170, 49463,   864,    35, 25522, 14696, 12170,\n","            26,     5,    82,    58,   460,  2814,    89, 49463,  3216,    50,\n","           440,   116,  1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n","Item Number:  15 experiment#:  tensor([[2]])\n","DETOKENIZE:  ['</s>{Current Chinese leaders have distinctive characteristics that give them significant advantages over the United States in foreign policy.} question: {The us has advantages over China in foreign policy. } Yes or No? answer: Ġ']\n","Labels:  tensor([[1]])\n","Target Token:  tensor([[3084]])\n","Input_ids:  tensor([[    2, 45152, 42124,  1111,   917,    33, 16141, 12720,    14,   492,\n","           106,  1233, 12340,    81,     5,   315,   532,    11,  1093,   714,\n","         49463,   864,    35, 25522,   133,   201,    34, 12340,    81,   436,\n","            11,  1093,   714,     4, 35524,  3216,    50,   440,   116,  1948,\n","            35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n","Item Number:  16 experiment#:  tensor([[2]])\n","DETOKENIZE:  [\"</s>{The Chinese calendar was used to calculate the year of Japan's foundation by counting back the 1,260 years of the Chinese cosmological cycle.} question: {Japan's foundation was determined by using the Chinese calendar.} Yes or No? answer: Ġ\"]\n","Labels:  tensor([[0]])\n","Target Token:  tensor([[9904]])\n","Input_ids:  tensor([[    2, 45152,   133,  1111,  7127,    21,   341,     7, 15756,     5,\n","            76,     9,  1429,    18,  4811,    30, 10581,   124,     5,   112,\n","             6, 21566,   107,     9,     5,  1111, 12793,   119,  9779,  4943,\n","         49463,   864,    35, 25522, 21318,    18,  4811,    21,  3030,    30,\n","           634,     5,  1111,  7127, 49463,  3216,    50,   440,   116,  1948,\n","            35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1]])\n","Item Number:  17 experiment#:  tensor([[2]])\n","DETOKENIZE:  [\"</s>{Two clues in the Pennsylvania  1) The boy had said, I'm going to go to the dinner dance and kill some people.} question: {There was only one clue in Pennsylvania and it had nothing to do with the boy.} Yes or No? answer: Ġ\"]\n","Labels:  tensor([[1]])\n","Target Token:  tensor([[3084]])\n","Input_ids:  tensor([[    2, 45152,  9058, 14885,    11,     5,  4367,  1437,   112,    43,\n","            20,  2143,    56,    26,     6,    38,   437,   164,     7,   213,\n","             7,     5,  3630,  3836,     8,  3549,   103,    82, 49463,   864,\n","            35, 25522,   970,    21,   129,    65, 18664,    11,  4367,     8,\n","            24,    56,  1085,     7,   109,    19,     5,  2143, 49463,  3216,\n","            50,   440,   116,  1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1]])\n","Item Number:  18 experiment#:  tensor([[2]])\n","DETOKENIZE:  ['</s>{8 A stoichiometry of 1.03 is typical when the FGD process is producing gypsum by-product, while a stoichiometry of 1.05 is needed to produce waste suitable for a landfill.} question: {A stoichiometry of 1.07 is typical when the FGD process is producing gypsum by-product} Yes or No? answer: Ġ']\n","Labels:  tensor([[1]])\n","Target Token:  tensor([[3084]])\n","Input_ids:  tensor([[    2, 45152,   398,    83, 20572, 20000, 40899,     9,   112,     4,\n","          3933,    16,  6097,    77,     5, 25408,   495,   609,    16,  5591,\n","         18124,  3275,   783,    30,    12, 20565,     6,   150,    10, 20572,\n","         20000, 40899,     9,   112,     4,  2546,    16,   956,     7,  2592,\n","          3844, 10686,    13,    10, 21289, 49463,   864,    35, 25522,   250,\n","         20572, 20000, 40899,     9,   112,     4,  3570,    16,  6097,    77,\n","             5, 25408,   495,   609,    16,  5591, 18124,  3275,   783,    30,\n","            12, 20565, 24303,  3216,    50,   440,   116,  1948,    35,  4236,\n","         21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1]])\n","Item Number:  19 experiment#:  tensor([[2]])\n","DETOKENIZE:  [\"</s>{oh yes yeah yeah yeah that's true too that's true} question: {It is true} Yes or No? answer: Ġ\"]\n","Labels:  tensor([[0]])\n","Target Token:  tensor([[9904]])\n","Input_ids:  tensor([[    2, 45152,  2678,  4420, 11380, 11380, 11380,    14,    18,  1528,\n","           350,    14,    18,  1528, 24303,   864,    35, 25522,   243,    16,\n","          1528, 24303,  3216,    50,   440,   116,  1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1]])\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qCU9xUpVx1ei"},"outputs":[],"source":["# Custom model class for sequence classification\n","\n","import torch.nn as nn\n","from transformers import AutoModelForSequenceClassification, AutoTokenizer\n","\n","class OPT_VanillaFT(nn.Module):\n","    def __init__(self, model_name, num_labels):\n","        super().__init__()\n","        self.model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n","\n","    def forward(self, input_ids, attention_mask=None, labels=None):\n","        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","        logits = outputs.logits\n","        return logits"]},{"cell_type":"code","source":["# Define optimizer and loss function\n","model = OPT_VanillaFT(model_name, num_labels=2)\n","# print(model)\n","optimizer = AdamW(model.parameters(), lr=1e-6)\n","criterion = nn.CrossEntropyLoss()\n","\n","# Training loop\n","model.to(device)\n","model.train()\n","\n","train_losses = []\n","val_losses = []\n","train_accuracies = []\n","val_accuracies = []\n","\n","num_epochs = 6\n","batch_size = 1  # Reduce batch size to 4\n","for epoch in range(num_epochs):\n","    print(\"epoch: \", epoch)\n","    total_loss = 0.0\n","    train_acc = 0.0\n","    for i, batch in enumerate(dataloader_experiment):\n","\n","        # print(\"input_ids: \", batch[\"input_ids\"])\n","\n","\n","        input_ids = batch[\"input_ids\"].to(device)\n","        attention_mask = batch[\"attention_mask\"].to(device)\n","        labels = batch[\"label\"].to(device).squeeze(0)\n","\n","\n","        # print(\"labels: \", batch[\"label\"], labels.shape)\n","        optimizer.zero_grad()\n","\n","        # Forward pass\n","        outputs = model(input_ids, None, labels) #, labels=labels)\n","        labels = torch.nn.functional.one_hot(labels, num_classes=2).float()\n","        # print(\"outputs SHAPE: \", outputs.shape)\n","        # print(outputs)\n","        # # Compute loss\n","        loss = criterion(outputs, labels)\n","        # print(\"loss: \", loss)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","\n","        # Compute accuracy\n","        train_acc = train_acc + 1*(torch.argmax(outputs, dim=1).item()==batch[\"label\"].to(device).squeeze(0).item())\n","\n","    average_loss = total_loss / (i+1)\n","    train_losses.append(average_loss)\n","    train_accuracies.append(train_acc / (i+1))\n","    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {average_loss:.6f}, Accuracy: {train_accuracies[-1]:.9f}\")\n","\n","    # val_losses.append(val_loss / num_validations)\n","    # val_accuracies.append(val_accuracy / num_validations)\n","\n","    # print(f\"Validation Loss: {val_losses[-1]:.6f}, Validation Accuracy: {val_accuracies[-1]:.9f}\")\n","\n","    # model.train()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yKFssGNxx40D","outputId":"6eb8c336-1b39-4682-dc41-9afb1a71e9fb","executionInfo":{"status":"ok","timestamp":1714490583790,"user_tz":240,"elapsed":732512,"user":{"displayName":"Nitesh Agarwal","userId":"18032940909522888771"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of OPTForSequenceClassification were not initialized from the model checkpoint at facebook/opt-2.7b and are newly initialized: ['score.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["epoch:  0\n","Epoch 1/6, Train Loss: 0.914436, Accuracy: 0.437500000\n","epoch:  1\n","Epoch 2/6, Train Loss: 0.664959, Accuracy: 0.500000000\n","epoch:  2\n","Epoch 3/6, Train Loss: 0.550712, Accuracy: 0.812500000\n","epoch:  3\n","Epoch 4/6, Train Loss: 0.361896, Accuracy: 0.937500000\n","epoch:  4\n","Epoch 5/6, Train Loss: 0.235857, Accuracy: 1.000000000\n","epoch:  5\n","Epoch 6/6, Train Loss: 0.150947, Accuracy: 1.000000000\n"]}]},{"cell_type":"code","source":["# Validation\n","model.eval()\n","val_loss = 0.0\n","val_accuracy = 0.0\n","with torch.no_grad():\n","    for batch in dataloader_indomain_val_experiment:\n","        src = batch[\"input_ids\"].to(device)\n","        attention_mask = batch[\"attention_mask\"].to(device)\n","        labels = batch[\"label\"].to(device).squeeze(0)\n","\n","        # Forward pass\n","        outputs = model(input_ids, None, labels)\n","\n","        # Compute loss\n","        # if 'logits' in outputs:\n","        loss = criterion(outputs, labels)\n","        val_loss += loss.item()\n","\n","        # Compute accuracy\n","        val_accuracy += 1*(torch.argmax(outputs, dim=1).item()==batch[\"label\"].to(device).squeeze(0).item())\n","    val_loss = val_loss/num_validations\n","    val_accuracy = val_accuracy/num_validations\n","    print(f\"Validation Loss: {val_loss:.6f}, Validation Accuracy: {val_accuracy:.9f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sGIvEE4MZPMw","executionInfo":{"status":"ok","timestamp":1714490583790,"user_tz":240,"elapsed":5,"user":{"displayName":"Nitesh Agarwal","userId":"18032940909522888771"}},"outputId":"be8d8a5e-7e83-455e-c644-560bda42cabb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Validation Loss: 1.745123, Validation Accuracy: 0.494120180\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i7DXLZYPedey","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714490586083,"user_tz":240,"elapsed":2297,"user":{"displayName":"Nitesh Agarwal","userId":"18032940909522888771"}},"outputId":"a6101cf9-e1eb-46da-869f-a3aff943a38e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['premise', 'hypothesis', 'label', 'parse_premise', 'parse_hypothesis', 'binary_parse_premise', 'binary_parse_hypothesis', 'heuristic', 'subcase', 'template'],\n","        num_rows: 30000\n","    })\n","    validation: Dataset({\n","        features: ['premise', 'hypothesis', 'label', 'parse_premise', 'parse_hypothesis', 'binary_parse_premise', 'binary_parse_hypothesis', 'heuristic', 'subcase', 'template'],\n","        num_rows: 30000\n","    })\n","})"]},"metadata":{},"execution_count":28}],"source":["dataset_ood = load_dataset(\"hans\")\n","dataset_ood"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oAFcefWkwmPs"},"outputs":[],"source":["dataset_ood_val = (dataset_ood['validation']).filter(lambda example: example[\"heuristic\"] == 'lexical_overlap')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jn4ufgMzwmNS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714490586083,"user_tz":240,"elapsed":4,"user":{"displayName":"Nitesh Agarwal","userId":"18032940909522888771"}},"outputId":"e6e2fb92-4e72-4b4f-ad20-0ab2886adf0d"},"outputs":[{"output_type":"stream","name":"stdout","text":["indices_ood_val:  [6252 4684 1731 ... 9410 1671  474]\n"]},{"output_type":"execute_result","data":{"text/plain":["Dataset({\n","    features: ['premise', 'hypothesis', 'label', 'parse_premise', 'parse_hypothesis', 'binary_parse_premise', 'binary_parse_hypothesis', 'heuristic', 'subcase', 'template'],\n","    num_rows: 1024\n","})"]},"metadata":{},"execution_count":30}],"source":["# Perform the filters and splits from the original datasets\n","\n","\n","random_split_seed_ood = 42 # set above, equal to 42\n","\n","examples_per_exp =  examples_per_exp # 16\n","num_experiments = num_experiments # 10\n","num_validations = num_validations # 16*64 #64*16 = 1024 #6692\n","\n","np.random.seed(random_split_seed_ood)\n","indices_ood_val = np.random.choice(range(len(dataset_ood_val)), size=num_validations, replace=False)\n","print(\"indices_ood_val: \", indices_ood_val)\n","\n","dataset_ood_val_sel = dataset_ood_val.select(indices_ood_val)\n","dataset_ood_val_sel"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3aAqJWOqwmKR"},"outputs":[],"source":["# format examples functions formats according to different types of formats for ICL both training and validation examples\n","\n","# select format to use here:\n","format_train_val = format_train_val # set it at the top of notebook in a common place\n","\n","\n","def format_examples_validation_VALOOD(example_val, format_val = format_train_val):\n","    if format_val== 'minimal':\n","      # \"minimal\" format\n","      return {'text': \"{\"  + example_val['premise'] + \"} {\" + example_val['hypothesis'] + \"}\" + \" ? Ġ\"}\n","    elif format_val== 'gpt3':\n","      # \"minimal\" format\n","      return {'text': \"{\"  + example_val['premise'] + \"} question: {\" + example_val['hypothesis'] + \"}\" + \" Yes or No? answer: Ġ\"}\n","\n","def create_combined_dataset(val_dataset, num_expts=num_experiments):\n","    combined_dataset = []\n","\n","    for irep in range(num_expts):\n","      for val_ex in val_dataset:\n","\n","        combined_ex = {'text': '', 'label': torch.tensor(val_ex['label'], dtype=torch.long).to(device), 'exp': irep+1}\n","\n","        combined_ex['text'] += val_ex['text']\n","\n","        combined_dataset.append([combined_ex])\n","\n","    return combined_dataset\n","\n","\n","def dynamic_padding_collate_fn_VALOOD(batch):\n","    # This function is created to be able to tokenize dynamically to max length within each batch\n","    # Also, by modifying the tokenizer used, several other options are available\n","    # for example, if we set padding to a specified max_length, for example the model max_length, is also an option, not the default though\n","    # the default is the dynamic padding\n","\n","    batch = [item for sublist in batch for item in sublist]\n","\n","    texts = [item['text'] for item in batch]\n","    labels = [item['label'] for item in batch]\n","    exps = [item['exp'] for item in batch]\n","\n","    # choose option\n","    tokenized_inputs = OPT_tokenizer(texts, padding=\"longest\", truncation=True, return_tensors=\"pt\")\n","\n","    labels_tensor = torch.unsqueeze(torch.tensor(labels, dtype=torch.long).to(device),0)\n","    exps_tensor = torch.unsqueeze(torch.tensor(exps, dtype=torch.long).to(device),0)\n","\n","    return {\n","        'text': texts,\n","        'input_ids': tokenized_inputs['input_ids'],\n","        'attention_mask': tokenized_inputs['attention_mask'],\n","        'label': labels_tensor,\n","        'exp': exps_tensor,\n","    }\n","\n","\n","class CustomDataset(Dataset):\n","    def __init__(self, combined_dataset):\n","        self.dataset = combined_dataset\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx):\n","        return self.dataset[idx]\n"]},{"cell_type":"code","source":["# First the samples are formatted according to selection above\n","# Important to check selection and re-run cell above so that it is taken by the mapping function correctly\n","\n","formatted_val_dataset_ood = dataset_ood_val_sel.map(format_examples_validation_VALOOD)\n","\n","# Initialize custom dataset with the combined dataset\n","# print result to check correctness\n","\n","combined_dataset_VALOOD = create_combined_dataset(\n","                                          val_dataset = formatted_val_dataset_ood,\n","                                          num_expts=num_experiments\n","                                           )\n","\n","custom_dataset_VALOOD = CustomDataset(combined_dataset_VALOOD)\n","print(custom_dataset_VALOOD)\n","\n","custom_dataset_VALOOD_EXP = CustomDataset([item for item in custom_dataset_VALOOD if item[0]['exp'] == SEL_EXP_TRAIN_CD])\n","\n","# Last step, we create Dataloader passing the bx_size for inference (typically: 1, 4, 8, 16)\n","bx_size = bx_size # set it up at the beg of NB\n","dataloader_VALOOD = DataLoader(custom_dataset_VALOOD_EXP, batch_size=bx_size, collate_fn=dynamic_padding_collate_fn_VALOOD, shuffle=False) #shuffle=False for reproducibility"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2u9wEYxr2OLD","executionInfo":{"status":"ok","timestamp":1714490587356,"user_tz":240,"elapsed":1276,"user":{"displayName":"Nitesh Agarwal","userId":"18032940909522888771"}},"outputId":"e1761b1d-ee06-4f2c-be16-db1a871d0609"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<__main__.CustomDataset object at 0x780537b22680>\n"]}]},{"cell_type":"code","source":["# Validation\n","model.eval()\n","val_loss = 0.0\n","val_accuracy = 0.0\n","with torch.no_grad():\n","    for batch in dataloader_VALOOD:\n","        # print(batch)\n","\n","        src = batch[\"input_ids\"].to(device)\n","        attention_mask = batch[\"attention_mask\"].to(device)\n","        labels = batch[\"label\"].to(device).squeeze(0)\n","\n","        # Forward pass\n","        outputs = model(input_ids, None, labels)\n","\n","        # Compute loss\n","        # if 'logits' in outputs:\n","        loss = criterion(outputs, labels)\n","        val_loss += loss.item()\n","        # break\n","\n","        # Compute accuracy\n","        val_accuracy += 1*(torch.argmax(outputs, dim=1).item()==batch[\"label\"].to(device).squeeze(0).item())\n","    val_loss = val_loss/num_validations\n","    val_accuracy = val_accuracy/num_validations\n","    print(f\"Validation Loss: {val_loss:.6f}, Validation Accuracy: {val_accuracy:.9f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wBWNm-Cbx72J","executionInfo":{"status":"ok","timestamp":1714490587357,"user_tz":240,"elapsed":10,"user":{"displayName":"Nitesh Agarwal","userId":"18032940909522888771"}},"outputId":"04aae570-3801-4183-c2e5-9c7bae373113"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Validation Loss: 1.632745 , Validation Accuracy: 0.472724761\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"yu95zGyZrS9E"},"execution_count":null,"outputs":[]}]}