{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1BceVUAvDAUht306cAQTJRRapAJRYummn","timestamp":1714491603002},{"file_id":"14Ns_9ncA9MimDZGTdbGAwO-d46xoeRVB","timestamp":1714461649212},{"file_id":"1ytKBpLWHGEZQNGuMGDJ_A9tpviXlMHQ3","timestamp":1714461631160},{"file_id":"1-WeRH2m8157msxCi4BWAFEMa6fmYdKHQ","timestamp":1714452733001},{"file_id":"1o7G7hBqY_teAY1dfBxFaPYfOS218mNZb","timestamp":1714201977972},{"file_id":"1-Eg41pChNrXtqPstHWgyHtI5d4QPSb7-","timestamp":1714174001163},{"file_id":"1uf_FvSnFW56aaBHVBm6yJQDBTn20lRBQ","timestamp":1713724435266}],"machine_shape":"hm","gpuType":"L4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# 1. IMPORT LIBRARIES"],"metadata":{"id":"Tw8CFJ12Pp6T"}},{"cell_type":"code","source":["#!pip install -q datasets accelerate\n","#!pip install -q git+https://github.com/huggingface/transformers.git@main\n","# !pip install -q git+https://github.com/huggingface/peft.git\n","# !pip install -q bitsandbytes datasets accelerate loralib\n","\n","\n","\n","#!pip install -q datasets accelerate\n","# !pip install -q git+https://github.com/huggingface/transformers.git@main\n","# !pip install -q git+https://github.com/huggingface/peft.git\n","# !pip install -q bitsandbytes datasets accelerate loralib\n","!pip install -q datasets\n","!pip install --upgrade transformers\n","!pip install tensorflow\n","\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","import gc\n","from torch.cuda.amp import autocast, GradScaler\n","\n","\n","from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, OPTForCausalLM, GPT2Tokenizer, AdamW, AutoModelForSequenceClassification\n","\n","!pip install -q accelerate\n","\n","# !pip install -q git+https://github.com/huggingface/transformers.git@main\n","# !pip install -q git+https://github.com/huggingface/peft.git\n","# !pip install -q bitsandbytes datasets accelerate loralib\n","\n","\n","import matplotlib.pyplot as plt\n","import pandas as pd\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2ayaqVMRAXaT","outputId":"cdfc6dc3-0c46-43fb-a0c0-d42366fbe758","executionInfo":{"status":"ok","timestamp":1714490845866,"user_tz":240,"elapsed":24860,"user":{"displayName":"Nitesh Agarwal","userId":"18032940909522888771"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.22.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n","Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.11.0)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.62.2)\n","Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n","Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n","Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n","Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.2)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"]}]},{"cell_type":"code","source":["\n","!python --version\n","!nvcc --version\n","!pip install nvcc4jupyter\n","%load_ext nvcc4jupyter\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mY6pDshTHDtg","outputId":"b6fa205c-1ec8-4394-cce6-e1fb44538c87","executionInfo":{"status":"ok","timestamp":1714490851230,"user_tz":240,"elapsed":5368,"user":{"displayName":"Nitesh Agarwal","userId":"18032940909522888771"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Python 3.10.12\n","nvcc: NVIDIA (R) Cuda compiler driver\n","Copyright (c) 2005-2023 NVIDIA Corporation\n","Built on Tue_Aug_15_22:02:13_PDT_2023\n","Cuda compilation tools, release 12.2, V12.2.140\n","Build cuda_12.2.r12.2/compiler.33191640_0\n","Requirement already satisfied: nvcc4jupyter in /usr/local/lib/python3.10/dist-packages (1.2.1)\n","Detected platform \"Colab\". Running its setup...\n","Source files will be saved in \"/tmp/tmp1hnrlccj\".\n"]}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import random"],"metadata":{"id":"hOBeMp00RKSi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2. SET MAIN INPUTS FOR NOTEBOOK"],"metadata":{"id":"gBZINjFskJOT"}},{"cell_type":"code","source":["bx_size = 1                                 # batch size for inference OR TRAIN with the OPT\n","format_train_val = 'gpt3'                   # 'minimal' or 'gpt3\n","task_name = 'mnli'                          # 'mnli'\n","model_name = \"facebook/opt-2.7b\"            # model options below\n","examples_per_exp =  16                       # 16\n","num_experiments = 10                         # 10\n","num_validations = 1024   # not used yet in this NB (when later on doing validation needs to be specified at 1024)\n","\n","SEL_EXP_TRAIN_CD = 7                        # Select experiment to run\n","\n","\n","# model_name = \"facebook/opt-125m\"\n","# model_name = \"facebook/opt-350m\"\n","# model_name = \"facebook/opt-1.3b\"\n","# model_name = \"facebook/opt-2.7b\"\n","# model_name = \"facebook/opt-6.7b\""],"metadata":{"id":"tvPGy9EnkIKK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2. SET DEVICE"],"metadata":{"id":"ZBgRakKbCXl3"}},{"cell_type":"code","source":["import os\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""],"metadata":{"id":"aKf7vOw2MTPk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","\n","# Check if CUDA (GPU support) is available\n","cuda_available = torch.cuda.is_available()\n","print(f\"CUDA Available: {cuda_available}\")\n","\n","# If CUDA is available, print the GPU name(s)\n","if cuda_available:\n","    print(f\"GPU Name(s): {torch.cuda.get_device_name(0)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"18useFCOLdX2","outputId":"c3338890-c01a-47f7-9af4-9e933c173bf8","executionInfo":{"status":"ok","timestamp":1714490851230,"user_tz":240,"elapsed":5,"user":{"displayName":"Nitesh Agarwal","userId":"18032940909522888771"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["CUDA Available: True\n","GPU Name(s): NVIDIA L4\n"]}]},{"cell_type":"code","source":["device = \"cpu\"\n","\n","# device_count = torch.cuda.device_count()\n","# if device_count > 0:\n","#     print(\"Select GPU device\")\n","#     device = torch.device(\"cuda\")\n","# else:\n","#     print(\"Select GPU device\")\n","#     device = torch.device(\"cpu\")\n","\n","print(device)\n","# torch.cuda.is_available()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QXFJO33ECUVB","outputId":"c32af777-7947-409c-84c6-0977323d8908","executionInfo":{"status":"ok","timestamp":1714490851231,"user_tz":240,"elapsed":5,"user":{"displayName":"Nitesh Agarwal","userId":"18032940909522888771"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cpu\n"]}]},{"cell_type":"markdown","source":["## 3. IMPORT TOKENIZER AND SELECT MODEL"],"metadata":{"id":"DjaLyC3l7hVV"}},{"cell_type":"code","source":["# Choose model to work with:\n","\n","# model_name = \"facebook/opt-125m\"\n","# model_name = \"facebook/opt-350m\"\n","# model_name = \"facebook/opt-1.3b\"\n","# model_name = \"facebook/opt-2.7b\"\n","# model_name = \"facebook/opt-6.7b\"\n","\n","model_name = model_name # it is set up in top of NB"],"metadata":{"id":"2Kt5rM4s7g0T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["OPT_tokenizer = GPT2Tokenizer.from_pretrained(model_name)"],"metadata":{"id":"g49c1NTt_rcO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4. IMPORT NLI DATASET FOR TRAINING AND VALIDATION: MNLI"],"metadata":{"id":"kMS1b9WqPZWC"}},{"cell_type":"code","source":["# reference: https://github.com/uds-lsv/llmft/blob/main/notebooks/majority_baseline.ipynb\n","# this reference is useful for cleaning the neutral sentences of the dataset, just keeping the 0 and 1."],"metadata":{"id":"kGFeGMISPx7D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from collections import Counter\n","from datasets import load_dataset, ClassLabel"],"metadata":{"id":"9bQUsgPp7g9m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# this comes from original paper, to remove neutral examples from MNLI\n","def binarize_mnli(dataset, remove_neutral=True):\n","    if remove_neutral:\n","        # neutral class has label 1\n","        dataset = dataset.filter(lambda example: example[\"label\"] != 1)\n","\n","    # change labels of contradiction examples from 2 to 1\n","    def change_label(example):\n","        # convert labels 2 into labels 1. this merges the neutral and contradiction class\n","        example[\"label\"] = 1 if example[\"label\"] == 2 else example[\"label\"]\n","        return example\n","\n","    # change labels\n","    dataset = dataset.map(change_label)\n","\n","    # change features to reflect the new labels\n","    features = dataset[\"train\"].features.copy()\n","    features[\"label\"] = ClassLabel(num_classes=2, names=['entailment', 'contradiction'], id=None)\n","    dataset = dataset.cast(features)  # overwrite old features\n","\n","    return dataset\n"],"metadata":{"id":"l1N1cM5z7m16"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = load_dataset(\"glue\", task_name)"],"metadata":{"id":"T10hQZj079Le"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# binarize dataset\n","if task_name == \"mnli\":\n","    dataset = binarize_mnli(dataset, remove_neutral=True) # mnli\n"],"metadata":{"id":"QI3oAEj279Hl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# analyze and visualize dataset imported\n","\n","print(\"task_name:\", task_name)\n","for split in [\"train\", \"validation_matched\"]:\n","    c = Counter(dataset[split][\"label\"])\n","    total = len(list(c.elements()))\n","    print(\"Total number of samples:\", total)\n","    print(split)\n","    for k in c:\n","        print(f\"fraction of labels per class: {k}={c[k] / total}\")\n","print(dataset)"],"metadata":{"id":"qIFzmEFR8GQ1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714490855982,"user_tz":240,"elapsed":3,"user":{"displayName":"Nitesh Agarwal","userId":"18032940909522888771"}},"outputId":"0d26900d-a786-401b-ddad-dda4c491b0d2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["task_name: mnli\n","Total number of samples: 261802\n","train\n","fraction of labels per class: 0=0.49999236063895613\n","fraction of labels per class: 1=0.5000076393610439\n","Total number of samples: 6692\n","validation_matched\n","fraction of labels per class: 1=0.4801255230125523\n","fraction of labels per class: 0=0.5198744769874477\n","DatasetDict({\n","    train: Dataset({\n","        features: ['premise', 'hypothesis', 'label', 'idx'],\n","        num_rows: 261802\n","    })\n","    validation_matched: Dataset({\n","        features: ['premise', 'hypothesis', 'label', 'idx'],\n","        num_rows: 6692\n","    })\n","    validation_mismatched: Dataset({\n","        features: ['premise', 'hypothesis', 'label', 'idx'],\n","        num_rows: 6703\n","    })\n","    test_matched: Dataset({\n","        features: ['premise', 'hypothesis', 'label', 'idx'],\n","        num_rows: 9796\n","    })\n","    test_mismatched: Dataset({\n","        features: ['premise', 'hypothesis', 'label', 'idx'],\n","        num_rows: 9847\n","    })\n","})\n"]}]},{"cell_type":"code","source":["# Perform the filters and splits from the original datasets\n","\n","\n","random_split_seed = 42\n","\n","examples_per_exp =  examples_per_exp # set above 16\n","num_experiments = num_experiments # set above 10\n","num_validations = num_validations # set above 16*64 #64*16 = 1024 #6692\n","\n","max_train_samples = examples_per_exp*num_experiments\n","train_dataset = dataset['train']\n","print(train_dataset)\n","\n","train_dataset_yes_all = dataset['train'].filter(lambda example: example[\"label\"] == 0)\n","train_dataset_no_all = dataset['train'].filter(lambda example: example[\"label\"] == 1)\n","print(train_dataset_yes_all)\n","print(train_dataset_no_all)\n","\n","val_dataset_all_indomain = dataset['validation_matched']\n","\n","# randomly select a subset of the training data\n","max_train_samples = min(len(train_dataset), max_train_samples)\n","\n","np.random.seed(random_split_seed)\n","indices_yes = np.random.choice(range(len(train_dataset_yes_all)), size=int(max_train_samples/2), replace=False)\n","print(\"indices_yes: \", indices_yes)\n","\n","np.random.seed(random_split_seed+1)\n","indices_no = np.random.choice(range(len(train_dataset_no_all)), size=int(max_train_samples/2), replace=False)\n","print(\"indices_no: \", indices_no)\n","\n","np.random.seed(random_split_seed+2)\n","indices_val_indomain = np.random.choice(range(len(val_dataset_all_indomain)), size=num_validations, replace=False)\n","print(\"indices_val: \", indices_val_indomain)"],"metadata":{"id":"7v9CdYUvSxGf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714490856321,"user_tz":240,"elapsed":341,"user":{"displayName":"Nitesh Agarwal","userId":"18032940909522888771"}},"outputId":"e751f94d-9c3f-49e4-9ae0-e11ff952af42"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset({\n","    features: ['premise', 'hypothesis', 'label', 'idx'],\n","    num_rows: 261802\n","})\n","Dataset({\n","    features: ['premise', 'hypothesis', 'label', 'idx'],\n","    num_rows: 130899\n","})\n","Dataset({\n","    features: ['premise', 'hypothesis', 'label', 'idx'],\n","    num_rows: 130903\n","})\n","indices_yes:  [108195  86013  39482  39689  10288  11589  94511  78690  36953  74067\n","  93678  83921  83896  21665  76736    651  48482  40811 127490  49367\n"," 121664  39918  60933 126502  65765  12966  33438   7201  19815  49187\n","  29116  48565 125127  60274  33985 130032 104535 120345 104033  44914\n","  89806  87143 103906  15697  29521   4906  46884  75442  57625  32365\n","  70562  78463  18684  45639  30223 118624  40945  75797  63681  77117\n","  16126 130579   2132 113346  68080   7433 120366 122242  75493  64389\n","  95467  86480  52323  42308 101738  51386 126981  27346  45655 121440]\n","indices_no:  [ 54039  34647  34994 102702  14063 110662  33077  24477  24337  19083\n","  61263 109299 107760  88071  22063  90740 113958   9163  45235  32885\n","  58399  59560 102582  10964  38283  16146  72067  55788  60576  21220\n","  41478 123489  38278  15117  71374  69791  39777 122448  10098  35761\n","  74547 109598  19072  61567  56626 102957  18014  14118  46250 117891\n","  87958 113798 107148 121622  88599   8239 119796  69862   2704 112545\n"," 121565 111890  19129 115169  29330  47129  79077  34942  28934  12323\n","  85926 103422  91532  32522   4654 108738  24476  86650 117487  61013]\n","indices_val:  [1771 4591 1869 ... 1111  809 1914]\n"]}]},{"cell_type":"code","source":["train_dataset_yes = train_dataset_yes_all.select(indices_yes)\n","train_dataset_no = train_dataset_no_all.select(indices_no)\n","\n","val_dataset_indomain = val_dataset_all_indomain.select(indices_val_indomain)\n","print(\"Train Dataset Yes: \", train_dataset_yes)\n","print(\"Train Dataset No: \", train_dataset_no)\n","print(\"Validation Dataset (in-domain): \", val_dataset_indomain)"],"metadata":{"id":"W5nx1ugOr5kd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714490856321,"user_tz":240,"elapsed":9,"user":{"displayName":"Nitesh Agarwal","userId":"18032940909522888771"}},"outputId":"bb522a67-864a-4bd9-907c-e66f634725f0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train Dataset Yes:  Dataset({\n","    features: ['premise', 'hypothesis', 'label', 'idx'],\n","    num_rows: 80\n","})\n","Train Dataset No:  Dataset({\n","    features: ['premise', 'hypothesis', 'label', 'idx'],\n","    num_rows: 80\n","})\n","Validation Dataset (in-domain):  Dataset({\n","    features: ['premise', 'hypothesis', 'label', 'idx'],\n","    num_rows: 1024\n","})\n"]}]},{"cell_type":"code","source":["# Calculate the number of 0 and 1 in validation dataset\n","# and calculate the majority class accuracy\n","\n","val_dataset_indomain_yes = val_dataset_indomain.filter(lambda example: example[\"label\"] == 0)\n","val_dataset_indomain_no = val_dataset_indomain.filter(lambda example: example[\"label\"] == 1)\n","print(val_dataset_indomain_yes)\n","print(val_dataset_indomain_no)\n","print(\"Majority Class Accuracy: \", 100*max(len(val_dataset_indomain_yes), len(val_dataset_indomain_no))/(len(val_dataset_indomain_yes) + len(val_dataset_indomain_no)))"],"metadata":{"id":"15sgsMLaVGwC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714490856322,"user_tz":240,"elapsed":8,"user":{"displayName":"Nitesh Agarwal","userId":"18032940909522888771"}},"outputId":"2016aa2f-a194-4e0b-81b2-c532b8a4f27a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset({\n","    features: ['premise', 'hypothesis', 'label', 'idx'],\n","    num_rows: 536\n","})\n","Dataset({\n","    features: ['premise', 'hypothesis', 'label', 'idx'],\n","    num_rows: 488\n","})\n","Majority Class Accuracy:  52.34375\n"]}]},{"cell_type":"code","source":["\n","format_train_val = format_train_val # set it at the top of notebook in a common place\n","\n","def format_examples(example_val, format_val = format_train_val):\n","    if format_val== 'minimal':\n","      # \"minimal\" format\n","      return {'text': \"{\"  + example_val['premise'] + \"} {\" + example_val['hypothesis'] + \"}\" + \" ? Ġ\"}\n","    elif format_val== 'gpt3':\n","      # \"gpt3\" format\n","      return {'text': \"{\"  + example_val['premise'] + \"} question: {\" + example_val['hypothesis'] + \"}\" + \" Yes or No? answer: Ġ\"}\n","\n","def create_combined_dataset(train_ds_yes, train_ds_no, num_expts=num_experiments, num_train_examples=examples_per_exp):\n","    combined_dataset = []\n","    train_examples_yes = [example for example in train_ds_yes]\n","    train_examples_no = [example for example in train_ds_no]\n","\n","    for irep in range(num_expts):\n","          sampled_train_exs_yes = train_examples_yes[int(irep*num_train_examples/2) : int((irep +1)*num_train_examples/2)]\n","          sampled_train_exs_no = train_examples_no[int(irep*num_train_examples/2) : int((irep +1)*num_train_examples/2)]\n","          merged_sampled_train_exs = sampled_train_exs_yes + sampled_train_exs_no\n","          shuffled_list = merged_sampled_train_exs.copy()\n","          random.seed(irep)\n","          random.shuffle(shuffled_list)\n","\n","          for idx_shuffled_list in range(len(shuffled_list)):\n","\n","            if shuffled_list[idx_shuffled_list]['label'] == 0:\n","              target_token = 9904\n","            else:\n","              target_token = 3084\n","\n","            combined_ex = {'text': '', 'label': torch.tensor(shuffled_list[idx_shuffled_list]['label'], dtype=torch.long).to(device), 'exp': irep+1, 'target_token': torch.tensor(target_token, dtype=torch.long).to(device)}\n","\n","            combined_ex['text'] += shuffled_list[idx_shuffled_list]['text']\n","\n","            combined_dataset.append([combined_ex])\n","\n","    return combined_dataset\n","\n","\n","def dynamic_padding_collate_fn(batch):\n","\n","    batch = [item for sublist in batch for item in sublist]\n","\n","    texts = [item['text'] for item in batch]\n","    labels = [item['label'] for item in batch]\n","    exps = [item['exp'] for item in batch]\n","    target_tokens = [item['target_token'] for item in batch]\n","\n","    # choose option\n","    tokenized_inputs = OPT_tokenizer(texts, padding=\"longest\", truncation=True, return_tensors=\"pt\")\n","    # tokenized_inputs = OPT_tokenizer(texts, padding=\"max_length\", max_length = 2048, truncation=True, return_tensors=\"pt\")\n","\n","    labels_tensor = torch.unsqueeze(torch.tensor(labels, dtype=torch.long).to(device),0)\n","    exps_tensor = torch.unsqueeze(torch.tensor(exps, dtype=torch.long).to(device),0)\n","    target_token_tensor = torch.unsqueeze(torch.tensor(target_tokens, dtype=torch.long).to(device),0)\n","\n","    return {\n","        'text': texts,\n","        'input_ids': tokenized_inputs['input_ids'],\n","        'attention_mask': tokenized_inputs['attention_mask'],\n","        'label': labels_tensor,\n","        'exp': exps_tensor,\n","        'target_token': target_token_tensor\n","    }\n","\n","class CustomDataset(Dataset):\n","    def __init__(self, combined_dataset):\n","        self.dataset = combined_dataset\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx):\n","        return self.dataset[idx]\n"],"metadata":{"id":"JlqLEdhW84X1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","formatted_train_dataset_yes = train_dataset_yes.map(format_examples)\n","formatted_train_dataset_no = train_dataset_no.map(format_examples)\n","\n","# print result to check correctness\n","\n","combined_dataset = create_combined_dataset(\n","                                          train_ds_yes = formatted_train_dataset_yes,\n","                                          train_ds_no = formatted_train_dataset_no,\n","                                          num_expts=num_experiments,\n","                                          num_train_examples=examples_per_exp\n","                                           )\n","\n","custom_dataset = CustomDataset(combined_dataset)\n","custom_dataset_experiment = CustomDataset([item for item in custom_dataset if item[0]['exp'] == SEL_EXP_TRAIN_CD])\n","print(custom_dataset_experiment)\n","\n","# Last step, we create Dataloader passing the bx_size for inference/training (typically: 1, 4, 8, 16)\n","bx_size = bx_size # set it up at the beg of NB\n","dataloader_experiment = DataLoader(custom_dataset_experiment, batch_size=bx_size, collate_fn=dynamic_padding_collate_fn, shuffle=False) #shuffle=False for reproducibility"],"metadata":{"id":"Iux0yDPz9l91","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714490856322,"user_tz":240,"elapsed":8,"user":{"displayName":"Nitesh Agarwal","userId":"18032940909522888771"}},"outputId":"f16d557a-6b21-4d2a-a886-daf7e9ed7592"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<__main__.CustomDataset object at 0x7ff303c69d80>\n"]}]},{"cell_type":"code","source":["# This is to inspect that the dataloader is performing as expected\n","# Also using the decoding to check back that results are expected and examples can be compared\n","\n","# USE SIMILAR TO THIS TO PASS TO YOUR MODEL - SEE TENSOR DIMENSIONS AND ADJUST WITH SQUEEZE / UNSQUEEZE AS NEEDED THE DATALOADER OUTPUT AS INPUT TO YOUR MODEL\n","\n","for i, batch in enumerate(dataloader_experiment):\n","    if i<200:\n","      print(\"Item Number: \", i, \"experiment#: \", batch['exp'])\n","      print(\"DETOKENIZE: \", OPT_tokenizer.batch_decode(batch['input_ids']))\n","      print(\"Labels: \", batch['label'])\n","      print(\"Target Token: \", batch['target_token'])\n","      print(\"Input_ids: \", batch['input_ids'])\n","      print(\"Attention_Mask: \", batch['attention_mask'])\n","    else:\n","      break"],"metadata":{"id":"gLkf358E_4kn","executionInfo":{"status":"ok","timestamp":1714490856322,"user_tz":240,"elapsed":7,"user":{"displayName":"Nitesh Agarwal","userId":"18032940909522888771"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b62c5881-3d0f-4502-f7db-0f9c1ad874a4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"]},{"output_type":"stream","name":"stdout","text":["Item Number:  0 experiment#:  tensor([[7]])\n","DETOKENIZE:  [\"</s>{Somehow, that wasn't the answer I'd hoped for.} question: {I heard exactly what I wanted to hear.} Yes or No? answer: Ġ\"]\n","Labels:  tensor([[1]])\n","Target Token:  tensor([[3084]])\n","Input_ids:  tensor([[    2, 45152,  6323,  9178,     6,    14,   938,    75,     5,  1948,\n","            38,  1017,  5207,    13, 49463,   864,    35, 25522,   100,  1317,\n","          2230,    99,    38,   770,     7,  1798, 49463,  3216,    50,   440,\n","           116,  1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n","Item Number:  1 experiment#:  tensor([[7]])\n","DETOKENIZE:  [\"</s>{yeah i mean i've liked them ever since the season that they won the Super Bowl last i think it was eighty four i think it was} question: {I like them in year 2000.} Yes or No? answer: Ġ\"]\n","Labels:  tensor([[0]])\n","Target Token:  tensor([[9904]])\n","Input_ids:  tensor([[    2, 45152, 42803,   939,  1266,   939,   348,  6640,   106,   655,\n","           187,     5,   191,    14,    51,   351,     5,  1582,  2616,    94,\n","           939,   206,    24,    21, 42991,   237,   939,   206,    24,    21,\n","         24303,   864,    35, 25522,   100,   101,   106,    11,    76,  3788,\n","         49463,  3216,    50,   440,   116,  1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1]])\n","Item Number:  2 experiment#:  tensor([[7]])\n","DETOKENIZE:  ['</s>{uh-huh no no kids} question: {No, plenty of kids.  } Yes or No? answer: Ġ']\n","Labels:  tensor([[1]])\n","Target Token:  tensor([[3084]])\n","Input_ids:  tensor([[    2, 45152,  2957,    12,   298,  2957,   117,   117,  1159, 24303,\n","           864,    35, 25522,  3084,     6,  2710,     9,  1159,     4,  1437,\n","         35524,  3216,    50,   440,   116,  1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1]])\n","Item Number:  3 experiment#:  tensor([[7]])\n","DETOKENIZE:  ['</s>{By the 1980s, thanks to the broad avenues and new subway s tops built for the games, this had become one of the liveliest, trendiest, and demographically youngest quarters of the city.} question: {The city improvements on the youngest quarters made it an attractive area.} Yes or No? answer: Ġ']\n","Labels:  tensor([[0]])\n","Target Token:  tensor([[9904]])\n","Input_ids:  tensor([[    2, 45152,  2765,     5,  5114,    29,     6,  2446,     7,     5,\n","          4007, 19922,     8,    92, 15604,   579, 13657,  1490,    13,     5,\n","           426,     6,    42,    56,   555,    65,     9,     5,   697, 27911,\n","             6,  2904,  7098,     6,     8,  4410, 33195,  8733,  5666,     9,\n","             5,   343, 49463,   864,    35, 25522,   133,   343,  5139,    15,\n","             5,  8733,  5666,   156,    24,    41,  6043,   443, 49463,  3216,\n","            50,   440,   116,  1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n","Item Number:  4 experiment#:  tensor([[7]])\n","DETOKENIZE:  ['</s>{The 15th-century Gothic Casa y Torre de los Lujanes (House and Tower of the Lujanes) has an imposing stone portal and mudejar tower.} question: {Casa y Torre de los Lujanes is a stone built 15th century building. } Yes or No? answer: Ġ']\n","Labels:  tensor([[0]])\n","Target Token:  tensor([[9904]])\n","Input_ids:  tensor([[    2, 45152,   133,   379,   212,    12, 11046, 41362,  7233,   102,\n","          1423,  6623,   241,   263,  3774,  5078,   267, 11372,    36, 18691,\n","             8,  7186,     9,     5,  5078,   267, 11372,    43,    34,    41,\n","         13223,  7326, 16698,     8,   475,  6343, 11978,  9368, 49463,   864,\n","            35, 25522,   347,  8810,  1423,  6623,   241,   263,  3774,  5078,\n","           267, 11372,    16,    10,  7326,  1490,   379,   212,  3220,   745,\n","             4, 35524,  3216,    50,   440,   116,  1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n","Item Number:  5 experiment#:  tensor([[7]])\n","DETOKENIZE:  ['</s>{Julius wired to town for his own car, and they scoured the neighbourhood daily with unflagging zeal.} question: {Julius went by foot and was soon exhausted of scouring the neighborhood.} Yes or No? answer: Ġ']\n","Labels:  tensor([[1]])\n","Target Token:  tensor([[3084]])\n","Input_ids:  tensor([[    2, 45152, 22403,  6125, 26977,     7,  1139,    13,    39,   308,\n","           512,     6,     8,    51,  2850,  8855,     5, 12258,  1230,    19,\n","         29747, 12771, 33103, 49463,   864,    35, 25522, 22403,  6125,   439,\n","            30,  2767,     8,    21,  1010, 17067,     9,  2850, 15794,     5,\n","          3757, 49463,  3216,    50,   440,   116,  1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1]])\n","Item Number:  6 experiment#:  tensor([[7]])\n","DETOKENIZE:  [\"</s>{okay in Star Man he was the guy chasing after um Jeff Bridges and Karen Allen or Nancy Allen you know with short nerdy guy with glasses he's} question: {Nancy Allen was chasing after Jeff Bridges.} Yes or No? answer: Ġ\"]\n","Labels:  tensor([[1]])\n","Target Token:  tensor([[3084]])\n","Input_ids:  tensor([[    2, 45152,  1638,   857,    11,  2141,  1554,    37,    21,     5,\n","          2173, 11277,    71,  7252,  2321, 18765,     8,  7836,  3823,    50,\n","          8239,  3823,    47,   216,    19,   765, 38286,  7180,  2173,    19,\n","         11121,    37,    18, 24303,   864,    35, 25522,   487,  9875,  3823,\n","            21, 11277,    71,  2321, 18765, 49463,  3216,    50,   440,   116,\n","          1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1]])\n","Item Number:  7 experiment#:  tensor([[7]])\n","DETOKENIZE:  [\"</s>{i mean the Kurds have tried it before they've gotten their butts kicked and this is just another time that it's happening} question: {The Kurds tried it but lost.} Yes or No? answer: Ġ\"]\n","Labels:  tensor([[0]])\n","Target Token:  tensor([[9904]])\n","Input_ids:  tensor([[    2, 45152,   118,  1266,     5, 20673,    33,  1381,    24,   137,\n","            51,   348,  5335,    49,    53,  1872,  5836,     8,    42,    16,\n","            95,   277,    86,    14,    24,    18,  2909, 24303,   864,    35,\n","         25522,   133, 20673,  1381,    24,    53,   685, 49463,  3216,    50,\n","           440,   116,  1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n","Item Number:  8 experiment#:  tensor([[7]])\n","DETOKENIZE:  ['</s>{All but one of the programs provided community legal education, 89 percent engaged in outreach activities, and 75 percent disseminated pro se information.} question: {There were no programs that did not provide community legal education.} Yes or No? answer: Ġ']\n","Labels:  tensor([[1]])\n","Target Token:  tensor([[3084]])\n","Input_ids:  tensor([[    2, 45152,  3684,    53,    65,     9,     5,  1767,  1286,   435,\n","          1030,  1265,     6,  8572,   135,  4009,    11, 12356,  1713,     6,\n","             8,  3337,   135, 27369,  1070,  1759,   842,   335, 49463,   864,\n","            35, 25522,   970,    58,   117,  1767,    14,   222,    45,   694,\n","           435,  1030,  1265, 49463,  3216,    50,   440,   116,  1948,    35,\n","          4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1]])\n","Item Number:  9 experiment#:  tensor([[7]])\n","DETOKENIZE:  ['</s>{It was finally abandoned towards the fourth century a.d. when the country looked to a new group of deities.} question: {The country has always worshiped only one group of deities.} Yes or No? answer: Ġ']\n","Labels:  tensor([[1]])\n","Target Token:  tensor([[3084]])\n","Input_ids:  tensor([[    2, 45152,   243,    21,  1747,  6978,  1567,     5,   887,  3220,\n","            10,     4,   417,     4,    77,     5,   247,  1415,     7,    10,\n","            92,   333,     9, 46054, 49463,   864,    35, 25522,   133,   247,\n","            34,   460, 13405,   196,   129,    65,   333,     9, 46054, 49463,\n","          3216,    50,   440,   116,  1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n","Item Number:  10 experiment#:  tensor([[7]])\n","DETOKENIZE:  [\"</s>{They can't imagine being a part of it.} question: {They know exactly what it is like. } Yes or No? answer: Ġ\"]\n","Labels:  tensor([[1]])\n","Target Token:  tensor([[3084]])\n","Input_ids:  tensor([[    2, 45152,  1213,    64,    75,  4744,   145,    10,   233,     9,\n","            24, 49463,   864,    35, 25522,  1213,   216,  2230,    99,    24,\n","            16,   101,     4, 35524,  3216,    50,   440,   116,  1948,    35,\n","          4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1]])\n","Item Number:  11 experiment#:  tensor([[7]])\n","DETOKENIZE:  [\"</s>{right i i think in most cases i'd have to say no not unless somebody really enjoys it or} question: {I do not think so unless somebody likes it.  } Yes or No? answer: Ġ\"]\n","Labels:  tensor([[0]])\n","Target Token:  tensor([[9904]])\n","Input_ids:  tensor([[    2, 45152,  4070,   939,   939,   206,    11,   144,  1200,   939,\n","          1017,    33,     7,   224,   117,    45,  3867,  4909,   269, 11952,\n","            24,    50, 24303,   864,    35, 25522,   100,   109,    45,   206,\n","            98,  3867,  4909,  3829,    24,     4,  1437, 35524,  3216,    50,\n","           440,   116,  1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n","Item Number:  12 experiment#:  tensor([[7]])\n","DETOKENIZE:  [\"</s>{At the end of the legendary road to Hana on Maui's remote and lush east shore, this quiet upscale resort on 66 acres with a wild volcanic oceanfront is the oldest in Maui (1946).} question: {The oldest resort in Maui is located at the end of the road to Hana.} Yes or No? answer: Ġ\"]\n","Labels:  tensor([[0]])\n","Target Token:  tensor([[9904]])\n","Input_ids:  tensor([[    2, 45152,  3750,     5,   253,     9,     5,  9227,   921,     7,\n","           289,  1113,    15, 14147,   118,    18,  6063,     8, 25115,  3017,\n","          8373,     6,    42,  5128, 22563,  5753,    15,  5138,  6419,    19,\n","            10,  3418, 23183,  6444,  9289,    16,     5,  7763,    11, 14147,\n","           118,    36,  1646,  3761,   322, 24303,   864,    35, 25522,   133,\n","          7763,  5753,    11, 14147,   118,    16,  2034,    23,     5,   253,\n","             9,     5,   921,     7,   289,  1113, 49463,  3216,    50,   440,\n","           116,  1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1]])\n","Item Number:  13 experiment#:  tensor([[7]])\n","DETOKENIZE:  ['</s>{The Arab town was erected atop this sandy layer and many treasures are still lost below the surface of the modern streets.} question: {The town has no history underneath it.} Yes or No? answer: Ġ']\n","Labels:  tensor([[1]])\n","Target Token:  tensor([[3084]])\n","Input_ids:  tensor([[    2, 45152,   133,  4681,  1139,    21, 23594, 12619,    42, 33813,\n","         10490,     8,   171, 30981,    32,   202,   685,   874,     5,  4084,\n","             9,     5,  2297,  2827, 49463,   864,    35, 25522,   133,  1139,\n","            34,   117,   750, 12213,    24, 49463,  3216,    50,   440,   116,\n","          1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n","Item Number:  14 experiment#:  tensor([[7]])\n","DETOKENIZE:  [\"</s>{Once on top you can see the ruins of Herod's once-magnificent palace, the ramp that sealed the Zealots' fate, and, way down below, the sketchy remains of the Roman siege camps.} question: {You can see the ruins of Herod's palace from the top of this site.  } Yes or No? answer: Ġ\"]\n","Labels:  tensor([[0]])\n","Target Token:  tensor([[9904]])\n","Input_ids:  tensor([[    2, 45152, 11475,    15,   299,    47,    64,   192,     5, 24757,\n","             9, 45637,    18,   683,    12,   119, 11244, 43160, 15653,     6,\n","             5,  6854,    14, 10497,     5, 39760,  5992,   108,  7658,     6,\n","             8,     6,   169,   159,   874,     6,     5, 15923,   219,  1189,\n","             9,     5,  7733, 19951,  7376, 49463,   864,    35, 25522,  1185,\n","            64,   192,     5, 24757,     9, 45637,    18, 15653,    31,     5,\n","           299,     9,    42,  1082,     4,  1437, 35524,  3216,    50,   440,\n","           116,  1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1]])\n","Item Number:  15 experiment#:  tensor([[7]])\n","DETOKENIZE:  [\"</s>{The neighborhood around the church has enough old-fashioned charm to retain something of the town's 19th-century pioneering atmosphere.} question: {The neighborhood surrounding the church has an old charm. } Yes or No? answer: Ġ\"]\n","Labels:  tensor([[0]])\n","Target Token:  tensor([[9904]])\n","Input_ids:  tensor([[    2, 45152,   133,  3757,   198,     5,  2352,    34,   615,   793,\n","            12, 22950, 16426,     7,  7615,   402,     9,     5,  1139,    18,\n","           753,   212,    12, 11046, 22653,  5466, 49463,   864,    35, 25522,\n","           133,  3757,  3817,     5,  2352,    34,    41,   793, 16426,     4,\n","         35524,  3216,    50,   440,   116,  1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1]])\n"]}]},{"cell_type":"markdown","source":["# IN-DOMAIN VALIDATION DATASET"],"metadata":{"id":"4gxoA3Lop7hO"}},{"cell_type":"code","source":["\n","def create_combined_dataset_indomain_VALIDATION(val_dataset, num_expts=num_experiments):\n","    combined_dataset = []\n","\n","    for irep in range(num_expts):\n","      for val_ex in val_dataset:\n","\n","            if val_ex['label'] == 0:\n","              target_token = 9904\n","            else:\n","              target_token = 3084\n","\n","            combined_ex = {'text': '', 'label': torch.tensor(val_ex['label'], dtype=torch.long).to(device), 'exp': irep+1, 'target_token': torch.tensor(target_token, dtype=torch.long).to(device)}\n","\n","            combined_ex['text'] += val_ex['text']\n","\n","            # Append the new combined example to the combined dataset\n","            combined_dataset.append([combined_ex])\n","\n","    return combined_dataset\n"],"metadata":{"id":"wc31z8yZUQWm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["formatted_val_dataset_indomain = val_dataset_indomain.map(format_examples)\n","\n","combined_dataset_INDOMAIN_VALIDATION = create_combined_dataset_indomain_VALIDATION(\n","                                          val_dataset = formatted_val_dataset_indomain,\n","                                          num_expts=num_experiments\n","                                           )\n","\n","custom_dataset_indomain_validation = CustomDataset(combined_dataset_INDOMAIN_VALIDATION)\n","custom_dataset_indomain_val_experiment = CustomDataset([item for item in custom_dataset_indomain_validation if item[0]['exp'] == SEL_EXP_TRAIN_CD])\n","print(custom_dataset_indomain_val_experiment)\n","\n","# Last step, we create Dataloader passing the bx_size for inference/training (typically: 1, 4, 8, 16)\n","bx_size = bx_size # set it up at the beg of NB\n","dataloader_indomain_val_experiment = DataLoader(custom_dataset_indomain_val_experiment, batch_size=bx_size, collate_fn=dynamic_padding_collate_fn, shuffle=False) #shuffle=False for reproducibility"],"metadata":{"id":"7XG7PTIOrV1O","executionInfo":{"status":"ok","timestamp":1714490857314,"user_tz":240,"elapsed":994,"user":{"displayName":"Nitesh Agarwal","userId":"18032940909522888771"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"72111780-aa6e-4588-8940-28287dc4af4a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<__main__.CustomDataset object at 0x7ff44d81e7d0>\n"]}]},{"cell_type":"code","source":["\n","for i, batch in enumerate(dataloader_indomain_val_experiment):\n","    if i<20:\n","      print(\"Item Number: \", i, \"experiment#: \", batch['exp'])\n","      print(\"DETOKENIZE: \", OPT_tokenizer.batch_decode(batch['input_ids']))\n","      print(\"Labels: \", batch['label'])\n","      print(\"Target Token: \", batch['target_token'])\n","      print(\"Input_ids: \", batch['input_ids'])\n","      print(\"Attention_Mask: \", batch['attention_mask'])\n","    else:\n","      break"],"metadata":{"id":"oQBs1lgxsPhO","executionInfo":{"status":"ok","timestamp":1714490857314,"user_tz":240,"elapsed":4,"user":{"displayName":"Nitesh Agarwal","userId":"18032940909522888771"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"60fb31c6-d55a-48dd-b031-1ce7fc0b8e9a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Item Number:  0 experiment#:  tensor([[7]])\n","DETOKENIZE:  [\"</s>{no it didn't} question: {Yes it did.} Yes or No? answer: Ġ\"]\n","Labels:  tensor([[1]])\n","Target Token:  tensor([[3084]])\n","Input_ids:  tensor([[    2, 45152,  2362,    24,   399,    75, 24303,   864,    35, 25522,\n","          9904,    24,   222, 49463,  3216,    50,   440,   116,  1948,    35,\n","          4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n","Item Number:  1 experiment#:  tensor([[7]])\n","DETOKENIZE:  [\"</s>{Who? asked Tommy.} question: {Tommy didn't know, who.} Yes or No? answer: Ġ\"]\n","Labels:  tensor([[0]])\n","Target Token:  tensor([[9904]])\n","Input_ids:  tensor([[    2, 45152, 12375,   116,   553,  8880, 49463,   864,    35, 25522,\n","         15691,  4783,   399,    75,   216,     6,    54, 49463,  3216,    50,\n","           440,   116,  1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1]])\n","Item Number:  2 experiment#:  tensor([[7]])\n","DETOKENIZE:  ['</s>{Paroseas cave, reef, and wreck diving around its shores, giving the diver a wide range of environments to explore.} question: {The diver has no variety in places to explore, they are monotonous. } Yes or No? answer: Ġ']\n","Labels:  tensor([[1]])\n","Target Token:  tensor([[3084]])\n","Input_ids:  tensor([[    2, 45152, 22011,  3876,   281, 12742,     6, 28350,     6,     8,\n","         15107, 12909,   198,    63, 20597,     6,  1311,     5, 13105,    10,\n","          1810,  1186,     9, 11534,     7,  5393, 49463,   864,    35, 25522,\n","           133, 13105,    34,   117,  3143,    11,  2127,     7,  5393,     6,\n","            51,    32,  6154, 27334,  1827,     4, 35524,  3216,    50,   440,\n","           116,  1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1]])\n","Item Number:  3 experiment#:  tensor([[7]])\n","DETOKENIZE:  [\"</s>{um i've visited the Wyoming area i'm not sure exactly where Dances with Wolves was filmed} question: {I don't know even though I visited the area.} Yes or No? answer: Ġ\"]\n","Labels:  tensor([[0]])\n","Target Token:  tensor([[9904]])\n","Input_ids:  tensor([[    2, 45152,   783,   939,   348,  3790,     5, 11027,   443,   939,\n","           437,    45,   686,  2230,   147,   211,  5332,    19, 13889,    21,\n","         10571, 24303,   864,    35, 25522,   100,   218,    75,   216,   190,\n","           600,    38,  3790,     5,   443, 49463,  3216,    50,   440,   116,\n","          1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n","Item Number:  4 experiment#:  tensor([[7]])\n","DETOKENIZE:  [\"</s>{i think Buffalo is an up an coming team they're going to they're showing some real promise for the next uh few years} question: {Buffalo is showing some real promise for the next few years, I think they are an up and coming team.} Yes or No? answer: Ġ\"]\n","Labels:  tensor([[0]])\n","Target Token:  tensor([[9904]])\n","Input_ids:  tensor([[    2, 45152,   118,   206,  5958,    16,    41,    62,    41,   567,\n","           165,    51,   214,   164,     7,    51,   214,  2018,   103,   588,\n","          4198,    13,     5,   220, 37463,   367,   107, 24303,   864,    35,\n","         25522, 42021,  7747,    16,  2018,   103,   588,  4198,    13,     5,\n","           220,   367,   107,     6,    38,   206,    51,    32,    41,    62,\n","             8,   567,   165, 49463,  3216,    50,   440,   116,  1948,    35,\n","          4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n","Item Number:  5 experiment#:  tensor([[7]])\n","DETOKENIZE:  ['</s>{They did this to us.} question: {This was done by them.} Yes or No? answer: Ġ']\n","Labels:  tensor([[0]])\n","Target Token:  tensor([[9904]])\n","Input_ids:  tensor([[    2, 45152,  1213,   222,    42,     7,   201, 49463,   864,    35,\n","         25522,   713,    21,   626,    30,   106, 49463,  3216,    50,   440,\n","           116,  1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1]])\n","Item Number:  6 experiment#:  tensor([[7]])\n","DETOKENIZE:  ['</s>{uh-huh how about any matching programs} question: {What about matching programs? } Yes or No? answer: Ġ']\n","Labels:  tensor([[0]])\n","Target Token:  tensor([[9904]])\n","Input_ids:  tensor([[    2, 45152,  2957,    12,   298,  2957,   141,    59,   143,  8150,\n","          1767, 24303,   864,    35, 25522,  2264,    59,  8150,  1767,   116,\n","         35524,  3216,    50,   440,   116,  1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1]])\n","Item Number:  7 experiment#:  tensor([[7]])\n","DETOKENIZE:  ['</s>{MC2000-2, was initially considered and recommended by the Commission under the market test rules.} question: {MC2000-2 was recommended by the Commission.} Yes or No? answer: Ġ']\n","Labels:  tensor([[0]])\n","Target Token:  tensor([[9904]])\n","Input_ids:  tensor([[    2, 45152,  6018, 17472,    12,   176,     6,    21,  3225,  1687,\n","             8,  5131,    30,     5,  1463,   223,     5,   210,  1296,  1492,\n","         49463,   864,    35, 25522,  6018, 17472,    12,   176,    21,  5131,\n","            30,     5,  1463, 49463,  3216,    50,   440,   116,  1948,    35,\n","          4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n","Item Number:  8 experiment#:  tensor([[7]])\n","DETOKENIZE:  ['</s>{Asked about abortion the other day on CNN, Republican National Committee Chairman Jim Nicholson also invoked what is apparently the party-line  inclusive party.} question: {The Republican National Committee Chairman freelanced on the topic of abortion when asked about it on CNN instead of reiterating the party-line.} Yes or No? answer: Ġ']\n","Labels:  tensor([[1]])\n","Target Token:  tensor([[3084]])\n","Input_ids:  tensor([[    2, 45152, 46688,    59,  6428,     5,    97,   183,    15,  3480,\n","             6,  1172,   496,  1674,  3356,  2488, 19408,    67, 29198,    99,\n","            16,  4100,     5,   537,    12,  1902,  1437, 10510,   537, 49463,\n","           864,    35, 25522,   133,  1172,   496,  1674,  3356, 30270, 16325,\n","            15,     5,  5674,     9,  6428,    77,   553,    59,    24,    15,\n","          3480,  1386,     9, 26209,  1295,     5,   537,    12,  1902, 49463,\n","          3216,    50,   440,   116,  1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n","Item Number:  9 experiment#:  tensor([[7]])\n","DETOKENIZE:  ['</s>{It was like looking into a mirror, except infinitely more realistic.} question: {It was more realistic than looking in a mirror. } Yes or No? answer: Ġ']\n","Labels:  tensor([[0]])\n","Target Token:  tensor([[9904]])\n","Input_ids:  tensor([[    2, 45152,   243,    21,   101,   546,    88,    10,  9807,     6,\n","          4682, 40489,    55, 10556, 49463,   864,    35, 25522,   243,    21,\n","            55, 10556,    87,   546,    11,    10,  9807,     4, 35524,  3216,\n","            50,   440,   116,  1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n","Item Number:  10 experiment#:  tensor([[7]])\n","DETOKENIZE:  ['</s>{John Kasich dropped his presidential bid.} question: {John Kasich recommitted himself to the presidential bid and plans on winning.} Yes or No? answer: Ġ']\n","Labels:  tensor([[1]])\n","Target Token:  tensor([[3084]])\n","Input_ids:  tensor([[    2, 45152, 10567, 26004,  1882,    39,  1939,  2311, 49463,   864,\n","            35, 25522, 10567, 26004, 37573, 16430,  1003,     7,     5,  1939,\n","          2311,     8,   708,    15,  1298, 49463,  3216,    50,   440,   116,\n","          1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n","Item Number:  11 experiment#:  tensor([[7]])\n","DETOKENIZE:  [\"</s>{well that's good that's great} question: {Shit, that is bad, that is horrible.} Yes or No? answer: Ġ\"]\n","Labels:  tensor([[1]])\n","Target Token:  tensor([[3084]])\n","Input_ids:  tensor([[    2, 45152,  3056,    14,    18,   205,    14,    18,   372, 24303,\n","           864,    35, 25522,  3609,   405,     6,    14,    16,  1099,     6,\n","            14,    16, 11385, 49463,  3216,    50,   440,   116,  1948,    35,\n","          4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1]])\n","Item Number:  12 experiment#:  tensor([[7]])\n","DETOKENIZE:  ['</s>{There were beads of perspiration on his brow.} question: {He was perfectly calm and dry as he waited.} Yes or No? answer: Ġ']\n","Labels:  tensor([[1]])\n","Target Token:  tensor([[3084]])\n","Input_ids:  tensor([[    2, 45152,   970,    58, 35036,     9, 20187, 41678,    15,    39,\n","         27423, 49463,   864,    35, 25522,   894,    21,  6683,  6327,     8,\n","          3841,    25,    37,  9010, 49463,  3216,    50,   440,   116,  1948,\n","            35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1]])\n","Item Number:  13 experiment#:  tensor([[7]])\n","DETOKENIZE:  ['</s>{Even the most aged and infirm travel here to die, for nothing is more blessed for a devout Hindu than to die in the great waters of the Varanasi and thus be released from the eternal cycle of rebirth.} question: {Devout Hindus believe that dying in the Varanasi frees a soul from the cycle of rebirth.} Yes or No? answer: Ġ']\n","Labels:  tensor([[0]])\n","Target Token:  tensor([[9904]])\n","Input_ids:  tensor([[    2, 45152,  8170,     5,   144,  5180,     8,  4047,  9856,  1504,\n","           259,     7,  1597,     6,    13,  1085,    16,    55, 12230,    13,\n","            10, 36906, 12316,    87,     7,  1597,    11,     5,   372,  5794,\n","             9,     5,  9676, 16264,   118,     8,  4634,    28,   703,    31,\n","             5, 25023,  4943,     9, 39652, 49463,   864,    35, 25522, 30504,\n","           995, 30618,   679,    14,  8180,    11,     5,  9676, 16264,   118,\n","          7619,   293,    10,  7047,    31,     5,  4943,     9, 39652, 49463,\n","          3216,    50,   440,   116,  1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1]])\n","Item Number:  14 experiment#:  tensor([[7]])\n","DETOKENIZE:  ['</s>{You and your friends are not welcome here, said Severn.} question: {Severn said the people were always welcome there.} Yes or No? answer: Ġ']\n","Labels:  tensor([[1]])\n","Target Token:  tensor([[3084]])\n","Input_ids:  tensor([[    2, 45152,  1185,     8,   110,   964,    32,    45,  2814,   259,\n","             6,    26,  1608, 12170, 49463,   864,    35, 25522, 14696, 12170,\n","            26,     5,    82,    58,   460,  2814,    89, 49463,  3216,    50,\n","           440,   116,  1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n","Item Number:  15 experiment#:  tensor([[7]])\n","DETOKENIZE:  ['</s>{Current Chinese leaders have distinctive characteristics that give them significant advantages over the United States in foreign policy.} question: {The us has advantages over China in foreign policy. } Yes or No? answer: Ġ']\n","Labels:  tensor([[1]])\n","Target Token:  tensor([[3084]])\n","Input_ids:  tensor([[    2, 45152, 42124,  1111,   917,    33, 16141, 12720,    14,   492,\n","           106,  1233, 12340,    81,     5,   315,   532,    11,  1093,   714,\n","         49463,   864,    35, 25522,   133,   201,    34, 12340,    81,   436,\n","            11,  1093,   714,     4, 35524,  3216,    50,   440,   116,  1948,\n","            35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n","Item Number:  16 experiment#:  tensor([[7]])\n","DETOKENIZE:  [\"</s>{The Chinese calendar was used to calculate the year of Japan's foundation by counting back the 1,260 years of the Chinese cosmological cycle.} question: {Japan's foundation was determined by using the Chinese calendar.} Yes or No? answer: Ġ\"]\n","Labels:  tensor([[0]])\n","Target Token:  tensor([[9904]])\n","Input_ids:  tensor([[    2, 45152,   133,  1111,  7127,    21,   341,     7, 15756,     5,\n","            76,     9,  1429,    18,  4811,    30, 10581,   124,     5,   112,\n","             6, 21566,   107,     9,     5,  1111, 12793,   119,  9779,  4943,\n","         49463,   864,    35, 25522, 21318,    18,  4811,    21,  3030,    30,\n","           634,     5,  1111,  7127, 49463,  3216,    50,   440,   116,  1948,\n","            35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1]])\n","Item Number:  17 experiment#:  tensor([[7]])\n","DETOKENIZE:  [\"</s>{Two clues in the Pennsylvania  1) The boy had said, I'm going to go to the dinner dance and kill some people.} question: {There was only one clue in Pennsylvania and it had nothing to do with the boy.} Yes or No? answer: Ġ\"]\n","Labels:  tensor([[1]])\n","Target Token:  tensor([[3084]])\n","Input_ids:  tensor([[    2, 45152,  9058, 14885,    11,     5,  4367,  1437,   112,    43,\n","            20,  2143,    56,    26,     6,    38,   437,   164,     7,   213,\n","             7,     5,  3630,  3836,     8,  3549,   103,    82, 49463,   864,\n","            35, 25522,   970,    21,   129,    65, 18664,    11,  4367,     8,\n","            24,    56,  1085,     7,   109,    19,     5,  2143, 49463,  3216,\n","            50,   440,   116,  1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1]])\n","Item Number:  18 experiment#:  tensor([[7]])\n","DETOKENIZE:  ['</s>{8 A stoichiometry of 1.03 is typical when the FGD process is producing gypsum by-product, while a stoichiometry of 1.05 is needed to produce waste suitable for a landfill.} question: {A stoichiometry of 1.07 is typical when the FGD process is producing gypsum by-product} Yes or No? answer: Ġ']\n","Labels:  tensor([[1]])\n","Target Token:  tensor([[3084]])\n","Input_ids:  tensor([[    2, 45152,   398,    83, 20572, 20000, 40899,     9,   112,     4,\n","          3933,    16,  6097,    77,     5, 25408,   495,   609,    16,  5591,\n","         18124,  3275,   783,    30,    12, 20565,     6,   150,    10, 20572,\n","         20000, 40899,     9,   112,     4,  2546,    16,   956,     7,  2592,\n","          3844, 10686,    13,    10, 21289, 49463,   864,    35, 25522,   250,\n","         20572, 20000, 40899,     9,   112,     4,  3570,    16,  6097,    77,\n","             5, 25408,   495,   609,    16,  5591, 18124,  3275,   783,    30,\n","            12, 20565, 24303,  3216,    50,   440,   116,  1948,    35,  4236,\n","         21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1]])\n","Item Number:  19 experiment#:  tensor([[7]])\n","DETOKENIZE:  [\"</s>{oh yes yeah yeah yeah that's true too that's true} question: {It is true} Yes or No? answer: Ġ\"]\n","Labels:  tensor([[0]])\n","Target Token:  tensor([[9904]])\n","Input_ids:  tensor([[    2, 45152,  2678,  4420, 11380, 11380, 11380,    14,    18,  1528,\n","           350,    14,    18,  1528, 24303,   864,    35, 25522,   243,    16,\n","          1528, 24303,  3216,    50,   440,   116,  1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1]])\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qCU9xUpVx1ei"},"outputs":[],"source":["# Custom model class for sequence classification\n","\n","import torch.nn as nn\n","from transformers import AutoModelForSequenceClassification, AutoTokenizer\n","\n","class OPT_VanillaFT(nn.Module):\n","    def __init__(self, model_name, num_labels):\n","        super().__init__()\n","        self.model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n","\n","    def forward(self, input_ids, attention_mask=None, labels=None):\n","        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","        logits = outputs.logits\n","        return logits"]},{"cell_type":"code","source":["# Define optimizer and loss function\n","model = OPT_VanillaFT(model_name, num_labels=2)\n","# print(model)\n","optimizer = AdamW(model.parameters(), lr=1e-6)\n","criterion = nn.CrossEntropyLoss()\n","\n","# Training loop\n","model.to(device)\n","model.train()\n","\n","train_losses = []\n","val_losses = []\n","train_accuracies = []\n","val_accuracies = []\n","\n","num_epochs = 6\n","batch_size = 1  # Reduce batch size to 4\n","for epoch in range(num_epochs):\n","    print(\"epoch: \", epoch)\n","    total_loss = 0.0\n","    train_acc = 0.0\n","    for i, batch in enumerate(dataloader_experiment):\n","\n","        # print(\"input_ids: \", batch[\"input_ids\"])\n","\n","\n","        input_ids = batch[\"input_ids\"].to(device)\n","        attention_mask = batch[\"attention_mask\"].to(device)\n","        labels = batch[\"label\"].to(device).squeeze(0)\n","\n","\n","        # print(\"labels: \", batch[\"label\"], labels.shape)\n","        optimizer.zero_grad()\n","\n","        # Forward pass\n","        outputs = model(input_ids, None, labels) #, labels=labels)\n","        labels = torch.nn.functional.one_hot(labels, num_classes=2).float()\n","        # print(\"outputs SHAPE: \", outputs.shape)\n","        # print(outputs)\n","        # # Compute loss\n","        loss = criterion(outputs, labels)\n","        # print(\"loss: \", loss)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","\n","        # Compute accuracy\n","        train_acc = train_acc + 1*(torch.argmax(outputs, dim=1).item()==batch[\"label\"].to(device).squeeze(0).item())\n","\n","    average_loss = total_loss / (i+1)\n","    train_losses.append(average_loss)\n","    train_accuracies.append(train_acc / (i+1))\n","    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {average_loss:.6f}, Accuracy: {train_accuracies[-1]:.9f}\")\n","\n","    # val_losses.append(val_loss / num_validations)\n","    # val_accuracies.append(val_accuracy / num_validations)\n","\n","    # print(f\"Validation Loss: {val_losses[-1]:.6f}, Validation Accuracy: {val_accuracies[-1]:.9f}\")\n","\n","    # model.train()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yKFssGNxx40D","outputId":"f85c0e55-374c-48f0-fda1-0e7cec88df20","executionInfo":{"status":"ok","timestamp":1714491581231,"user_tz":240,"elapsed":723670,"user":{"displayName":"Nitesh Agarwal","userId":"18032940909522888771"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of OPTForSequenceClassification were not initialized from the model checkpoint at facebook/opt-2.7b and are newly initialized: ['score.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["epoch:  0\n","Epoch 1/6, Train Loss: 0.871126, Accuracy: 0.500000000\n","epoch:  1\n","Epoch 2/6, Train Loss: 0.597541, Accuracy: 0.687500000\n","epoch:  2\n","Epoch 3/6, Train Loss: 0.306443, Accuracy: 1.000000000\n","epoch:  3\n","Epoch 4/6, Train Loss: 0.299125, Accuracy: 1.000000000\n","epoch:  4\n","Epoch 5/6, Train Loss: 0.143083, Accuracy: 1.000000000\n","epoch:  5\n","Epoch 6/6, Train Loss: 0.073460, Accuracy: 1.000000000\n"]}]},{"cell_type":"code","source":["# Validation\n","model.eval()\n","val_loss = 0.0\n","val_accuracy = 0.0\n","with torch.no_grad():\n","    for batch in dataloader_indomain_val_experiment:\n","        src = batch[\"input_ids\"].to(device)\n","        attention_mask = batch[\"attention_mask\"].to(device)\n","        labels = batch[\"label\"].to(device).squeeze(0)\n","\n","        # Forward pass\n","        outputs = model(input_ids, None, labels)\n","\n","        # Compute loss\n","        # if 'logits' in outputs:\n","        loss = criterion(outputs, labels)\n","        val_loss += loss.item()\n","\n","        # Compute accuracy\n","        val_accuracy += 1*(torch.argmax(outputs, dim=1).item()==batch[\"label\"].to(device).squeeze(0).item())\n","    val_loss = val_loss/num_validations\n","    val_accuracy = val_accuracy/num_validations\n","    print(f\"Validation Loss: {val_loss:.6f}, Validation Accuracy: {val_accuracy:.9f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sGIvEE4MZPMw","executionInfo":{"status":"ok","timestamp":1714491581232,"user_tz":240,"elapsed":15,"user":{"displayName":"Nitesh Agarwal","userId":"18032940909522888771"}},"outputId":"007fb829-a10e-4aab-d875-5cf202745407"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Validation Loss: 1.454523, Validation Accuracy: 0.574520180\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i7DXLZYPedey","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714491583285,"user_tz":240,"elapsed":2065,"user":{"displayName":"Nitesh Agarwal","userId":"18032940909522888771"}},"outputId":"3e7487c9-9bf4-4530-cbce-f1b39216fe3a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['premise', 'hypothesis', 'label', 'parse_premise', 'parse_hypothesis', 'binary_parse_premise', 'binary_parse_hypothesis', 'heuristic', 'subcase', 'template'],\n","        num_rows: 30000\n","    })\n","    validation: Dataset({\n","        features: ['premise', 'hypothesis', 'label', 'parse_premise', 'parse_hypothesis', 'binary_parse_premise', 'binary_parse_hypothesis', 'heuristic', 'subcase', 'template'],\n","        num_rows: 30000\n","    })\n","})"]},"metadata":{},"execution_count":28}],"source":["dataset_ood = load_dataset(\"hans\")\n","dataset_ood"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oAFcefWkwmPs"},"outputs":[],"source":["dataset_ood_val = (dataset_ood['validation']).filter(lambda example: example[\"heuristic\"] == 'lexical_overlap')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jn4ufgMzwmNS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714491583285,"user_tz":240,"elapsed":6,"user":{"displayName":"Nitesh Agarwal","userId":"18032940909522888771"}},"outputId":"cca60589-4f59-47e8-9f9b-7a2561054ccc"},"outputs":[{"output_type":"stream","name":"stdout","text":["indices_ood_val:  [6252 4684 1731 ... 9410 1671  474]\n"]},{"output_type":"execute_result","data":{"text/plain":["Dataset({\n","    features: ['premise', 'hypothesis', 'label', 'parse_premise', 'parse_hypothesis', 'binary_parse_premise', 'binary_parse_hypothesis', 'heuristic', 'subcase', 'template'],\n","    num_rows: 1024\n","})"]},"metadata":{},"execution_count":30}],"source":["# Perform the filters and splits from the original datasets\n","\n","\n","random_split_seed_ood = 42 # set above, equal to 42\n","\n","examples_per_exp =  examples_per_exp # 16\n","num_experiments = num_experiments # 10\n","num_validations = num_validations # 16*64 #64*16 = 1024 #6692\n","\n","np.random.seed(random_split_seed_ood)\n","indices_ood_val = np.random.choice(range(len(dataset_ood_val)), size=num_validations, replace=False)\n","print(\"indices_ood_val: \", indices_ood_val)\n","\n","dataset_ood_val_sel = dataset_ood_val.select(indices_ood_val)\n","dataset_ood_val_sel"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3aAqJWOqwmKR"},"outputs":[],"source":["# format examples functions formats according to different types of formats for ICL both training and validation examples\n","\n","# select format to use here:\n","format_train_val = format_train_val # set it at the top of notebook in a common place\n","\n","\n","def format_examples_validation_VALOOD(example_val, format_val = format_train_val):\n","    if format_val== 'minimal':\n","      # \"minimal\" format\n","      return {'text': \"{\"  + example_val['premise'] + \"} {\" + example_val['hypothesis'] + \"}\" + \" ? Ġ\"}\n","    elif format_val== 'gpt3':\n","      # \"minimal\" format\n","      return {'text': \"{\"  + example_val['premise'] + \"} question: {\" + example_val['hypothesis'] + \"}\" + \" Yes or No? answer: Ġ\"}\n","\n","def create_combined_dataset(val_dataset, num_expts=num_experiments):\n","    combined_dataset = []\n","\n","    for irep in range(num_expts):\n","      for val_ex in val_dataset:\n","\n","        combined_ex = {'text': '', 'label': torch.tensor(val_ex['label'], dtype=torch.long).to(device), 'exp': irep+1}\n","\n","        combined_ex['text'] += val_ex['text']\n","\n","        combined_dataset.append([combined_ex])\n","\n","    return combined_dataset\n","\n","\n","def dynamic_padding_collate_fn_VALOOD(batch):\n","    # This function is created to be able to tokenize dynamically to max length within each batch\n","    # Also, by modifying the tokenizer used, several other options are available\n","    # for example, if we set padding to a specified max_length, for example the model max_length, is also an option, not the default though\n","    # the default is the dynamic padding\n","\n","    batch = [item for sublist in batch for item in sublist]\n","\n","    texts = [item['text'] for item in batch]\n","    labels = [item['label'] for item in batch]\n","    exps = [item['exp'] for item in batch]\n","\n","    # choose option\n","    tokenized_inputs = OPT_tokenizer(texts, padding=\"longest\", truncation=True, return_tensors=\"pt\")\n","\n","    labels_tensor = torch.unsqueeze(torch.tensor(labels, dtype=torch.long).to(device),0)\n","    exps_tensor = torch.unsqueeze(torch.tensor(exps, dtype=torch.long).to(device),0)\n","\n","    return {\n","        'text': texts,\n","        'input_ids': tokenized_inputs['input_ids'],\n","        'attention_mask': tokenized_inputs['attention_mask'],\n","        'label': labels_tensor,\n","        'exp': exps_tensor,\n","    }\n","\n","\n","class CustomDataset(Dataset):\n","    def __init__(self, combined_dataset):\n","        self.dataset = combined_dataset\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx):\n","        return self.dataset[idx]\n"]},{"cell_type":"code","source":["# First the samples are formatted according to selection above\n","# Important to check selection and re-run cell above so that it is taken by the mapping function correctly\n","\n","formatted_val_dataset_ood = dataset_ood_val_sel.map(format_examples_validation_VALOOD)\n","\n","# Initialize custom dataset with the combined dataset\n","# print result to check correctness\n","\n","combined_dataset_VALOOD = create_combined_dataset(\n","                                          val_dataset = formatted_val_dataset_ood,\n","                                          num_expts=num_experiments\n","                                           )\n","\n","custom_dataset_VALOOD = CustomDataset(combined_dataset_VALOOD)\n","print(custom_dataset_VALOOD)\n","\n","custom_dataset_VALOOD_EXP = CustomDataset([item for item in custom_dataset_VALOOD if item[0]['exp'] == SEL_EXP_TRAIN_CD])\n","\n","# Last step, we create Dataloader passing the bx_size for inference (typically: 1, 4, 8, 16)\n","bx_size = bx_size # set it up at the beg of NB\n","dataloader_VALOOD = DataLoader(custom_dataset_VALOOD_EXP, batch_size=bx_size, collate_fn=dynamic_padding_collate_fn_VALOOD, shuffle=False) #shuffle=False for reproducibility"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2u9wEYxr2OLD","executionInfo":{"status":"ok","timestamp":1714491584589,"user_tz":240,"elapsed":1307,"user":{"displayName":"Nitesh Agarwal","userId":"18032940909522888771"}},"outputId":"2972a512-864f-41bd-e67f-b02bcb1e4333"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<__main__.CustomDataset object at 0x7ff303c203a0>\n"]}]},{"cell_type":"code","source":["# Validation\n","model.eval()\n","val_loss = 0.0\n","val_accuracy = 0.0\n","with torch.no_grad():\n","    for batch in dataloader_VALOOD:\n","        # print(batch)\n","\n","        src = batch[\"input_ids\"].to(device)\n","        attention_mask = batch[\"attention_mask\"].to(device)\n","        labels = batch[\"label\"].to(device).squeeze(0)\n","\n","        # Forward pass\n","        outputs = model(input_ids, None, labels)\n","\n","        # Compute loss\n","        # if 'logits' in outputs:\n","        loss = criterion(outputs, labels)\n","        val_loss += loss.item()\n","        # break\n","\n","        # Compute accuracy\n","        val_accuracy += 1*(torch.argmax(outputs, dim=1).item()==batch[\"label\"].to(device).squeeze(0).item())\n","    val_loss = val_loss/num_validations\n","    val_accuracy = val_accuracy/num_validations\n","    print(f\"Validation Loss: {val_loss:.6f}, Validation Accuracy: {val_accuracy:.9f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wBWNm-Cbx72J","executionInfo":{"status":"ok","timestamp":1714491584590,"user_tz":240,"elapsed":14,"user":{"displayName":"Nitesh Agarwal","userId":"18032940909522888771"}},"outputId":"33a923ef-57f6-47f5-f13b-bbd5aea74f9f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Validation Loss: 1.537254 , Validation Accuracy: 0.507824761\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"yu95zGyZrS9E"},"execution_count":null,"outputs":[]}]}