{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1RPTGv1rNwiZWNDRpNChSIHwZZeI0OtDz","timestamp":1714490950721},{"file_id":"1BceVUAvDAUht306cAQTJRRapAJRYummn","timestamp":1714488974227},{"file_id":"14Ns_9ncA9MimDZGTdbGAwO-d46xoeRVB","timestamp":1714461649212},{"file_id":"1ytKBpLWHGEZQNGuMGDJ_A9tpviXlMHQ3","timestamp":1714461631160},{"file_id":"1-WeRH2m8157msxCi4BWAFEMa6fmYdKHQ","timestamp":1714452733001},{"file_id":"1o7G7hBqY_teAY1dfBxFaPYfOS218mNZb","timestamp":1714201977972},{"file_id":"1-Eg41pChNrXtqPstHWgyHtI5d4QPSb7-","timestamp":1714174001163},{"file_id":"1uf_FvSnFW56aaBHVBm6yJQDBTn20lRBQ","timestamp":1713724435266}],"machine_shape":"hm","gpuType":"L4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# 1. IMPORT LIBRARIES"],"metadata":{"id":"Tw8CFJ12Pp6T"}},{"cell_type":"code","source":["#!pip install -q datasets accelerate\n","#!pip install -q git+https://github.com/huggingface/transformers.git@main\n","# !pip install -q git+https://github.com/huggingface/peft.git\n","# !pip install -q bitsandbytes datasets accelerate loralib\n","\n","\n","\n","#!pip install -q datasets accelerate\n","# !pip install -q git+https://github.com/huggingface/transformers.git@main\n","# !pip install -q git+https://github.com/huggingface/peft.git\n","# !pip install -q bitsandbytes datasets accelerate loralib\n","!pip install -q datasets\n","!pip install --upgrade transformers\n","!pip install tensorflow\n","\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","import gc\n","from torch.cuda.amp import autocast, GradScaler\n","\n","\n","from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, OPTForCausalLM, GPT2Tokenizer, AdamW, AutoModelForSequenceClassification\n","\n","!pip install -q accelerate\n","\n","# !pip install -q git+https://github.com/huggingface/transformers.git@main\n","# !pip install -q git+https://github.com/huggingface/peft.git\n","# !pip install -q bitsandbytes datasets accelerate loralib\n","\n","\n","import matplotlib.pyplot as plt\n","import pandas as pd\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2ayaqVMRAXaT","outputId":"e7acba77-d91d-438b-8475-319928be8f55","executionInfo":{"status":"ok","timestamp":1714490143208,"user_tz":240,"elapsed":25334,"user":{"displayName":"Nitesh Agarwal","userId":"18032940909522888771"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.22.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n","Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.11.0)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.62.2)\n","Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n","Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n","Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n","Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.2)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"]}]},{"cell_type":"code","source":["\n","!python --version\n","!nvcc --version\n","!pip install nvcc4jupyter\n","%load_ext nvcc4jupyter\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mY6pDshTHDtg","outputId":"7bbf666c-72fa-4f9e-8d21-57c74ee44759","executionInfo":{"status":"ok","timestamp":1714490148627,"user_tz":240,"elapsed":5422,"user":{"displayName":"Nitesh Agarwal","userId":"18032940909522888771"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Python 3.10.12\n","nvcc: NVIDIA (R) Cuda compiler driver\n","Copyright (c) 2005-2023 NVIDIA Corporation\n","Built on Tue_Aug_15_22:02:13_PDT_2023\n","Cuda compilation tools, release 12.2, V12.2.140\n","Build cuda_12.2.r12.2/compiler.33191640_0\n","Requirement already satisfied: nvcc4jupyter in /usr/local/lib/python3.10/dist-packages (1.2.1)\n","Detected platform \"Colab\". Running its setup...\n","Source files will be saved in \"/tmp/tmp8yvogvk_\".\n"]}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import random"],"metadata":{"id":"hOBeMp00RKSi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2. SET MAIN INPUTS FOR NOTEBOOK"],"metadata":{"id":"gBZINjFskJOT"}},{"cell_type":"code","source":["bx_size = 1                                 # batch size for inference OR TRAIN with the OPT\n","format_train_val = 'gpt3'                   # 'minimal' or 'gpt3\n","task_name = 'mnli'                          # 'mnli'\n","model_name = \"facebook/opt-2.7b\"            # model options below\n","examples_per_exp =  16                       # 16\n","num_experiments = 10                         # 10\n","num_validations = 1024   # not used yet in this NB (when later on doing validation needs to be specified at 1024)\n","\n","SEL_EXP_TRAIN_CD = 9                        # Select experiment to run\n","\n","\n","# model_name = \"facebook/opt-125m\"\n","# model_name = \"facebook/opt-350m\"\n","# model_name = \"facebook/opt-1.3b\"\n","# model_name = \"facebook/opt-2.7b\"\n","# model_name = \"facebook/opt-6.7b\""],"metadata":{"id":"tvPGy9EnkIKK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2. SET DEVICE"],"metadata":{"id":"ZBgRakKbCXl3"}},{"cell_type":"code","source":["import os\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""],"metadata":{"id":"aKf7vOw2MTPk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","\n","# Check if CUDA (GPU support) is available\n","cuda_available = torch.cuda.is_available()\n","print(f\"CUDA Available: {cuda_available}\")\n","\n","# If CUDA is available, print the GPU name(s)\n","if cuda_available:\n","    print(f\"GPU Name(s): {torch.cuda.get_device_name(0)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"18useFCOLdX2","outputId":"294e5ec6-4bbe-42fd-b4e5-dc74489ba3ad","executionInfo":{"status":"ok","timestamp":1714490148628,"user_tz":240,"elapsed":9,"user":{"displayName":"Nitesh Agarwal","userId":"18032940909522888771"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["CUDA Available: True\n","GPU Name(s): NVIDIA L4\n"]}]},{"cell_type":"code","source":["device = \"cpu\"\n","\n","# device_count = torch.cuda.device_count()\n","# if device_count > 0:\n","#     print(\"Select GPU device\")\n","#     device = torch.device(\"cuda\")\n","# else:\n","#     print(\"Select GPU device\")\n","#     device = torch.device(\"cpu\")\n","\n","print(device)\n","# torch.cuda.is_available()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QXFJO33ECUVB","outputId":"c1f87c35-47e2-4c98-e830-baca8fcf646c","executionInfo":{"status":"ok","timestamp":1714490148628,"user_tz":240,"elapsed":7,"user":{"displayName":"Nitesh Agarwal","userId":"18032940909522888771"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cpu\n"]}]},{"cell_type":"markdown","source":["## 3. IMPORT TOKENIZER AND SELECT MODEL"],"metadata":{"id":"DjaLyC3l7hVV"}},{"cell_type":"code","source":["# Choose model to work with:\n","\n","# model_name = \"facebook/opt-125m\"\n","# model_name = \"facebook/opt-350m\"\n","# model_name = \"facebook/opt-1.3b\"\n","# model_name = \"facebook/opt-2.7b\"\n","# model_name = \"facebook/opt-6.7b\"\n","\n","model_name = model_name # it is set up in top of NB"],"metadata":{"id":"2Kt5rM4s7g0T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["OPT_tokenizer = GPT2Tokenizer.from_pretrained(model_name)"],"metadata":{"id":"g49c1NTt_rcO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4. IMPORT NLI DATASET FOR TRAINING AND VALIDATION: MNLI"],"metadata":{"id":"kMS1b9WqPZWC"}},{"cell_type":"code","source":["# reference: https://github.com/uds-lsv/llmft/blob/main/notebooks/majority_baseline.ipynb\n","# this reference is useful for cleaning the neutral sentences of the dataset, just keeping the 0 and 1."],"metadata":{"id":"kGFeGMISPx7D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from collections import Counter\n","from datasets import load_dataset, ClassLabel"],"metadata":{"id":"9bQUsgPp7g9m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# this comes from original paper, to remove neutral examples from MNLI\n","def binarize_mnli(dataset, remove_neutral=True):\n","    if remove_neutral:\n","        # neutral class has label 1\n","        dataset = dataset.filter(lambda example: example[\"label\"] != 1)\n","\n","    # change labels of contradiction examples from 2 to 1\n","    def change_label(example):\n","        # convert labels 2 into labels 1. this merges the neutral and contradiction class\n","        example[\"label\"] = 1 if example[\"label\"] == 2 else example[\"label\"]\n","        return example\n","\n","    # change labels\n","    dataset = dataset.map(change_label)\n","\n","    # change features to reflect the new labels\n","    features = dataset[\"train\"].features.copy()\n","    features[\"label\"] = ClassLabel(num_classes=2, names=['entailment', 'contradiction'], id=None)\n","    dataset = dataset.cast(features)  # overwrite old features\n","\n","    return dataset\n"],"metadata":{"id":"l1N1cM5z7m16"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = load_dataset(\"glue\", task_name)"],"metadata":{"id":"T10hQZj079Le"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# binarize dataset\n","if task_name == \"mnli\":\n","    dataset = binarize_mnli(dataset, remove_neutral=True) # mnli\n"],"metadata":{"id":"QI3oAEj279Hl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# analyze and visualize dataset imported\n","\n","print(\"task_name:\", task_name)\n","for split in [\"train\", \"validation_matched\"]:\n","    c = Counter(dataset[split][\"label\"])\n","    total = len(list(c.elements()))\n","    print(\"Total number of samples:\", total)\n","    print(split)\n","    for k in c:\n","        print(f\"fraction of labels per class: {k}={c[k] / total}\")\n","print(dataset)"],"metadata":{"id":"qIFzmEFR8GQ1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714490153273,"user_tz":240,"elapsed":241,"user":{"displayName":"Nitesh Agarwal","userId":"18032940909522888771"}},"outputId":"b481b9cf-67ea-4577-ade9-ac9d83473d2c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["task_name: mnli\n","Total number of samples: 261802\n","train\n","fraction of labels per class: 0=0.49999236063895613\n","fraction of labels per class: 1=0.5000076393610439\n","Total number of samples: 6692\n","validation_matched\n","fraction of labels per class: 1=0.4801255230125523\n","fraction of labels per class: 0=0.5198744769874477\n","DatasetDict({\n","    train: Dataset({\n","        features: ['premise', 'hypothesis', 'label', 'idx'],\n","        num_rows: 261802\n","    })\n","    validation_matched: Dataset({\n","        features: ['premise', 'hypothesis', 'label', 'idx'],\n","        num_rows: 6692\n","    })\n","    validation_mismatched: Dataset({\n","        features: ['premise', 'hypothesis', 'label', 'idx'],\n","        num_rows: 6703\n","    })\n","    test_matched: Dataset({\n","        features: ['premise', 'hypothesis', 'label', 'idx'],\n","        num_rows: 9796\n","    })\n","    test_mismatched: Dataset({\n","        features: ['premise', 'hypothesis', 'label', 'idx'],\n","        num_rows: 9847\n","    })\n","})\n"]}]},{"cell_type":"code","source":["# Perform the filters and splits from the original datasets\n","\n","\n","random_split_seed = 42\n","\n","examples_per_exp =  examples_per_exp # set above 16\n","num_experiments = num_experiments # set above 10\n","num_validations = num_validations # set above 16*64 #64*16 = 1024 #6692\n","\n","max_train_samples = examples_per_exp*num_experiments\n","train_dataset = dataset['train']\n","print(train_dataset)\n","\n","train_dataset_yes_all = dataset['train'].filter(lambda example: example[\"label\"] == 0)\n","train_dataset_no_all = dataset['train'].filter(lambda example: example[\"label\"] == 1)\n","print(train_dataset_yes_all)\n","print(train_dataset_no_all)\n","\n","val_dataset_all_indomain = dataset['validation_matched']\n","\n","# randomly select a subset of the training data\n","max_train_samples = min(len(train_dataset), max_train_samples)\n","\n","np.random.seed(random_split_seed)\n","indices_yes = np.random.choice(range(len(train_dataset_yes_all)), size=int(max_train_samples/2), replace=False)\n","print(\"indices_yes: \", indices_yes)\n","\n","np.random.seed(random_split_seed+1)\n","indices_no = np.random.choice(range(len(train_dataset_no_all)), size=int(max_train_samples/2), replace=False)\n","print(\"indices_no: \", indices_no)\n","\n","np.random.seed(random_split_seed+2)\n","indices_val_indomain = np.random.choice(range(len(val_dataset_all_indomain)), size=num_validations, replace=False)\n","print(\"indices_val: \", indices_val_indomain)"],"metadata":{"id":"7v9CdYUvSxGf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714490153273,"user_tz":240,"elapsed":6,"user":{"displayName":"Nitesh Agarwal","userId":"18032940909522888771"}},"outputId":"f03ce331-f8c6-4721-e70d-114798e4df85"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset({\n","    features: ['premise', 'hypothesis', 'label', 'idx'],\n","    num_rows: 261802\n","})\n","Dataset({\n","    features: ['premise', 'hypothesis', 'label', 'idx'],\n","    num_rows: 130899\n","})\n","Dataset({\n","    features: ['premise', 'hypothesis', 'label', 'idx'],\n","    num_rows: 130903\n","})\n","indices_yes:  [108195  86013  39482  39689  10288  11589  94511  78690  36953  74067\n","  93678  83921  83896  21665  76736    651  48482  40811 127490  49367\n"," 121664  39918  60933 126502  65765  12966  33438   7201  19815  49187\n","  29116  48565 125127  60274  33985 130032 104535 120345 104033  44914\n","  89806  87143 103906  15697  29521   4906  46884  75442  57625  32365\n","  70562  78463  18684  45639  30223 118624  40945  75797  63681  77117\n","  16126 130579   2132 113346  68080   7433 120366 122242  75493  64389\n","  95467  86480  52323  42308 101738  51386 126981  27346  45655 121440]\n","indices_no:  [ 54039  34647  34994 102702  14063 110662  33077  24477  24337  19083\n","  61263 109299 107760  88071  22063  90740 113958   9163  45235  32885\n","  58399  59560 102582  10964  38283  16146  72067  55788  60576  21220\n","  41478 123489  38278  15117  71374  69791  39777 122448  10098  35761\n","  74547 109598  19072  61567  56626 102957  18014  14118  46250 117891\n","  87958 113798 107148 121622  88599   8239 119796  69862   2704 112545\n"," 121565 111890  19129 115169  29330  47129  79077  34942  28934  12323\n","  85926 103422  91532  32522   4654 108738  24476  86650 117487  61013]\n","indices_val:  [1771 4591 1869 ... 1111  809 1914]\n"]}]},{"cell_type":"code","source":["train_dataset_yes = train_dataset_yes_all.select(indices_yes)\n","train_dataset_no = train_dataset_no_all.select(indices_no)\n","\n","val_dataset_indomain = val_dataset_all_indomain.select(indices_val_indomain)\n","print(\"Train Dataset Yes: \", train_dataset_yes)\n","print(\"Train Dataset No: \", train_dataset_no)\n","print(\"Validation Dataset (in-domain): \", val_dataset_indomain)"],"metadata":{"id":"W5nx1ugOr5kd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714490153274,"user_tz":240,"elapsed":5,"user":{"displayName":"Nitesh Agarwal","userId":"18032940909522888771"}},"outputId":"5a8f1bae-3ea8-4053-8480-68f298ee3797"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train Dataset Yes:  Dataset({\n","    features: ['premise', 'hypothesis', 'label', 'idx'],\n","    num_rows: 80\n","})\n","Train Dataset No:  Dataset({\n","    features: ['premise', 'hypothesis', 'label', 'idx'],\n","    num_rows: 80\n","})\n","Validation Dataset (in-domain):  Dataset({\n","    features: ['premise', 'hypothesis', 'label', 'idx'],\n","    num_rows: 1024\n","})\n"]}]},{"cell_type":"code","source":["# Calculate the number of 0 and 1 in validation dataset\n","# and calculate the majority class accuracy\n","\n","val_dataset_indomain_yes = val_dataset_indomain.filter(lambda example: example[\"label\"] == 0)\n","val_dataset_indomain_no = val_dataset_indomain.filter(lambda example: example[\"label\"] == 1)\n","print(val_dataset_indomain_yes)\n","print(val_dataset_indomain_no)\n","print(\"Majority Class Accuracy: \", 100*max(len(val_dataset_indomain_yes), len(val_dataset_indomain_no))/(len(val_dataset_indomain_yes) + len(val_dataset_indomain_no)))"],"metadata":{"id":"15sgsMLaVGwC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714490153274,"user_tz":240,"elapsed":4,"user":{"displayName":"Nitesh Agarwal","userId":"18032940909522888771"}},"outputId":"be46159a-c1ce-4ed0-9d99-238fd97dc959"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset({\n","    features: ['premise', 'hypothesis', 'label', 'idx'],\n","    num_rows: 536\n","})\n","Dataset({\n","    features: ['premise', 'hypothesis', 'label', 'idx'],\n","    num_rows: 488\n","})\n","Majority Class Accuracy:  52.34375\n"]}]},{"cell_type":"code","source":["\n","format_train_val = format_train_val # set it at the top of notebook in a common place\n","\n","def format_examples(example_val, format_val = format_train_val):\n","    if format_val== 'minimal':\n","      # \"minimal\" format\n","      return {'text': \"{\"  + example_val['premise'] + \"} {\" + example_val['hypothesis'] + \"}\" + \" ? Ġ\"}\n","    elif format_val== 'gpt3':\n","      # \"gpt3\" format\n","      return {'text': \"{\"  + example_val['premise'] + \"} question: {\" + example_val['hypothesis'] + \"}\" + \" Yes or No? answer: Ġ\"}\n","\n","def create_combined_dataset(train_ds_yes, train_ds_no, num_expts=num_experiments, num_train_examples=examples_per_exp):\n","    combined_dataset = []\n","    train_examples_yes = [example for example in train_ds_yes]\n","    train_examples_no = [example for example in train_ds_no]\n","\n","    for irep in range(num_expts):\n","          sampled_train_exs_yes = train_examples_yes[int(irep*num_train_examples/2) : int((irep +1)*num_train_examples/2)]\n","          sampled_train_exs_no = train_examples_no[int(irep*num_train_examples/2) : int((irep +1)*num_train_examples/2)]\n","          merged_sampled_train_exs = sampled_train_exs_yes + sampled_train_exs_no\n","          shuffled_list = merged_sampled_train_exs.copy()\n","          random.seed(irep)\n","          random.shuffle(shuffled_list)\n","\n","          for idx_shuffled_list in range(len(shuffled_list)):\n","\n","            if shuffled_list[idx_shuffled_list]['label'] == 0:\n","              target_token = 9904\n","            else:\n","              target_token = 3084\n","\n","            combined_ex = {'text': '', 'label': torch.tensor(shuffled_list[idx_shuffled_list]['label'], dtype=torch.long).to(device), 'exp': irep+1, 'target_token': torch.tensor(target_token, dtype=torch.long).to(device)}\n","\n","            combined_ex['text'] += shuffled_list[idx_shuffled_list]['text']\n","\n","            combined_dataset.append([combined_ex])\n","\n","    return combined_dataset\n","\n","\n","def dynamic_padding_collate_fn(batch):\n","\n","    batch = [item for sublist in batch for item in sublist]\n","\n","    texts = [item['text'] for item in batch]\n","    labels = [item['label'] for item in batch]\n","    exps = [item['exp'] for item in batch]\n","    target_tokens = [item['target_token'] for item in batch]\n","\n","    # choose option\n","    tokenized_inputs = OPT_tokenizer(texts, padding=\"longest\", truncation=True, return_tensors=\"pt\")\n","    # tokenized_inputs = OPT_tokenizer(texts, padding=\"max_length\", max_length = 2048, truncation=True, return_tensors=\"pt\")\n","\n","    labels_tensor = torch.unsqueeze(torch.tensor(labels, dtype=torch.long).to(device),0)\n","    exps_tensor = torch.unsqueeze(torch.tensor(exps, dtype=torch.long).to(device),0)\n","    target_token_tensor = torch.unsqueeze(torch.tensor(target_tokens, dtype=torch.long).to(device),0)\n","\n","    return {\n","        'text': texts,\n","        'input_ids': tokenized_inputs['input_ids'],\n","        'attention_mask': tokenized_inputs['attention_mask'],\n","        'label': labels_tensor,\n","        'exp': exps_tensor,\n","        'target_token': target_token_tensor\n","    }\n","\n","class CustomDataset(Dataset):\n","    def __init__(self, combined_dataset):\n","        self.dataset = combined_dataset\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx):\n","        return self.dataset[idx]\n"],"metadata":{"id":"JlqLEdhW84X1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","formatted_train_dataset_yes = train_dataset_yes.map(format_examples)\n","formatted_train_dataset_no = train_dataset_no.map(format_examples)\n","\n","# print result to check correctness\n","\n","combined_dataset = create_combined_dataset(\n","                                          train_ds_yes = formatted_train_dataset_yes,\n","                                          train_ds_no = formatted_train_dataset_no,\n","                                          num_expts=num_experiments,\n","                                          num_train_examples=examples_per_exp\n","                                           )\n","\n","custom_dataset = CustomDataset(combined_dataset)\n","custom_dataset_experiment = CustomDataset([item for item in custom_dataset if item[0]['exp'] == SEL_EXP_TRAIN_CD])\n","print(custom_dataset_experiment)\n","\n","# Last step, we create Dataloader passing the bx_size for inference/training (typically: 1, 4, 8, 16)\n","bx_size = bx_size # set it up at the beg of NB\n","dataloader_experiment = DataLoader(custom_dataset_experiment, batch_size=bx_size, collate_fn=dynamic_padding_collate_fn, shuffle=False) #shuffle=False for reproducibility"],"metadata":{"id":"Iux0yDPz9l91","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714490153492,"user_tz":240,"elapsed":7,"user":{"displayName":"Nitesh Agarwal","userId":"18032940909522888771"}},"outputId":"c3185203-4d6f-4b6e-cf1a-4af50ed6fdac"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<__main__.CustomDataset object at 0x7f0efc1a46a0>\n"]}]},{"cell_type":"code","source":["# This is to inspect that the dataloader is performing as expected\n","# Also using the decoding to check back that results are expected and examples can be compared\n","\n","# USE SIMILAR TO THIS TO PASS TO YOUR MODEL - SEE TENSOR DIMENSIONS AND ADJUST WITH SQUEEZE / UNSQUEEZE AS NEEDED THE DATALOADER OUTPUT AS INPUT TO YOUR MODEL\n","\n","for i, batch in enumerate(dataloader_experiment):\n","    if i<200:\n","      print(\"Item Number: \", i, \"experiment#: \", batch['exp'])\n","      print(\"DETOKENIZE: \", OPT_tokenizer.batch_decode(batch['input_ids']))\n","      print(\"Labels: \", batch['label'])\n","      print(\"Target Token: \", batch['target_token'])\n","      print(\"Input_ids: \", batch['input_ids'])\n","      print(\"Attention_Mask: \", batch['attention_mask'])\n","    else:\n","      break"],"metadata":{"id":"gLkf358E_4kn","executionInfo":{"status":"ok","timestamp":1714490153492,"user_tz":240,"elapsed":6,"user":{"displayName":"Nitesh Agarwal","userId":"18032940909522888771"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"5d13c8a4-266b-413d-fdaa-b595e0cf4258"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"]},{"output_type":"stream","name":"stdout","text":["Item Number:  0 experiment#:  tensor([[9]])\n","DETOKENIZE:  ['</s>{yeah but do you think he would fine the lawyer} question: {He is being fined by his own lawyer.} Yes or No? answer: Ġ']\n","Labels:  tensor([[1]])\n","Target Token:  tensor([[3084]])\n","Input_ids:  tensor([[    2, 45152, 42803,    53,   109,    47,   206,    37,    74,  2051,\n","             5,  2470, 24303,   864,    35, 25522,   894,    16,   145, 10110,\n","            30,    39,   308,  2470, 49463,  3216,    50,   440,   116,  1948,\n","            35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1]])\n","Item Number:  1 experiment#:  tensor([[9]])\n","DETOKENIZE:  ['</s>{Jeff Rutherford, Trylon Communications  (212) 725-2295 jeffru@tryloncommunications.com} question: {Jeff Rutherford just got fired from Trylon so his contacts are not working anymore.} Yes or No? answer: Ġ']\n","Labels:  tensor([[1]])\n","Target Token:  tensor([[3084]])\n","Input_ids:  tensor([[    2, 45152, 19663, 22232,     6, 11087, 10849,  6076,  1437,    36,\n","         22145,    43,   262,  1244,    12,  2036,  4015,  4112,  3145,  2070,\n","          1039, 33869, 10849, 42718,     4,   175, 24303,   864,    35, 25522,\n","         19663, 22232,    95,   300,  2277,    31, 11087, 10849,    98,    39,\n","          9872,    32,    45,   447,  5988, 49463,  3216,    50,   440,   116,\n","          1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1]])\n","Item Number:  2 experiment#:  tensor([[9]])\n","DETOKENIZE:  ['</s>{Maryland has a lot to trumpet, Bergmark added. } question: {Maryland is struggling.} Yes or No? answer: Ġ']\n","Labels:  tensor([[1]])\n","Target Token:  tensor([[3084]])\n","Input_ids:  tensor([[    2, 45152, 24877,  1245,    34,    10,   319,     7, 32909,     6,\n","         15303,  6920,   355,     4, 35524,   864,    35, 25522, 24877,  1245,\n","            16,  3306, 49463,  3216,    50,   440,   116,  1948,    35,  4236,\n","         21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1]])\n","Item Number:  3 experiment#:  tensor([[9]])\n","DETOKENIZE:  ['</s>{Industrious CW-armed terrorists could kill thousands of New York subway riders in a day.} question: {An attack on the quiet New York subway would result in minimal casualties. } Yes or No? answer: Ġ']\n","Labels:  tensor([[1]])\n","Target Token:  tensor([[3084]])\n","Input_ids:  tensor([[    2, 45152, 36926, 39554, 17660,    12, 17651,  7263,   115,  3549,\n","          1583,     9,   188,   469, 15604,  7887,    11,    10,   183, 49463,\n","           864,    35, 25522,  4688,   908,    15,     5,  5128,   188,   469,\n","         15604,    74,   898,    11,  9865, 12675,     4, 35524,  3216,    50,\n","           440,   116,  1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n","Item Number:  4 experiment#:  tensor([[9]])\n","DETOKENIZE:  [\"</s>{i tend to not want to go anywhere after that but this way you know you can go to the fitness center right from work and they have aerobics and you know all this machines and all that sort of stuff we're not we're not high rollers like Dallas we don't have a pool but} question: {The fitness center has a pool and nothing else.} Yes or No? answer: Ġ\"]\n","Labels:  tensor([[1]])\n","Target Token:  tensor([[3084]])\n","Input_ids:  tensor([[    2, 45152,   118,  3805,     7,    45,   236,     7,   213,  4558,\n","            71,    14,    53,    42,   169,    47,   216,    47,    64,   213,\n","             7,     5,  5704,  1312,   235,    31,   173,     8,    51,    33,\n","         16482,  2413,  2857,     8,    47,   216,    70,    42,  6271,     8,\n","            70,    14,  2345,     9,  2682,    52,   214,    45,    52,   214,\n","            45,   239,  3825,   268,   101,  3160,    52,   218,    75,    33,\n","            10,  3716,    53, 24303,   864,    35, 25522,   133,  5704,  1312,\n","            34,    10,  3716,     8,  1085,  1493, 49463,  3216,    50,   440,\n","           116,  1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n","Item Number:  5 experiment#:  tensor([[9]])\n","DETOKENIZE:  ['</s>{Most beaches protected from the open ocean have rowboats, canoes, or pedalos for rent by the hour.} question: {There are beaches protected from the open ocean. } Yes or No? answer: Ġ']\n","Labels:  tensor([[0]])\n","Target Token:  tensor([[9904]])\n","Input_ids:  tensor([[    2, 45152,  2895, 11487,  4371,    31,     5,   490,  6444,    33,\n","          3236, 34015,     6,    64,  8013,     6,    50, 26965,   366,    13,\n","          5956,    30,     5,  1946, 49463,   864,    35, 25522,   970,    32,\n","         11487,  4371,    31,     5,   490,  6444,     4, 35524,  3216,    50,\n","           440,   116,  1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n","Item Number:  6 experiment#:  tensor([[9]])\n","DETOKENIZE:  [\"</s>{Increased cost must also be considered with regard to the President's proposal.} question: {There was no cost increase that needs to be considered.} Yes or No? answer: Ġ\"]\n","Labels:  tensor([[1]])\n","Target Token:  tensor([[3084]])\n","Input_ids:  tensor([[    2, 45152, 46636,   701,   531,    67,    28,  1687,    19,  6203,\n","             7,     5,   270,    18,  2570, 49463,   864,    35, 25522,   970,\n","            21,   117,   701,   712,    14,   782,     7,    28,  1687, 49463,\n","          3216,    50,   440,   116,  1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n","Item Number:  7 experiment#:  tensor([[9]])\n","DETOKENIZE:  [\"</s>{Participants believed that boards need to do a better job of identifying their constituencies and understanding and addressing their concerns.} question: {Participants believe that the board is doing a great job and they don't need to change.} Yes or No? answer: Ġ\"]\n","Labels:  tensor([[1]])\n","Target Token:  tensor([[3084]])\n","Input_ids:  tensor([[    2, 45152, 44759,  3277,  2047,    14,  6904,   240,     7,   109,\n","            10,   357,   633,     9,  9397,    49, 22449,     8,  2969,     8,\n","          6477,    49,  1379, 49463,   864,    35, 25522, 44759,  3277,   679,\n","            14,     5,   792,    16,   608,    10,   372,   633,     8,    51,\n","           218,    75,   240,     7,   464, 49463,  3216,    50,   440,   116,\n","          1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1]])\n","Item Number:  8 experiment#:  tensor([[9]])\n","DETOKENIZE:  ['</s>{The foundations of the fort can still be seen, but artifacts from the site are displayed in the Huntly House Museum in Edinburgh.} question: {The artifacts have been bought collectively by an individual.} Yes or No? answer: Ġ']\n","Labels:  tensor([[1]])\n","Target Token:  tensor([[3084]])\n","Input_ids:  tensor([[    2, 45152,   133, 16733,     9,     5, 15016,    64,   202,    28,\n","           450,     6,    53, 23150,    31,     5,  1082,    32,  7899,    11,\n","             5,  7567,   352,   446,  4355,    11,  9652, 49463,   864,    35,\n","         25522,   133, 23150,    33,    57,  2162, 14332,    30,    41,  1736,\n","         49463,  3216,    50,   440,   116,  1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1]])\n","Item Number:  9 experiment#:  tensor([[9]])\n","DETOKENIZE:  [\"</s>{This year's gathering is limited to lawyers and paralegals working for legal aid organizations.} question: {The gathering this year is limited to lawyers and paralegals who work for legal aid organizations.} Yes or No? answer: Ġ\"]\n","Labels:  tensor([[0]])\n","Target Token:  tensor([[9904]])\n","Input_ids:  tensor([[    2, 45152,   713,    76,    18,  5660,    16,  1804,     7,  3969,\n","             8,  2242,  1627,   571,  1536,   447,    13,  1030,  2887,  2665,\n","         49463,   864,    35, 25522,   133,  5660,    42,    76,    16,  1804,\n","             7,  3969,     8,  2242,  1627,   571,  1536,    54,   173,    13,\n","          1030,  2887,  2665, 49463,  3216,    50,   440,   116,  1948,    35,\n","          4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1]])\n","Item Number:  10 experiment#:  tensor([[9]])\n","DETOKENIZE:  [\"</s>{Israel's best accommodation option, according to many travellers, and usually the first stopping place for visiting heads of state.} question: {It is the best hotel in Israel.} Yes or No? answer: Ġ\"]\n","Labels:  tensor([[0]])\n","Target Token:  tensor([[9904]])\n","Input_ids:  tensor([[    2, 45152, 20517,    18,   275, 11607,  1973,     6,   309,     7,\n","           171, 18268,     6,     8,  2333,     5,    78,  8197,   317,    13,\n","          3918,  3885,     9,   194, 49463,   864,    35, 25522,   243,    16,\n","             5,   275,  2303,    11,  1870, 49463,  3216,    50,   440,   116,\n","          1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n","Item Number:  11 experiment#:  tensor([[9]])\n","DETOKENIZE:  ['</s>{But since as many as 30 percent to 40 percent of the graduates at schools like CUNY go into small or solo practices within a few years of graduating, the deans argue, it seems folly not to teach them how to stay afloat financially and take on low-income clients at the same time.} question: {The school deans believe that it would be foolish to not endow their students with basic financial knowledge, as well as training in accepting lower-earning clientele, since many of the students become private practitioners or part of a small business. } Yes or No? answer: Ġ']\n","Labels:  tensor([[0]])\n","Target Token:  tensor([[9904]])\n","Input_ids:  tensor([[    2, 45152,  1708,   187,    25,   171,    25,   389,   135,     7,\n","           843,   135,     9,     5, 11295,    23,  1304,   101,   230,  4154,\n","           975,   213,    88,   650,    50,  5540,  3464,   624,    10,   367,\n","           107,     9, 15128,     6,     5,   263,  1253,  5848,     6,    24,\n","          1302, 41660,    45,     7,  6396,   106,   141,     7,  1095, 26555,\n","         10625,     8,   185,    15,   614,    12,  7214,  2539,    23,     5,\n","           276,    86, 49463,   864,    35, 25522,   133,   334,   263,  1253,\n","           679,    14,    24,    74,    28, 22789,     7,    45,   253,  1722,\n","            49,   521,    19,  3280,   613,  2655,     6,    25,   157,    25,\n","          1058,    11,  8394,   795,    12,  4352,  3509,  3653,  6902,     6,\n","           187,   171,     9,     5,   521,   555,   940, 21492,    50,   233,\n","             9,    10,   650,   265,     4, 35524,  3216,    50,   440,   116,\n","          1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1]])\n","Item Number:  12 experiment#:  tensor([[9]])\n","DETOKENIZE:  ['</s>{And there is Sallie Mae (Student Loan Marketing Association), which creates a similar secondary market in subsidized student loans.} question: {And Sallie Mae is there (Student Loan Marketing Association), it creates a similar secondary market in subsidized student loans.} Yes or No? answer: Ġ']\n","Labels:  tensor([[0]])\n","Target Token:  tensor([[9904]])\n","Input_ids:  tensor([[    2, 45152,  2409,    89,    16,   208,  1250,   324, 13427,    36,\n","         43541, 18934,  9020,  1544,   238,    61,  6670,    10,  1122,  5929,\n","           210,    11, 28397,  1294,  2973, 49463,   864,    35, 25522,  2409,\n","           208,  1250,   324, 13427,    16,    89,    36, 43541, 18934,  9020,\n","          1544,   238,    24,  6670,    10,  1122,  5929,   210,    11, 28397,\n","          1294,  2973, 49463,  3216,    50,   440,   116,  1948,    35,  4236,\n","         21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n","Item Number:  13 experiment#:  tensor([[9]])\n","DETOKENIZE:  ['</s>{hum-um no i used to i really did uh years ago and uh i was thinking about that not too long ago that} question: {I was thinking about it and I did it years ago.} Yes or No? answer: Ġ']\n","Labels:  tensor([[0]])\n","Target Token:  tensor([[9904]])\n","Input_ids:  tensor([[    2, 45152, 18257,    12,   783,   117,   939,   341,     7,   939,\n","           269,   222, 37463,   107,   536,     8, 37463,   939,    21,  2053,\n","            59,    14,    45,   350,   251,   536,    14, 24303,   864,    35,\n","         25522,   100,    21,  2053,    59,    24,     8,    38,   222,    24,\n","           107,   536, 49463,  3216,    50,   440,   116,  1948,    35,  4236,\n","         21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1]])\n","Item Number:  14 experiment#:  tensor([[9]])\n","DETOKENIZE:  ['</s>{Nudity on stage can be powerful, and is still protested, but not as powerful as the frightening glimpse I had as a child of Ethel Merman in Gypsy. Now, if you could get the ghost of Ethel Merman and the undead Mickey Rooney to strip to the waist for 10 rounds of bare-knuckle action, that would be truly frightening stage violence.} question: {Stage violence can be very frightening in some cases.} Yes or No? answer: Ġ']\n","Labels:  tensor([[0]])\n","Target Token:  tensor([[9904]])\n","Input_ids:  tensor([[    2, 45152,   487,  1906,  1571,    15,  1289,    64,    28,  2247,\n","             6,     8,    16,   202, 17278,     6,    53,    45,    25,  2247,\n","            25,     5, 21111, 11986,    38,    56,    25,    10,   920,     9,\n","           381, 30157,   256,  7043,    11, 11882, 23566,   479,   978,     6,\n","           114,    47,   115,   120,     5, 15934,     9,   381, 30157,   256,\n","          7043,     8,     5, 44486, 20210, 14769,     7,  9572,     7,     5,\n","         13977,    13,   158,  5509,     9, 10905,    12, 15204, 29907,   814,\n","             6,    14,    74,    28,  3127, 21111,  1289,  1476, 49463,   864,\n","            35, 25522, 42378,  1476,    64,    28,   182, 21111,    11,   103,\n","          1200, 49463,  3216,    50,   440,   116,  1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1]])\n","Item Number:  15 experiment#:  tensor([[9]])\n","DETOKENIZE:  [\"</s>{but really the main reason is for fuel you know and and when you read so much information that tells you that meats and different animal products you know are causing uh can cause uh different kinds of diseases you know  like heart heart diseases and different diabetes and things like that different kinds of diseases it's like uh is it really worth it and the people i i really think in general people in the United States just don't eat enough vegetables you know because you can talk grain all you want as far as you know cleaning your system out so to speak but you can't beat vegetables to give you all the nutrients and vitamins and and the the the kind that you can't get from a little pill} question: {Vegetables have a lot of nutrients and vitamins we need.} Yes or No? answer: Ġ\"]\n","Labels:  tensor([[0]])\n","Target Token:  tensor([[9904]])\n","Input_ids:  tensor([[    2, 45152,  4297,   269,     5,  1049,  1219,    16,    13,  2423,\n","            47,   216,     8,     8,    77,    47,  1166,    98,   203,   335,\n","            14,  3026,    47,    14, 26627,     8,   430,  3477,   785,    47,\n","           216,    32,  3735, 37463,    64,  1303, 37463,   430,  6134,     9,\n","          6357,    47,   216,  1437,   101,  1144,  1144,  6357,     8,   430,\n","          7704,     8,   383,   101,    14,   430,  6134,     9,  6357,    24,\n","            18,   101, 37463,    16,    24,   269,   966,    24,     8,     5,\n","            82,   939,   939,   269,   206,    11,   937,    82,    11,     5,\n","           315,   532,    95,   218,    75,  3529,   615,  8942,    47,   216,\n","           142,    47,    64,  1067, 10924,    70,    47,   236,    25,   444,\n","            25,    47,   216,  8143,   110,   467,    66,    98,     7,  1994,\n","            53,    47,    64,    75,  1451,  8942,     7,   492,    47,    70,\n","             5, 20012,     8, 26656,     8,     8,     5,     5,     5,   761,\n","            14,    47,    64,    75,   120,    31,    10,   410, 13106, 24303,\n","           864,    35, 25522, 30660,  6460,  6058,    33,    10,   319,     9,\n","         20012,     8, 26656,    52,   240, 49463,  3216,    50,   440,   116,\n","          1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"]}]},{"cell_type":"markdown","source":["# IN-DOMAIN VALIDATION DATASET"],"metadata":{"id":"4gxoA3Lop7hO"}},{"cell_type":"code","source":["\n","def create_combined_dataset_indomain_VALIDATION(val_dataset, num_expts=num_experiments):\n","    combined_dataset = []\n","\n","    for irep in range(num_expts):\n","      for val_ex in val_dataset:\n","\n","            if val_ex['label'] == 0:\n","              target_token = 9904\n","            else:\n","              target_token = 3084\n","\n","            combined_ex = {'text': '', 'label': torch.tensor(val_ex['label'], dtype=torch.long).to(device), 'exp': irep+1, 'target_token': torch.tensor(target_token, dtype=torch.long).to(device)}\n","\n","            combined_ex['text'] += val_ex['text']\n","\n","            # Append the new combined example to the combined dataset\n","            combined_dataset.append([combined_ex])\n","\n","    return combined_dataset\n"],"metadata":{"id":"wc31z8yZUQWm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["formatted_val_dataset_indomain = val_dataset_indomain.map(format_examples)\n","\n","combined_dataset_INDOMAIN_VALIDATION = create_combined_dataset_indomain_VALIDATION(\n","                                          val_dataset = formatted_val_dataset_indomain,\n","                                          num_expts=num_experiments\n","                                           )\n","\n","custom_dataset_indomain_validation = CustomDataset(combined_dataset_INDOMAIN_VALIDATION)\n","custom_dataset_indomain_val_experiment = CustomDataset([item for item in custom_dataset_indomain_validation if item[0]['exp'] == SEL_EXP_TRAIN_CD])\n","print(custom_dataset_indomain_val_experiment)\n","\n","# Last step, we create Dataloader passing the bx_size for inference/training (typically: 1, 4, 8, 16)\n","bx_size = bx_size # set it up at the beg of NB\n","dataloader_indomain_val_experiment = DataLoader(custom_dataset_indomain_val_experiment, batch_size=bx_size, collate_fn=dynamic_padding_collate_fn, shuffle=False) #shuffle=False for reproducibility"],"metadata":{"id":"7XG7PTIOrV1O","executionInfo":{"status":"ok","timestamp":1714490154592,"user_tz":240,"elapsed":1101,"user":{"displayName":"Nitesh Agarwal","userId":"18032940909522888771"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"4c0ec70f-cbc6-4b43-8802-fa591b91260e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<__main__.CustomDataset object at 0x7f102021f160>\n"]}]},{"cell_type":"code","source":["\n","for i, batch in enumerate(dataloader_indomain_val_experiment):\n","    if i<20:\n","      print(\"Item Number: \", i, \"experiment#: \", batch['exp'])\n","      print(\"DETOKENIZE: \", OPT_tokenizer.batch_decode(batch['input_ids']))\n","      print(\"Labels: \", batch['label'])\n","      print(\"Target Token: \", batch['target_token'])\n","      print(\"Input_ids: \", batch['input_ids'])\n","      print(\"Attention_Mask: \", batch['attention_mask'])\n","    else:\n","      break"],"metadata":{"id":"oQBs1lgxsPhO","executionInfo":{"status":"ok","timestamp":1714490154592,"user_tz":240,"elapsed":5,"user":{"displayName":"Nitesh Agarwal","userId":"18032940909522888771"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b0cbc38a-3fcd-44fd-ef4a-90d6adcedbc6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Item Number:  0 experiment#:  tensor([[9]])\n","DETOKENIZE:  [\"</s>{no it didn't} question: {Yes it did.} Yes or No? answer: Ġ\"]\n","Labels:  tensor([[1]])\n","Target Token:  tensor([[3084]])\n","Input_ids:  tensor([[    2, 45152,  2362,    24,   399,    75, 24303,   864,    35, 25522,\n","          9904,    24,   222, 49463,  3216,    50,   440,   116,  1948,    35,\n","          4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n","Item Number:  1 experiment#:  tensor([[9]])\n","DETOKENIZE:  [\"</s>{Who? asked Tommy.} question: {Tommy didn't know, who.} Yes or No? answer: Ġ\"]\n","Labels:  tensor([[0]])\n","Target Token:  tensor([[9904]])\n","Input_ids:  tensor([[    2, 45152, 12375,   116,   553,  8880, 49463,   864,    35, 25522,\n","         15691,  4783,   399,    75,   216,     6,    54, 49463,  3216,    50,\n","           440,   116,  1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1]])\n","Item Number:  2 experiment#:  tensor([[9]])\n","DETOKENIZE:  ['</s>{Paroseas cave, reef, and wreck diving around its shores, giving the diver a wide range of environments to explore.} question: {The diver has no variety in places to explore, they are monotonous. } Yes or No? answer: Ġ']\n","Labels:  tensor([[1]])\n","Target Token:  tensor([[3084]])\n","Input_ids:  tensor([[    2, 45152, 22011,  3876,   281, 12742,     6, 28350,     6,     8,\n","         15107, 12909,   198,    63, 20597,     6,  1311,     5, 13105,    10,\n","          1810,  1186,     9, 11534,     7,  5393, 49463,   864,    35, 25522,\n","           133, 13105,    34,   117,  3143,    11,  2127,     7,  5393,     6,\n","            51,    32,  6154, 27334,  1827,     4, 35524,  3216,    50,   440,\n","           116,  1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1]])\n","Item Number:  3 experiment#:  tensor([[9]])\n","DETOKENIZE:  [\"</s>{um i've visited the Wyoming area i'm not sure exactly where Dances with Wolves was filmed} question: {I don't know even though I visited the area.} Yes or No? answer: Ġ\"]\n","Labels:  tensor([[0]])\n","Target Token:  tensor([[9904]])\n","Input_ids:  tensor([[    2, 45152,   783,   939,   348,  3790,     5, 11027,   443,   939,\n","           437,    45,   686,  2230,   147,   211,  5332,    19, 13889,    21,\n","         10571, 24303,   864,    35, 25522,   100,   218,    75,   216,   190,\n","           600,    38,  3790,     5,   443, 49463,  3216,    50,   440,   116,\n","          1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n","Item Number:  4 experiment#:  tensor([[9]])\n","DETOKENIZE:  [\"</s>{i think Buffalo is an up an coming team they're going to they're showing some real promise for the next uh few years} question: {Buffalo is showing some real promise for the next few years, I think they are an up and coming team.} Yes or No? answer: Ġ\"]\n","Labels:  tensor([[0]])\n","Target Token:  tensor([[9904]])\n","Input_ids:  tensor([[    2, 45152,   118,   206,  5958,    16,    41,    62,    41,   567,\n","           165,    51,   214,   164,     7,    51,   214,  2018,   103,   588,\n","          4198,    13,     5,   220, 37463,   367,   107, 24303,   864,    35,\n","         25522, 42021,  7747,    16,  2018,   103,   588,  4198,    13,     5,\n","           220,   367,   107,     6,    38,   206,    51,    32,    41,    62,\n","             8,   567,   165, 49463,  3216,    50,   440,   116,  1948,    35,\n","          4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n","Item Number:  5 experiment#:  tensor([[9]])\n","DETOKENIZE:  ['</s>{They did this to us.} question: {This was done by them.} Yes or No? answer: Ġ']\n","Labels:  tensor([[0]])\n","Target Token:  tensor([[9904]])\n","Input_ids:  tensor([[    2, 45152,  1213,   222,    42,     7,   201, 49463,   864,    35,\n","         25522,   713,    21,   626,    30,   106, 49463,  3216,    50,   440,\n","           116,  1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1]])\n","Item Number:  6 experiment#:  tensor([[9]])\n","DETOKENIZE:  ['</s>{uh-huh how about any matching programs} question: {What about matching programs? } Yes or No? answer: Ġ']\n","Labels:  tensor([[0]])\n","Target Token:  tensor([[9904]])\n","Input_ids:  tensor([[    2, 45152,  2957,    12,   298,  2957,   141,    59,   143,  8150,\n","          1767, 24303,   864,    35, 25522,  2264,    59,  8150,  1767,   116,\n","         35524,  3216,    50,   440,   116,  1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1]])\n","Item Number:  7 experiment#:  tensor([[9]])\n","DETOKENIZE:  ['</s>{MC2000-2, was initially considered and recommended by the Commission under the market test rules.} question: {MC2000-2 was recommended by the Commission.} Yes or No? answer: Ġ']\n","Labels:  tensor([[0]])\n","Target Token:  tensor([[9904]])\n","Input_ids:  tensor([[    2, 45152,  6018, 17472,    12,   176,     6,    21,  3225,  1687,\n","             8,  5131,    30,     5,  1463,   223,     5,   210,  1296,  1492,\n","         49463,   864,    35, 25522,  6018, 17472,    12,   176,    21,  5131,\n","            30,     5,  1463, 49463,  3216,    50,   440,   116,  1948,    35,\n","          4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n","Item Number:  8 experiment#:  tensor([[9]])\n","DETOKENIZE:  ['</s>{Asked about abortion the other day on CNN, Republican National Committee Chairman Jim Nicholson also invoked what is apparently the party-line  inclusive party.} question: {The Republican National Committee Chairman freelanced on the topic of abortion when asked about it on CNN instead of reiterating the party-line.} Yes or No? answer: Ġ']\n","Labels:  tensor([[1]])\n","Target Token:  tensor([[3084]])\n","Input_ids:  tensor([[    2, 45152, 46688,    59,  6428,     5,    97,   183,    15,  3480,\n","             6,  1172,   496,  1674,  3356,  2488, 19408,    67, 29198,    99,\n","            16,  4100,     5,   537,    12,  1902,  1437, 10510,   537, 49463,\n","           864,    35, 25522,   133,  1172,   496,  1674,  3356, 30270, 16325,\n","            15,     5,  5674,     9,  6428,    77,   553,    59,    24,    15,\n","          3480,  1386,     9, 26209,  1295,     5,   537,    12,  1902, 49463,\n","          3216,    50,   440,   116,  1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n","Item Number:  9 experiment#:  tensor([[9]])\n","DETOKENIZE:  ['</s>{It was like looking into a mirror, except infinitely more realistic.} question: {It was more realistic than looking in a mirror. } Yes or No? answer: Ġ']\n","Labels:  tensor([[0]])\n","Target Token:  tensor([[9904]])\n","Input_ids:  tensor([[    2, 45152,   243,    21,   101,   546,    88,    10,  9807,     6,\n","          4682, 40489,    55, 10556, 49463,   864,    35, 25522,   243,    21,\n","            55, 10556,    87,   546,    11,    10,  9807,     4, 35524,  3216,\n","            50,   440,   116,  1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n","Item Number:  10 experiment#:  tensor([[9]])\n","DETOKENIZE:  ['</s>{John Kasich dropped his presidential bid.} question: {John Kasich recommitted himself to the presidential bid and plans on winning.} Yes or No? answer: Ġ']\n","Labels:  tensor([[1]])\n","Target Token:  tensor([[3084]])\n","Input_ids:  tensor([[    2, 45152, 10567, 26004,  1882,    39,  1939,  2311, 49463,   864,\n","            35, 25522, 10567, 26004, 37573, 16430,  1003,     7,     5,  1939,\n","          2311,     8,   708,    15,  1298, 49463,  3216,    50,   440,   116,\n","          1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n","Item Number:  11 experiment#:  tensor([[9]])\n","DETOKENIZE:  [\"</s>{well that's good that's great} question: {Shit, that is bad, that is horrible.} Yes or No? answer: Ġ\"]\n","Labels:  tensor([[1]])\n","Target Token:  tensor([[3084]])\n","Input_ids:  tensor([[    2, 45152,  3056,    14,    18,   205,    14,    18,   372, 24303,\n","           864,    35, 25522,  3609,   405,     6,    14,    16,  1099,     6,\n","            14,    16, 11385, 49463,  3216,    50,   440,   116,  1948,    35,\n","          4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1]])\n","Item Number:  12 experiment#:  tensor([[9]])\n","DETOKENIZE:  ['</s>{There were beads of perspiration on his brow.} question: {He was perfectly calm and dry as he waited.} Yes or No? answer: Ġ']\n","Labels:  tensor([[1]])\n","Target Token:  tensor([[3084]])\n","Input_ids:  tensor([[    2, 45152,   970,    58, 35036,     9, 20187, 41678,    15,    39,\n","         27423, 49463,   864,    35, 25522,   894,    21,  6683,  6327,     8,\n","          3841,    25,    37,  9010, 49463,  3216,    50,   440,   116,  1948,\n","            35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1]])\n","Item Number:  13 experiment#:  tensor([[9]])\n","DETOKENIZE:  ['</s>{Even the most aged and infirm travel here to die, for nothing is more blessed for a devout Hindu than to die in the great waters of the Varanasi and thus be released from the eternal cycle of rebirth.} question: {Devout Hindus believe that dying in the Varanasi frees a soul from the cycle of rebirth.} Yes or No? answer: Ġ']\n","Labels:  tensor([[0]])\n","Target Token:  tensor([[9904]])\n","Input_ids:  tensor([[    2, 45152,  8170,     5,   144,  5180,     8,  4047,  9856,  1504,\n","           259,     7,  1597,     6,    13,  1085,    16,    55, 12230,    13,\n","            10, 36906, 12316,    87,     7,  1597,    11,     5,   372,  5794,\n","             9,     5,  9676, 16264,   118,     8,  4634,    28,   703,    31,\n","             5, 25023,  4943,     9, 39652, 49463,   864,    35, 25522, 30504,\n","           995, 30618,   679,    14,  8180,    11,     5,  9676, 16264,   118,\n","          7619,   293,    10,  7047,    31,     5,  4943,     9, 39652, 49463,\n","          3216,    50,   440,   116,  1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1]])\n","Item Number:  14 experiment#:  tensor([[9]])\n","DETOKENIZE:  ['</s>{You and your friends are not welcome here, said Severn.} question: {Severn said the people were always welcome there.} Yes or No? answer: Ġ']\n","Labels:  tensor([[1]])\n","Target Token:  tensor([[3084]])\n","Input_ids:  tensor([[    2, 45152,  1185,     8,   110,   964,    32,    45,  2814,   259,\n","             6,    26,  1608, 12170, 49463,   864,    35, 25522, 14696, 12170,\n","            26,     5,    82,    58,   460,  2814,    89, 49463,  3216,    50,\n","           440,   116,  1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n","Item Number:  15 experiment#:  tensor([[9]])\n","DETOKENIZE:  ['</s>{Current Chinese leaders have distinctive characteristics that give them significant advantages over the United States in foreign policy.} question: {The us has advantages over China in foreign policy. } Yes or No? answer: Ġ']\n","Labels:  tensor([[1]])\n","Target Token:  tensor([[3084]])\n","Input_ids:  tensor([[    2, 45152, 42124,  1111,   917,    33, 16141, 12720,    14,   492,\n","           106,  1233, 12340,    81,     5,   315,   532,    11,  1093,   714,\n","         49463,   864,    35, 25522,   133,   201,    34, 12340,    81,   436,\n","            11,  1093,   714,     4, 35524,  3216,    50,   440,   116,  1948,\n","            35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n","Item Number:  16 experiment#:  tensor([[9]])\n","DETOKENIZE:  [\"</s>{The Chinese calendar was used to calculate the year of Japan's foundation by counting back the 1,260 years of the Chinese cosmological cycle.} question: {Japan's foundation was determined by using the Chinese calendar.} Yes or No? answer: Ġ\"]\n","Labels:  tensor([[0]])\n","Target Token:  tensor([[9904]])\n","Input_ids:  tensor([[    2, 45152,   133,  1111,  7127,    21,   341,     7, 15756,     5,\n","            76,     9,  1429,    18,  4811,    30, 10581,   124,     5,   112,\n","             6, 21566,   107,     9,     5,  1111, 12793,   119,  9779,  4943,\n","         49463,   864,    35, 25522, 21318,    18,  4811,    21,  3030,    30,\n","           634,     5,  1111,  7127, 49463,  3216,    50,   440,   116,  1948,\n","            35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1]])\n","Item Number:  17 experiment#:  tensor([[9]])\n","DETOKENIZE:  [\"</s>{Two clues in the Pennsylvania  1) The boy had said, I'm going to go to the dinner dance and kill some people.} question: {There was only one clue in Pennsylvania and it had nothing to do with the boy.} Yes or No? answer: Ġ\"]\n","Labels:  tensor([[1]])\n","Target Token:  tensor([[3084]])\n","Input_ids:  tensor([[    2, 45152,  9058, 14885,    11,     5,  4367,  1437,   112,    43,\n","            20,  2143,    56,    26,     6,    38,   437,   164,     7,   213,\n","             7,     5,  3630,  3836,     8,  3549,   103,    82, 49463,   864,\n","            35, 25522,   970,    21,   129,    65, 18664,    11,  4367,     8,\n","            24,    56,  1085,     7,   109,    19,     5,  2143, 49463,  3216,\n","            50,   440,   116,  1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1]])\n","Item Number:  18 experiment#:  tensor([[9]])\n","DETOKENIZE:  ['</s>{8 A stoichiometry of 1.03 is typical when the FGD process is producing gypsum by-product, while a stoichiometry of 1.05 is needed to produce waste suitable for a landfill.} question: {A stoichiometry of 1.07 is typical when the FGD process is producing gypsum by-product} Yes or No? answer: Ġ']\n","Labels:  tensor([[1]])\n","Target Token:  tensor([[3084]])\n","Input_ids:  tensor([[    2, 45152,   398,    83, 20572, 20000, 40899,     9,   112,     4,\n","          3933,    16,  6097,    77,     5, 25408,   495,   609,    16,  5591,\n","         18124,  3275,   783,    30,    12, 20565,     6,   150,    10, 20572,\n","         20000, 40899,     9,   112,     4,  2546,    16,   956,     7,  2592,\n","          3844, 10686,    13,    10, 21289, 49463,   864,    35, 25522,   250,\n","         20572, 20000, 40899,     9,   112,     4,  3570,    16,  6097,    77,\n","             5, 25408,   495,   609,    16,  5591, 18124,  3275,   783,    30,\n","            12, 20565, 24303,  3216,    50,   440,   116,  1948,    35,  4236,\n","         21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1]])\n","Item Number:  19 experiment#:  tensor([[9]])\n","DETOKENIZE:  [\"</s>{oh yes yeah yeah yeah that's true too that's true} question: {It is true} Yes or No? answer: Ġ\"]\n","Labels:  tensor([[0]])\n","Target Token:  tensor([[9904]])\n","Input_ids:  tensor([[    2, 45152,  2678,  4420, 11380, 11380, 11380,    14,    18,  1528,\n","           350,    14,    18,  1528, 24303,   864,    35, 25522,   243,    16,\n","          1528, 24303,  3216,    50,   440,   116,  1948,    35,  4236, 21402]])\n","Attention_Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1]])\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qCU9xUpVx1ei"},"outputs":[],"source":["# Custom model class for sequence classification\n","\n","import torch.nn as nn\n","from transformers import AutoModelForSequenceClassification, AutoTokenizer\n","\n","class OPT_VanillaFT(nn.Module):\n","    def __init__(self, model_name, num_labels):\n","        super().__init__()\n","        self.model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n","\n","    def forward(self, input_ids, attention_mask=None, labels=None):\n","        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","        logits = outputs.logits\n","        return logits"]},{"cell_type":"code","source":["# Define optimizer and loss function\n","model = OPT_VanillaFT(model_name, num_labels=2)\n","# print(model)\n","optimizer = AdamW(model.parameters(), lr=1e-6)\n","criterion = nn.CrossEntropyLoss()\n","\n","# Training loop\n","model.to(device)\n","model.train()\n","\n","train_losses = []\n","val_losses = []\n","train_accuracies = []\n","val_accuracies = []\n","\n","num_epochs = 6\n","batch_size = 1  # Reduce batch size to 4\n","for epoch in range(num_epochs):\n","    print(\"epoch: \", epoch)\n","    total_loss = 0.0\n","    train_acc = 0.0\n","    for i, batch in enumerate(dataloader_experiment):\n","\n","        # print(\"input_ids: \", batch[\"input_ids\"])\n","\n","\n","        input_ids = batch[\"input_ids\"].to(device)\n","        attention_mask = batch[\"attention_mask\"].to(device)\n","        labels = batch[\"label\"].to(device).squeeze(0)\n","\n","\n","        # print(\"labels: \", batch[\"label\"], labels.shape)\n","        optimizer.zero_grad()\n","\n","        # Forward pass\n","        outputs = model(input_ids, None, labels) #, labels=labels)\n","        labels = torch.nn.functional.one_hot(labels, num_classes=2).float()\n","        # print(\"outputs SHAPE: \", outputs.shape)\n","        # print(outputs)\n","        # # Compute loss\n","        loss = criterion(outputs, labels)\n","        # print(\"loss: \", loss)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","\n","        # Compute accuracy\n","        train_acc = train_acc + 1*(torch.argmax(outputs, dim=1).item()==batch[\"label\"].to(device).squeeze(0).item())\n","\n","    average_loss = total_loss / (i+1)\n","    train_losses.append(average_loss)\n","    train_accuracies.append(train_acc / (i+1))\n","    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {average_loss:.6f}, Accuracy: {train_accuracies[-1]:.9f}\")\n","\n","    # val_losses.append(val_loss / num_validations)\n","    # val_accuracies.append(val_accuracy / num_validations)\n","\n","    # print(f\"Validation Loss: {val_losses[-1]:.6f}, Validation Accuracy: {val_accuracies[-1]:.9f}\")\n","\n","    # model.train()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yKFssGNxx40D","outputId":"1293bb6f-f2c9-46e9-e87c-7259a5abf53b","executionInfo":{"status":"ok","timestamp":1714490893169,"user_tz":240,"elapsed":738579,"user":{"displayName":"Nitesh Agarwal","userId":"18032940909522888771"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of OPTForSequenceClassification were not initialized from the model checkpoint at facebook/opt-2.7b and are newly initialized: ['score.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["epoch:  0\n","Epoch 1/6, Train Loss: 1.077211, Accuracy: 0.500000000\n","epoch:  1\n","Epoch 2/6, Train Loss: 0.867310, Accuracy: 0.500000000\n","epoch:  2\n","Epoch 3/6, Train Loss: 0.616526, Accuracy: 0.687500000\n","epoch:  3\n","Epoch 4/6, Train Loss: 0.402400, Accuracy: 0.937500000\n","epoch:  4\n","Epoch 5/6, Train Loss: 0.522381, Accuracy: 0.750000000\n","epoch:  5\n","Epoch 6/6, Train Loss: 0.326861, Accuracy: 0.937500000\n"]}]},{"cell_type":"code","source":["# Validation\n","model.eval()\n","val_loss = 0.0\n","val_accuracy = 0.0\n","with torch.no_grad():\n","    for batch in dataloader_indomain_val_experiment:\n","        src = batch[\"input_ids\"].to(device)\n","        attention_mask = batch[\"attention_mask\"].to(device)\n","        labels = batch[\"label\"].to(device).squeeze(0)\n","\n","        # Forward pass\n","        outputs = model(input_ids, None, labels)\n","\n","        # Compute loss\n","        # if 'logits' in outputs:\n","        loss = criterion(outputs, labels)\n","        val_loss += loss.item()\n","\n","        # Compute accuracy\n","        val_accuracy += 1*(torch.argmax(outputs, dim=1).item()==batch[\"label\"].to(device).squeeze(0).item())\n","    val_loss = val_loss/num_validations\n","    val_accuracy = val_accuracy/num_validations\n","    print(f\"Validation Loss: {val_loss:.6f}, Validation Accuracy: {val_accuracy:.9f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sGIvEE4MZPMw","executionInfo":{"status":"ok","timestamp":1714490893169,"user_tz":240,"elapsed":15,"user":{"displayName":"Nitesh Agarwal","userId":"18032940909522888771"}},"outputId":"a0422ee3-8e0a-45b4-f8b8-2cb638069a7a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Validation Loss: 1.501443, Validation Accuracy: 0.523437500\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i7DXLZYPedey","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714490895965,"user_tz":240,"elapsed":2808,"user":{"displayName":"Nitesh Agarwal","userId":"18032940909522888771"}},"outputId":"f987817d-d84b-424b-a614-c622674e7365"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['premise', 'hypothesis', 'label', 'parse_premise', 'parse_hypothesis', 'binary_parse_premise', 'binary_parse_hypothesis', 'heuristic', 'subcase', 'template'],\n","        num_rows: 30000\n","    })\n","    validation: Dataset({\n","        features: ['premise', 'hypothesis', 'label', 'parse_premise', 'parse_hypothesis', 'binary_parse_premise', 'binary_parse_hypothesis', 'heuristic', 'subcase', 'template'],\n","        num_rows: 30000\n","    })\n","})"]},"metadata":{},"execution_count":28}],"source":["dataset_ood = load_dataset(\"hans\")\n","dataset_ood"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oAFcefWkwmPs"},"outputs":[],"source":["dataset_ood_val = (dataset_ood['validation']).filter(lambda example: example[\"heuristic\"] == 'lexical_overlap')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jn4ufgMzwmNS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714490895965,"user_tz":240,"elapsed":4,"user":{"displayName":"Nitesh Agarwal","userId":"18032940909522888771"}},"outputId":"ede75573-5e01-4e07-8d69-fcff23d9ca54"},"outputs":[{"output_type":"stream","name":"stdout","text":["indices_ood_val:  [6252 4684 1731 ... 9410 1671  474]\n"]},{"output_type":"execute_result","data":{"text/plain":["Dataset({\n","    features: ['premise', 'hypothesis', 'label', 'parse_premise', 'parse_hypothesis', 'binary_parse_premise', 'binary_parse_hypothesis', 'heuristic', 'subcase', 'template'],\n","    num_rows: 1024\n","})"]},"metadata":{},"execution_count":30}],"source":["# Perform the filters and splits from the original datasets\n","\n","\n","random_split_seed_ood = 42 # set above, equal to 42\n","\n","examples_per_exp =  examples_per_exp # 16\n","num_experiments = num_experiments # 10\n","num_validations = num_validations # 16*64 #64*16 = 1024 #6692\n","\n","np.random.seed(random_split_seed_ood)\n","indices_ood_val = np.random.choice(range(len(dataset_ood_val)), size=num_validations, replace=False)\n","print(\"indices_ood_val: \", indices_ood_val)\n","\n","dataset_ood_val_sel = dataset_ood_val.select(indices_ood_val)\n","dataset_ood_val_sel"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3aAqJWOqwmKR"},"outputs":[],"source":["# format examples functions formats according to different types of formats for ICL both training and validation examples\n","\n","# select format to use here:\n","format_train_val = format_train_val # set it at the top of notebook in a common place\n","\n","\n","def format_examples_validation_VALOOD(example_val, format_val = format_train_val):\n","    if format_val== 'minimal':\n","      # \"minimal\" format\n","      return {'text': \"{\"  + example_val['premise'] + \"} {\" + example_val['hypothesis'] + \"}\" + \" ? Ġ\"}\n","    elif format_val== 'gpt3':\n","      # \"minimal\" format\n","      return {'text': \"{\"  + example_val['premise'] + \"} question: {\" + example_val['hypothesis'] + \"}\" + \" Yes or No? answer: Ġ\"}\n","\n","def create_combined_dataset(val_dataset, num_expts=num_experiments):\n","    combined_dataset = []\n","\n","    for irep in range(num_expts):\n","      for val_ex in val_dataset:\n","\n","        combined_ex = {'text': '', 'label': torch.tensor(val_ex['label'], dtype=torch.long).to(device), 'exp': irep+1}\n","\n","        combined_ex['text'] += val_ex['text']\n","\n","        combined_dataset.append([combined_ex])\n","\n","    return combined_dataset\n","\n","\n","def dynamic_padding_collate_fn_VALOOD(batch):\n","    # This function is created to be able to tokenize dynamically to max length within each batch\n","    # Also, by modifying the tokenizer used, several other options are available\n","    # for example, if we set padding to a specified max_length, for example the model max_length, is also an option, not the default though\n","    # the default is the dynamic padding\n","\n","    batch = [item for sublist in batch for item in sublist]\n","\n","    texts = [item['text'] for item in batch]\n","    labels = [item['label'] for item in batch]\n","    exps = [item['exp'] for item in batch]\n","\n","    # choose option\n","    tokenized_inputs = OPT_tokenizer(texts, padding=\"longest\", truncation=True, return_tensors=\"pt\")\n","\n","    labels_tensor = torch.unsqueeze(torch.tensor(labels, dtype=torch.long).to(device),0)\n","    exps_tensor = torch.unsqueeze(torch.tensor(exps, dtype=torch.long).to(device),0)\n","\n","    return {\n","        'text': texts,\n","        'input_ids': tokenized_inputs['input_ids'],\n","        'attention_mask': tokenized_inputs['attention_mask'],\n","        'label': labels_tensor,\n","        'exp': exps_tensor,\n","    }\n","\n","\n","class CustomDataset(Dataset):\n","    def __init__(self, combined_dataset):\n","        self.dataset = combined_dataset\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx):\n","        return self.dataset[idx]\n"]},{"cell_type":"code","source":["# First the samples are formatted according to selection above\n","# Important to check selection and re-run cell above so that it is taken by the mapping function correctly\n","\n","formatted_val_dataset_ood = dataset_ood_val_sel.map(format_examples_validation_VALOOD)\n","\n","# Initialize custom dataset with the combined dataset\n","# print result to check correctness\n","\n","combined_dataset_VALOOD = create_combined_dataset(\n","                                          val_dataset = formatted_val_dataset_ood,\n","                                          num_expts=num_experiments\n","                                           )\n","\n","custom_dataset_VALOOD = CustomDataset(combined_dataset_VALOOD)\n","print(custom_dataset_VALOOD)\n","\n","custom_dataset_VALOOD_EXP = CustomDataset([item for item in custom_dataset_VALOOD if item[0]['exp'] == SEL_EXP_TRAIN_CD])\n","\n","# Last step, we create Dataloader passing the bx_size for inference (typically: 1, 4, 8, 16)\n","bx_size = bx_size # set it up at the beg of NB\n","dataloader_VALOOD = DataLoader(custom_dataset_VALOOD_EXP, batch_size=bx_size, collate_fn=dynamic_padding_collate_fn_VALOOD, shuffle=False) #shuffle=False for reproducibility"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2u9wEYxr2OLD","executionInfo":{"status":"ok","timestamp":1714490897391,"user_tz":240,"elapsed":1428,"user":{"displayName":"Nitesh Agarwal","userId":"18032940909522888771"}},"outputId":"1180ddfa-f14e-4225-8115-24653c127cea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<__main__.CustomDataset object at 0x7f0efc1a5150>\n"]}]},{"cell_type":"code","source":["# Validation\n","model.eval()\n","val_loss = 0.0\n","val_accuracy = 0.0\n","with torch.no_grad():\n","    for batch in dataloader_VALOOD:\n","        # print(batch)\n","\n","        src = batch[\"input_ids\"].to(device)\n","        attention_mask = batch[\"attention_mask\"].to(device)\n","        labels = batch[\"label\"].to(device).squeeze(0)\n","\n","        # Forward pass\n","        outputs = model(input_ids, None, labels)\n","\n","        # Compute loss\n","        # if 'logits' in outputs:\n","        loss = criterion(outputs, labels)\n","        val_loss += loss.item()\n","        # break\n","\n","        # Compute accuracy\n","        val_accuracy += 1*(torch.argmax(outputs, dim=1).item()==batch[\"label\"].to(device).squeeze(0).item())\n","    val_loss = val_loss/num_validations\n","    val_accuracy = val_accuracy/num_validations\n","    print(f\"Validation Loss: {val_loss:.6f}, Validation Accuracy: {val_accuracy:.9f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wBWNm-Cbx72J","executionInfo":{"status":"ok","timestamp":1714490897391,"user_tz":240,"elapsed":4,"user":{"displayName":"Nitesh Agarwal","userId":"18032940909522888771"}},"outputId":"aa758295-20f6-4a58-8c3b-f0fb4f7164f2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Validation Loss: 1.555123, Validation Accuracy: 0.505859375\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"yu95zGyZrS9E"},"execution_count":null,"outputs":[]}]}